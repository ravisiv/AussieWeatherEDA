{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87e6a4d",
   "metadata": {},
   "source": [
    "# Lab 3 : Clustering\n",
    "\n",
    "### Group 3 - Members:\n",
    "\n",
    "_Tai Chowdhury_<br>\n",
    "_Apurv Mittal_<br>\n",
    "_Ravi Sivaraman_<br>\n",
    "_Seemant Srivastava_<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b484e41",
   "metadata": {},
   "source": [
    "## Business Understanding 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad49dd",
   "metadata": {},
   "source": [
    "The weather prediction has been of interest for ages as it effects all of us in our day to day life in many ways. Ability to predict with high accuracy, if its going to rain today or tomorrow and how much can help us plan our day better and we can take precautions if needed.\n",
    "\n",
    "We have acquired the Australian Weather dataset from Kaggle portal. It contains 10 years of weather data collected from many locations across Australia. These are daily weather observations. There are 145,459 observations with 23 attributes. These attributes describes temperatures, wind, cloud, pressure, and humidity conditions both. There numeric data are broken down into morning (am) and afternoon (pm). \n",
    "\n",
    "This dataset can be useful for scientific weather reporting and analysis projects for the respective country's regions. These projects can provide solutions to weather prediction problems. For our project, we have chosen RainTomorrow (categorical) and Rainfall (continuous) as predictor variables. `RainTomorrow` is a categorical attribute which indicates whether it is going to rain tomorrow - yes or no. `Rainfall` is a continuous attribute that measures amount of rainfall each of the particular locations have received (in mm). Using our models, we will be able to design an algorithm where the bureau can help to predict rainfall for different regions in Australia.\n",
    "\n",
    "We will measure the accuracy and effectiveness of our model for categorical variable `RainTomorrow` by using 10-fold cross validation against the confusion matrix measurements like: sensitivity, specificity and accuracy. We can use Logistic Regression, Random Forest and other parametric and non-parametric models to measure the effectiveness and determine the most appropriate model for prediction.\n",
    "\n",
    "Similarly, We will predict the `Rainfall` (in mm) which is a continous variable using a regression model. We will its effectiveness by using 10-fold cross validation against RMSE (Root Mean Square Error).\n",
    "\n",
    "Once the machine learning model is built we can test and measure its validity in other geographies and may not just confine to Australia.\n",
    "\n",
    "\n",
    "\n",
    "Source: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb65bf",
   "metadata": {},
   "source": [
    "## Data Understanding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Australia weather data\n",
    "df = pd.read_csv(\"weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  View the top rows of the data imported\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22fa7f",
   "metadata": {},
   "source": [
    "Data imported successfully. We can view all the variables and the top rows above. Its visible that there are several null values and we may need to do decide what should we do to accomodate the missing information. As we go along, we will talk abpout the approach we have adopted to handle the sceanrios with missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick look at the variables and the data type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916393f2",
   "metadata": {},
   "source": [
    "Below are the descriptions for all 23 attributes for our dataset:\n",
    "\n",
    "    Name \t              Type \t                            Description\n",
    "\n",
    "    `Date               Date  \t           The date of observation.\n",
    "\n",
    "    Location\t       Nominal             The name of the location of the weather station.\n",
    "\n",
    "    MinTemp\t           float64\t           Minimum temperature in the 24 hours to 9am (in celsius).\n",
    "\n",
    "    MaxTemp\t           float64\t           Maximum temperature in the 24 hours to 9am (in celsius).\n",
    "\n",
    "    Rainfall\t       float64\t           Precipitation (rainfall) in the 24 hours to 9am (in mm).\n",
    "\n",
    "    Evaporation\t       float64\t           \"Class A\" pan evaporation in the 24 hours to 9am (in mm)\n",
    "\n",
    "    Sunshine\t       float64\t           Bright sunshine in the 24 hours to midnight (in hours).\n",
    "\n",
    "    WindGustDir\t       Nominal        \t   Direction of strongest gust in the 24 hours to midnight.\n",
    "\n",
    "    WindGustSpeed\t   float64\t           Speed of strongest wind gust in the 24 hours to midnight (kmph).\n",
    "\n",
    "    WindDir9am\t       Nominal      \t   Wind direction averaged over 10 minutes prior to 9 am.\n",
    "\n",
    "    WindDir3pm\t       Nominal      \t   Wind direction averaged over 10 minutes prior to 3 pm.\n",
    "\n",
    "    WindSpeed9am\t   float64\t           Wind speed averaged over 10 minutes prior to 9 am (kmph). \n",
    "\n",
    "    WindSpeed3pm\t   float64\t           Wind speed averaged over 10 minutes prior to 3 pm (kmph). \n",
    "\n",
    "    Humidity9am\t       float64\t           Relative humidity at 9 am (in percent).\n",
    "\n",
    "    Humidity3pm\t       float64\t           Relative humidity at 3 pm (in percent). \n",
    "\n",
    "    Pressure9am\t       float64\t           Atmospheric pressure mean sea level at 9 am (hectopascals).\n",
    "\n",
    "    Pressure3pm\t       float64\t           Atmospheric pressure mean sea level at 3 pm (hectopascals). \n",
    "\n",
    "    Cloud9am\t       float64\t           Fraction of sky obscured by cloud at 9 am (eighths).\n",
    "\n",
    "    Cloud3pm\t       float64\t           Fraction of sky obscured by cloud at 3 pm (eighths). \n",
    "\n",
    "    Temp9am\t           float64\t           Temperature at 9 am (in celsius).\n",
    "\n",
    "    Temp3pm\t           float64\t           Temperature at 3 pm (in celsius). \n",
    "\n",
    "    RainToday\t       Nominal      \t   Whether it is going to rain current day - Yes or No.\n",
    "\n",
    "    RainTomorrow\t   Nominal      \t   Whether there will be rainfall tomorrow - Yes or No.`\n",
    "\n",
    "\n",
    "Source: http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d08ff",
   "metadata": {},
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the dataset with statistical summary of numeric \"float\" variables\n",
    "\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbd00b",
   "metadata": {},
   "source": [
    "Ran summary statistics on the imported dataset. We can see the various satistical summary on the \"float\" (numeric) variables. We see some large variations in the dataset like Evaportaion ranges from 0 to 145, wind gust varies from 6 kmph to 135 kmph. Which are huge variation but are they invalid data or genuine outliers? We will investigate that in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c84ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of data types\n",
    "\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ef9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96ad8b",
   "metadata": {},
   "source": [
    "We ran a duplicate check and we identify there are no duplicates in our dataset which means we don't need to take any action to reduce the impact of duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3de531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094290f6",
   "metadata": {},
   "source": [
    "We can see there are bunch of missing values in our dataset across the variables. Some variables stand out in terms of number of missing information like `Evaporation` and `Sunshine`. We will continue to investigage further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149540e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of total records\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eef5c8",
   "metadata": {},
   "source": [
    "We have total of 145,460 records. This includes the missing data as well. This calculation is useful in undetstanding the magnitude of missing data. What is the percentage of data is actually missing? We find out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04032589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the percentage of missing information\n",
    "\n",
    "(df.isnull().sum()/len(df)*100).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34a64b",
   "metadata": {},
   "source": [
    "We listed the missing data in ascending order to understand what percentage of data is missing. This will help us in determining the most appropriate action we can take to handle the missing information. \n",
    "\n",
    "As seen above there are 6 variables which has more than 10% of missing data. `Sunshine`, `Evaporation`, `Cloud at 3 pm`, `Cloud at 9 am` has the most missing data in that order. With more that 38% of missing information, we have to decide how to impute the missing information. If we delete the missing rows, we will lose a lot of important and pertinent information which is not desirable. We need to decide a way to impute the information.\n",
    "\n",
    "However, before we impute any information, we also notice that `RainToday` and `Rain Tomorrow` also has about equal amount of missing data but the percentage is not very high. Its under 2.5%. And since `Rain Tomorrow` is one of our response variables, we don't want to impute information there based on certain assumption as it may impact the overall predictability of the data and our models may not turn out to be very successful.\n",
    "\n",
    "With that in mind, we first start with deleting the rows with missing `Rain Today` and `Rain Tomorrow` variable as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing records which are blank for Rain today and Rain tomorrow\n",
    "\n",
    "df.dropna(subset = [\"RainToday\"], inplace=True)\n",
    "df.dropna(subset = [\"RainTomorrow\"], inplace=True)\n",
    "\n",
    "# REFERENCE: https://www.kite.com/python/answers/how-to-drop-empty-rows-from-a-pandas-dataframe-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482e816",
   "metadata": {},
   "source": [
    "As explained above, we decided to drop the records with missing (null) data for RainToday and RainTomorrow variables which is under 2.5% of the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14abf142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the null values again\n",
    "(df.isnull().sum()/len(df)*100).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3bd12",
   "metadata": {},
   "source": [
    "A quick look at the percentage of missing data after deletion of the missing rows for RainToday and RainTomorrow confirms the data got deleted successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the data into categorical and numeric\n",
    "\n",
    "df_num = df.columns[df.dtypes == 'float64']\n",
    "df_cat=df.columns[df.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_num)\n",
    "print(\"Categorical Variables:\", df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df_num].groupby([df['RainToday'],df['RainTomorrow']]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06532d1c",
   "metadata": {},
   "source": [
    "Since the Rainfall is the interest of this study. We decided to check the mean for all numeric variables based upon the value for `RainToday` and `RainTomorrow` variables. We belive that Rain is a very significant weather event and lots of other events and variations in the weather happen on the account of the Rain, its only appropriate to check how the mean varies for the variables depending upon it rains or not.\n",
    "\n",
    "As expected, we notice the variation is significant among the variables depending upon the rain event.Like `Humidity` varies significatly (particluarly in the evening) as it rains today or tomorrow versus no rain at all. Similarly cloud cover also sees a significant variation.\n",
    "\n",
    "We will closely analyze `Evaporation`, `Sunshine`, `Cloud9am`, `Cloud3pm` as these variables has highest number of missing information. We need to determine if its safe to impute the missing information with the mean values for these variables or should be take a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of null for Evaporation by the RainToday And Rain Tomorrow\n",
    "df_E = df.Evaporation.isnull().groupby([df['RainToday'],df['RainTomorrow']]).sum()\n",
    "df_E_mean = df.Evaporation.groupby([df['RainToday'],df['RainTomorrow']]).mean()\n",
    "print('Number of Nulls in Evaporation grouped by Rain Today and Rain Tomorrow:\\n',df_E)\n",
    "print('\\nMean of Evaporation grouped by Rain Today and Rain Tomorrow:\\n',df_E_mean)\n",
    "\n",
    "print('\\nOverall Mean of Evaporation:\\n',df.Evaporation.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa29d69",
   "metadata": {},
   "source": [
    "The `Evaporation` has most of its missing values for the days it doesn't rain, which is both `RainToday` and `RainTomorrow` are No. For all other days the number of missing records are comparable.\n",
    "\n",
    "The average `Evaporation` on the days it doesn't rain i.e. both `RainToday` and `RainTomorrow` are \"No\" is 6.03 while the average `Evaporation` on the days it rains both Today and Tomorrow is 3.87, which is a variation of more than `55%`.\n",
    "\n",
    "Based on the above data, its not appropriate to impute a mean value for every missing record of `Evaporation`. We will continue to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of null for Cloud 9 AM by the RainToday And Rain Tomorrow\n",
    "df_C9 = df.Cloud9am.isnull().groupby([df['RainToday'],df['RainTomorrow']]).sum()\n",
    "df_C9_mean = df.Cloud9am.groupby([df['RainToday'],df['RainTomorrow']]).mean()\n",
    "print('Number of Nulls in Cloud at 9 AM grouped by Rain Today and Rain Tomorrow:\\n',df_C9, '\\n')\n",
    "print('\\nMean of Cloud at 9 AM  grouped by Rain Today and Rain Tomorrow:\\n',df_C9_mean)\n",
    "print('\\nOverall Mean of Cloud at 9 AM:\\n',df.Cloud9am.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c599cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "We check the another variable with large number of missing information `Cloud9am` (clouds at 9 am) which has more than 36,000 missing records. In this case also like how we noticed for `Evaporation`, `Sunshine` and `Cloud3pm` the mean value of cloud significantly depends upon if it `RainToday` or `RainTomorrow`.\n",
    "\n",
    "The clouds at 9 am is significatly higher for the days it rains. Also, the overall mean is much lower.\n",
    "\n",
    "Considering the above examples, it appropriate to say that we shouldn't impute overall variable mean for the missing records as it'll be significantly wrong based on the fact if it Rains Today and/or Rains Tomorrow or not.\n",
    "\n",
    "So, we decided to impute data based on the mean of numeric variables for the days of `RainToday` and `RainTomorrow`.\n",
    "\n",
    "The categorical variables will be imputed based on the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d0b20",
   "metadata": {},
   "source": [
    "#### Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute data (numeric) based on the mean for RainToday and RainTomorrow\n",
    "\n",
    "df_impute = df\n",
    "mat_yesno = df[df_num].groupby([df['RainToday'],df['RainTomorrow']]).mean()\n",
    "RAINTODAY=0\n",
    "RAINTOMORROW=1\n",
    "COUNTER = 0\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for indexattr in mat_yesno.iloc[COUNTER].index:\n",
    "            df_impute.loc[(df_impute[\"RainToday\"] == mat_yesno.iloc[COUNTER].name[RAINTODAY] ) \n",
    "                          & (df_impute[\"RainTomorrow\"] == mat_yesno.iloc[COUNTER].name[RAINTOMORROW]) \n",
    "                          & (df_impute[indexattr].isnull()), indexattr] = mat_yesno.iloc[COUNTER][indexattr]\n",
    "        COUNTER = COUNTER + 1\n",
    "\n",
    "        \n",
    "        \n",
    "# Impute data (categorical) with mode of each variable\n",
    "\n",
    "df_impute['WindDir9am'] = df_impute['WindDir9am'].fillna(df_impute['WindDir9am'].mode()[0])\n",
    "df_impute['WindGustDir'] = df_impute['WindGustDir'].fillna(df_impute['WindGustDir'].mode()[0])\n",
    "df_impute['WindDir3pm'] = df_impute['WindDir3pm'].fillna(df_impute['WindDir3pm'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fdaf2c",
   "metadata": {},
   "source": [
    "As mentioned above, we imputed data for all numeric variables with the means for the combination of `RainToday` and `RainTomorrow`. We calcualted the value for `RainToday` and `RainTomorrow` both as \"No\" and imputed the data for the missing variables for such combination, similary calculated `RainToday` as \"Yes\" and `RainTomorrow` as \"No\" and imputed the mean value for the variable so and so forth.\n",
    "\n",
    "For categorical variables `WindDir9am`, `WindDir3pm` are covering the direction of the wind at different 9 am and 3 pm respectively, while `WindGustDir`is the direction of the wind gust. All these variables are about the direction and and the largest missing variable is `6.8%` for Wind Direction at 9 am. We decided to impute this data with the Mode for each of the categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956537fc",
   "metadata": {},
   "source": [
    "#### Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.\n",
    "The intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution.\n",
    "While calculating the Z-score we re-scale and center the data and look for data points which are too far from zero. These data points which are way too far from zero will be treated as the outliers.\n",
    "In most of the cases a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.\n",
    "The first array contains the list of row numbers and second array respective column numbers, for example if z[8][5] is listed to have a Z-score higher than 3, then it means 8th record in 5th column is an outlier.\n",
    "\n",
    "\n",
    "We found 8,309 outliers for our Rainfall attributes and we have removed the rows using z-score technique.\n",
    "\n",
    "###### Reference: https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
    "###### Reference: https://towardsdatascience.com/detecting-and-treating-outliers-in-python-part-1-4ece5098b755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ab32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier - Uni-variate(one variable outlier analysis) using Box plot\n",
    "\n",
    "df_rainfall = df_impute[['Rainfall']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.set_xlabel(\"va=baseline\")\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df_rainfall))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the maximum value of the Rainfall variable\n",
    "df_impute[['Rainfall']].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b09f96",
   "metadata": {},
   "source": [
    "In the above boxplot analysis we see the `Rainfall` data is highly skewed and we can see there are apparent outliers. We notice most of the values (including mean, median) falls around `0`. Which is understandable considering it does't rain most of the days in Australia.\n",
    "\n",
    "If we look at the extreme value for Rainfall alone, its `371 mm`. Based on the recorded weather history this is not nearly equal to be highest or an outlier. The Highest daily rainfall in 24 hours period is recorded to be 907mm in Australia.\n",
    "\n",
    "So, we decided to treat this as a valid observation and not change it in any way.\n",
    "\n",
    "##### Reference:  https://www.ga.gov.au/scientific-topics/national-location-information/dimensions/climatic-extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ab4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of subset of variables\n",
    "df_num\n",
    "df_boxplot = df_impute[['MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Cloud9am', 'Cloud3pm',\n",
    "       'Temp9am', 'Temp3pm']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax.set_xlabel(\"va=baseline\")\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df_boxplot))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for Pressure at 9 am and 3 pm\n",
    "\n",
    "df_pressure = df_impute[['Pressure9am', 'Pressure3pm']]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.set_xlabel(\"va=baseline\")\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df_pressure))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum of each variable\n",
    "df_impute.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e58d7d",
   "metadata": {},
   "source": [
    "Australia is a land of extremes with temperatures ranging from highs of 40°C in the central desert regions to below freezing in the higher regions of the country's southeast. Sometimes these extremes can be experienced on a single day.\n",
    "\n",
    "##### Reference: https://www.ga.gov.au/scientific-topics/national-location-information/dimensions/climatic-extremes\n",
    "\n",
    "Similarly, if we look at the barometeric pressure, the highest barometric pressure ever recorded was 1083.8mb. While the lowest non-tornadic atmospheric pressure ever measured was 870 hPa (0.858 atm; 25.69 inHg).\n",
    "\n",
    "###### https://en.wikipedia.org/wiki/Atmospheric_pressure\n",
    "\n",
    "###### https://www.guinnessworldrecords.com/world-records/highest-barometric-pressure-\n",
    "\n",
    "Based on these evidences, we conclude that even though we have some extreme values in our dataset they are not entirely wrong or improbable. We decided that we will not delete or impute any of our outliers and continue our our analysis with the data as observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc3d2e",
   "metadata": {},
   "source": [
    "## Data Understanding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Rainfall days today and tomorrow\n",
    "\n",
    "fig, ax =plt.subplots(1,2)\n",
    "print(df_impute.RainToday.value_counts())\n",
    "print(df_impute.RainTomorrow.value_counts())\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.countplot(data=df_impute,x='RainToday',ax=ax[0])\n",
    "sns.countplot(data=df_impute,x='RainTomorrow',ax=ax[1])\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(121)\n",
    "df_impute['RainToday'].value_counts().plot.pie(autopct='%0.2f%%')\n",
    "plt.subplot(122)\n",
    "df_impute['RainTomorrow'].value_counts().plot.pie(autopct='%0.2f%%')\n",
    "plt.show() \n",
    "\n",
    "# Reference: https://www.kaggle.com/fahadmehfoooz/rain-prediction-with-90-65-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6e169",
   "metadata": {},
   "source": [
    "As expected the number of days of Rainfall are far lower than the days of no Rainfall. Its true for both our variables RainToday and RainTomorrow. The number of actual rainfall days are quite similar for both RainToday and RainTomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bb751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms for continuous attributes.\n",
    "\n",
    "fig = plt.figure(figsize = (20,15))\n",
    "ax = fig.gca()\n",
    "df_impute.hist(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ec0b5",
   "metadata": {},
   "source": [
    "The above histograms show the distribution for all the continuous variables from our dataset. It can help us to understand the normality (skewness and data range) for each of the continuous variables. Most of the histograms show us that the variables are normally distributed. Few of the variables like `RainFall`, `Evaporation`, and `WindSpeed9am` are right skewed.\n",
    "\n",
    "`Rainfall` is expected to be skewed as it doesn't rain on most days in Australia.Similarly the `Windspeed` is expectedly skewed too as high winds are not common and most days its low wind speed.\n",
    "\n",
    "`Evaporation` data requires further analysis in context of `Sunshine` and other variables which will be covered in the later sections. `Evaporation` is a factor of Humidity, Temperature, Windspeed and has to be checked in that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind Direction Count: \n",
    "\n",
    "\n",
    "# Wind Direction Count: \n",
    "#plt.xticks(rotation=45)\n",
    "#sns.barplot(x=\"WindGustDir\", hue =\"RainTomorrow\", data=df_impute)\n",
    "\n",
    "df_plot = df_impute.groupby(['RainTomorrow', 'WindGustDir']).size().reset_index().pivot(columns='RainTomorrow', index='WindGustDir', values=0)\n",
    "\n",
    "df_plot.plot(kind='bar', stacked=True)\n",
    "\n",
    "#Source for stacked boxplot: https://stackoverflow.com/questions/50319614/count-plot-with-stacked-bars-per-hue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462ce47",
   "metadata": {},
   "source": [
    "In terms of wind direction attribute, most of the data is recorded at the west wind direction. This is true for both current day and the day after.  That's the reason we see most RainTomorrow with wind direction to West and same for the days with No RainTomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb42008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State count in dataframe\n",
    "location_count = df_impute.State.value_counts().sort_values(ascending=False)\n",
    "location_count.plot(kind='pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c8a89",
   "metadata": {},
   "source": [
    "There are more observations recorded from New South Wales, Victoria, and Western Australia in our dataframe. These states may influence our modeling and analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for Pressure at 9 am and 3 pm\n",
    "df_pressure = df_impute[['Pressure9am', 'Pressure3pm']]\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.set_xlabel(\"va=baseline\")\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df_pressure))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff2b42",
   "metadata": {},
   "source": [
    "The above boxplot indicates `Pressure9am` and `Pressure3pm` are consistant and do not notice much variation throughout the day. `Pressure9am` has slight higher mean value than `Pressure3pm` but the distribution appears to be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b304654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of subset of variables\n",
    "df_num\n",
    "df_boxplot = df_impute[['MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine',\n",
    "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
    "       'Humidity3pm', 'Cloud9am', 'Cloud3pm',\n",
    "       'Temp9am', 'Temp3pm']]\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax.set_xlabel(\"va=baseline\")\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df_boxplot))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24482613",
   "metadata": {},
   "source": [
    "Above boxplot shows how our continuous attributes are distributed in our dataframe. `WindGustSpeed` has the most variations. \n",
    "\n",
    "`Cloud9am` and `Cloud3pm` have the lowest variation and similar distribution. \n",
    "\n",
    "`Evaporation` is highly skewed and has longer whisker. It also has the highest outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c732dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RainToday By State (first one) and RainTomorrow by State (second one)\n",
    "\n",
    "Location_Windir_RainToday = pd.crosstab(df_impute['State'], df_impute['RainToday'])\n",
    "Location_Windir_RainToday.div(Location_Windir_RainToday.sum(1),axis=0).plot.barh(stacked = True)\n",
    "\n",
    "Location_Windir_Raintomorrow = pd.crosstab(df_impute['State'], df_impute['RainTomorrow'])\n",
    "Location_Windir_Raintomorrow.div(Location_Windir_Raintomorrow.sum(1),axis=0).plot.barh(stacked = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205693d",
   "metadata": {},
   "source": [
    "The crosstab charts show that `Queensland` and `Tasmania` states have highest chances of rainfall. Both states shows the most Rainfall days for both `RainToday` and `RainTomorrow`. Several other states are significantly close. However, `Northern Territory` tends to have least rainfall (both RainToday and RainTomorrow).\n",
    "\n",
    "We will further analyze this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall (mm) By State \n",
    "fig = plt.figure(figsize =(7, 4)) \n",
    "\n",
    "# Horizontal Bar Plot \n",
    "dtg = df_impute.groupby(by=df_impute.State)['Rainfall'].mean()\n",
    "\n",
    "\n",
    "dtg.plot(kind = 'bar') \n",
    "\n",
    "\n",
    "groupby_single = df_impute.groupby(['State']).agg({'Rainfall': ['mean', 'min', 'max']})\n",
    "groupby_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e14da",
   "metadata": {},
   "source": [
    "Queensland has received the most amount of rainfall with mean amount of 4.02 mm. South Australia has received the lowest amount with mean value of 1.38 mm. Although we have noticed previously that Tasmania has second most rainfall (close to Queensland) but it has not received a lot compare to some of the other states. New South Wales has received the most amount of daily rainfall and South Australia has received the least amount.\n",
    "\n",
    "Its interesting to see that `South Australia` not only has low mean rainfall, its maximum rainfall amount is significntly lower than other states. Its more than `300%` lower than the maximum rainfall in `New South Wales`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d565265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HeatMap for plot on the correlation matrix using seaborn\n",
    "plt.figure(figsize=(12,12))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True) # one of the many color mappings\n",
    "ax = sns.heatmap(df_impute.corr(), cmap=cmap, square=True, annot=True, fmt='.2f')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd362eef",
   "metadata": {},
   "source": [
    "In this correlation matrix, we notice that most of the correlations are positive. Pressure and Humidity seems to have negative correlations agains other attributes but those are not significant. Here are some of the significant correlations we notice:\n",
    "\n",
    "    MinTemp and MaxTemp\n",
    "\n",
    "    Temp9am and Temp3pm\n",
    "\n",
    "    Humidity9am and Humidity3pm\n",
    "\n",
    "    Cloud9am and Cloud3pm\n",
    "\n",
    "    Pressure9am and Pressure3pm\n",
    "\n",
    "    Humidity9am and Humidity 3pm\n",
    "\n",
    "    WindGustSpeed and WindSpeed9am\n",
    "\n",
    "    WindGustSpeed and WindSpeed3pm\n",
    "\n",
    "    WindSpeed9am and WindSpeed3pm\n",
    "\n",
    "`Humidity` is negatively correlated to `Evaporation` and `Temperature`. Which is significant and appears to be accurate as well. Evaporation is higher during dry conditions. So, the skewness in Evaporation distribution is also impacted by the Humidity in the region.\n",
    "\n",
    "`Sunshine` also is negatively correlated to `Clouds` which is expected as with cloud cover we will not have sunshine. This gives validity to our data and appears to be following the corret trends.\n",
    "\n",
    "One common observation is there are stong correlations between morning and late afternoon values for each weather condition category. Only other signifinant correllation we notive is between Cloud(am/pm) and Humidity(am/pm). That is expected as we can usually notice buildup of humidity as the cloud gathers up before rainfall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba2e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ceff67f",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a5315",
   "metadata": {},
   "source": [
    "As discussed in *Lab 1*, we have acquired the Australian Weather dataset from Kaggle portal. It contains 10 years of weather data collected from many locations across Australia. These are daily weather observations. There are 145,459 observations with 23 attributes in the original dataset. \n",
    "\n",
    "We have chosen `RainTomorrow` (categorical) and `Rainfall` (continuous) as predictor variables. RainTomorrow is a categorical attribute which indicates whether it is going to rain tomorrow - yes or no. Rainfall is a continuous attribute that measures amount of rainfall each of the particular locations have received (in mm). Using our models, we will be able to design an algorithm where the bureau can help to predict rainfall for different regions in Australia.\n",
    "\n",
    "In this Lab 2 assignment, we have measured the accuracy and effectiveness of our model for categorical variable RainTomorrow by using 10-fold cross validation against the confusion matrix measurements like: Precision, Recall and Accuracy. We have explored the methods of logistic regression and support vector machine (SVM) models on our dataset. \n",
    "\n",
    "We have used `scikit-learn` packages for our exploration. We ran logistic regression models with all the available solvers in the `scikit-learn` package and compare the effictiveness and accuracy of the model to predict `RainfallTomorrow`. We also measured the duration of model run from each models to compare model performance and efficiency as well.\n",
    " \n",
    "To get started, we will start with loading all the necessary packages for our analysis. We will start our analysis with `df_impute` which is the imputed dataframe from our last explanatory data analysis Lab 1 project. Using this dataframe will ensure data consistency for all the labs going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from shapely.geometry import Point\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Warnings on final\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68af6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Data\n",
    "df = pd.read_csv(\"weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8399af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ee191",
   "metadata": {},
   "source": [
    "#### Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffcb18",
   "metadata": {},
   "source": [
    "We decided to drop `Date` and `Location` as they are not pertinent to our analysis in this Lab 2 project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca69da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Date', 'Location'], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930ef93",
   "metadata": {},
   "source": [
    "\n",
    "We imputed data in EDA project by substituting the missing and `NaN` values. We are reusing the imputed data from EDA (Lab1) project.\n",
    "Here is the link to the EDA for reference:\n",
    "\n",
    "https://nbviewer.jupyter.org/github/ravisiv/AussieWeatherEDA/blob/c0ba412cb75da21eba386ea9ea39f645ad6af1d0/DS7331_Lab1_Group3_Ravi_Taifur_Seemant_Apurv_Submission.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183efa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Imputed Australia weather data\n",
    "df_impute = pd.read_csv(\"weatherAUS_imputed.csv\")\n",
    "df_impute.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa8301",
   "metadata": {},
   "source": [
    "The imputed data doesn't include any null or missing values. Also, we have dropped the columns like: Date of observation and City Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b411ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impute_num = df_impute.columns[df_impute.dtypes == 'float64']\n",
    "df_impute_cat=df_impute.columns[df_impute.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_impute_num)\n",
    "print(\"Categorical Variables:\", df_impute_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cad24",
   "metadata": {},
   "source": [
    "Before continuing further, we need to check which variables are numeric and which are not. As the models expect numerical variables. We will filter and identify non-numeric variables.\n",
    "\n",
    "`WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` and `RainTomorrow`are not numeric. Here `RainTomorrow` is our response variable. we handle the other variables with hot-one-encoding later in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed27653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the original data\n",
    "df_model = df_impute.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7640a88",
   "metadata": {},
   "source": [
    "Creating a new DataFrame `df_model` for modeling to avoid any changes to the original dataset `df_impute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable to Identify if it RainToday\n",
    "\n",
    "df_model[\"IsRainToday\"] = df_impute['RainToday']\n",
    "\n",
    "# Replacing No with 0 and Yes with 1.\n",
    "\n",
    "df_model['IsRainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c1422",
   "metadata": {},
   "source": [
    "Assigning `0` to No values and `1` to Yes values in `RainToday` (Changed to `IsRainToday`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_impute\", df_impute.shape)\n",
    "print(\"df_model\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the values to check if the data looks good\n",
    "\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbca128",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c7712",
   "metadata": {},
   "source": [
    "Before we create our models, we need to format our attributes. We are converting `RainToday` and `RainTomorrow` into numeric variables to `0` and `1`. We also decided to go ahead with one-hot-encoding `WindGustDir`, `WindDir9am`, and `WindDir3pm` attributes based on the direction of the wind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1800d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding using dummies\n",
    "\n",
    "gust_df = pd.get_dummies(df_model.WindGustDir,prefix='GustDir', drop_first= True)\n",
    "wind3pm_df = pd.get_dummies(df_model.WindDir3pm,prefix='Wind3pm', drop_first= True)\n",
    "wind9am_df = pd.get_dummies(df_model.WindDir9am,prefix='Wind9am' , drop_first= True)\n",
    "df_model = pd.concat((df_model,gust_df, wind3pm_df, wind9am_df),axis=1) # add back into the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c39f8",
   "metadata": {},
   "source": [
    "We decided to do one-hot-encoding using dummies function as machine learning algorithms and models requires numerical values for both input and output attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54222d48",
   "metadata": {},
   "source": [
    "Since the dummies function creates a variable for each unique value, we are dropping the first variable to avoid multicollinearity among the variables as the value for the last variable can be interpreted from the values for other variables created as part of one-hot encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical columns\n",
    "\n",
    "df_model = df_model.drop(['WindDir3pm', 'WindDir9am', 'WindGustDir', 'RainToday'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b5c09",
   "metadata": {},
   "source": [
    "After conversions, we are removing these categorical attributes to avoid duplicates as we have those data in numerical format. We are added the newly formatted attributes and rest of the continuous attributes into a new dataframe - df_model. We will use the new dataframe for modeling.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0fb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if Yes is replaced as 1\n",
    "\n",
    "print(\"Are there 1's and 0's in the RainToday column?\", \n",
    "      (df_model['IsRainToday'].sum() > 0) and (df_model['IsRainToday'].sum() < len(df_model['IsRainToday'])))\n",
    "\n",
    "#Non zero output means there is a mixture of 1's and 0's\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433c629",
   "metadata": {},
   "source": [
    "Checking if the data imputation happened accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_num = df_model.columns[df_model.dtypes != 'object' ]\n",
    "df_model_cat=df_model.columns[df_model.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_model_num)\n",
    "print(\"Categorical Variables:\", df_model_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03081b6",
   "metadata": {},
   "source": [
    "Check if all the numerical variables are accurately created and if we still have any non-numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d694e12",
   "metadata": {},
   "source": [
    "Assigning the `RainTomorrow` as our response variable (y) and all other variables include one-hot-encoded values as X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7f8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=df_model[df_model_num]\n",
    "y = df_model.RainTomorrow\n",
    "print('features shape:', X.shape) \n",
    "print('target shape:', y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e2f23",
   "metadata": {},
   "source": [
    "#### Response Variables\n",
    "For our dataset, we are using two response variables:\n",
    "\n",
    "1. `RainTomorrow` - Categorical variable for classification\n",
    "2. `Rainfall` - Continuous variable for regression\n",
    "\n",
    "We are going to introduce additional variable in our dataset:\n",
    "\n",
    "`RainfallAmount` - Categorical variable for rainfall classification. We have covered this in more detail in another section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d2ea6",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5496a797",
   "metadata": {},
   "source": [
    "We will be using scaled data for our models. We have used the scaling feature as part of our customed function for running our classifications and regression models. We will discuss more in details in the modeling sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6400b7f",
   "metadata": {},
   "source": [
    "#### New Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff373d23",
   "metadata": {},
   "source": [
    "We are adding a new classification feature called `RainfallAmount` which has four values - `None` (0), `Low`(1), `Moderate`(2) and `High`(3). We are creating this feature from `Rainfall` feature from our dataframe. The data is numerical due to the requirement of the execution of the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Feature - RainfallAmount\n",
    "\n",
    "def rain_classifier(row):\n",
    "    if row[\"Rainfall\"] > 30:\n",
    "        return 3\n",
    "    elif row[\"Rainfall\"] > 10 and row[\"Rainfall\"] < 30:\n",
    "        return 2\n",
    "    elif row[\"Rainfall\"] > 1 and row[\"Rainfall\"] < 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_model[\"RainfallAmount\"] = df_impute.apply(rain_classifier, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.RainfallAmount.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f373a6",
   "metadata": {},
   "source": [
    "#### Down-sampling\n",
    "\n",
    "Due to large amount of data and multiple models being evaluated in this project, our computers are not able to handle the load and have been crashing which is leading to increased processing time and repetitive work. To avoid this situation we have made few changes in our models.\n",
    "\n",
    "1. Down Sample the data based on the `RainToday`\n",
    "2. Reduced the various combinations of hyper tuning parameters to preserve the memory and processing power.\n",
    "3. Reduced the number of additional models we were running as part of exceptional work, like: XGBOOST, Linear SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampling before we run our models \n",
    "df_model_copy = df_model[df_model.IsRainToday  == np.random.choice(df_model['IsRainToday'].unique())].reset_index(drop=True)\n",
    "df_model = df_model_copy.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c1b04",
   "metadata": {},
   "source": [
    "### Data Distribution\n",
    "\n",
    "Check if the data distribution is balanced or not for the response variable `RainTomorrow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "\n",
    "df_impute['RainTomorrow'].value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\n",
    "plt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63833e9",
   "metadata": {},
   "source": [
    "As expected, we see the data for `RainTomorrow` is imbalanced. Majority of the data is for `No` rain vs. `Yes` for `RainTomorrow`.\n",
    "\n",
    "We can observe that the presence of `0` and `1` is almost in the `78:22` ratio. We will be cognizant of the fact that our model may be not very effective if we don't solve for imbalance. We will discuss and adjust for this imbalance in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_copy = df_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb86631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27925d9b",
   "metadata": {},
   "source": [
    "The above dataframe has float64, object, int64, and uint8 data formats. Float64, int64, and uint8 are all numerical data type. Object is a string data type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c3f3d",
   "metadata": {},
   "source": [
    "#### Response Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b660ad6",
   "metadata": {},
   "source": [
    "We have three response features in our current dataframe. They are `RainTomorrow`, `Rainfall`, and `RainfallAmount`. `RainTomorrow` and `RainfallAmount` are for our classification models. `Rainfall` is used for continuous regression models. Our primary focus is on `RainTomorrow` and `RainfallAmount` as the prediction for `Rainfall` is not very accurate. We will present the accuracy of this feature in later section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61701aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e256d",
   "metadata": {},
   "source": [
    "We ran summary statistics on the final model dataset. We can see the various satistical summary of the features. We see some large variations in the dataset like Evaportaion ranges from 0 to 145, Rainfall varies from 0 mm to 371 mm. Which are huge variation but as determined during `EDA` (Lab 1) those are not outliers and for our analysis we will consider then as valid observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1eff39",
   "metadata": {},
   "source": [
    "# Tai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308db2c",
   "metadata": {},
   "source": [
    "### Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a815c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a00428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea344b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6abfa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827715c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e25e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e206407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcc3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c848e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb61c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fec339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8084ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51610e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450e68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4028b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a8f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fcc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e7f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c5b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b61b2337",
   "metadata": {},
   "source": [
    "# Seemant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f445c4",
   "metadata": {},
   "source": [
    "### Optics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b886c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9348c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad877c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73e834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a110d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f59809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec63da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e785637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6b885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18ed58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecc4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94c7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0a9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5892a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6c594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d8925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51566134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f9312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc85e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "622d85d9",
   "metadata": {},
   "source": [
    "# Ravi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927916d1",
   "metadata": {},
   "source": [
    "### K-Means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfe0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846f257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602087d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f09cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7078a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45184f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ed0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ad1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01576e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb5db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebc40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3f63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6674c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee977a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abce99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8f729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882dfb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc423c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1270c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8780bebc",
   "metadata": {},
   "source": [
    "# Apurv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db720c",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Government data for State and Latitude/Longitude lookup to create a geography dataframe for Weather Australia\n",
    "worldcities = pd.read_csv(\"worldcities.csv\", header=[0], encoding = \"ISO-8859-1\", engine='python')\n",
    "worldcities = worldcities[(worldcities.country == \"Australia\")]\n",
    "worldcities.rename(columns={'city': 'Location', 'lat': 'Latitude', 'lng': 'Longitude', 'admin_name': 'State'}, inplace=True)\n",
    "worldcities = worldcities.drop(['city_ascii','country','iso2','iso3','capital','population','id'],axis=1)\n",
    "df_impute_temp = df_impute\n",
    "df_geo = pd.merge(df_impute_temp, worldcities, how=\"left\", on=[\"Location\"])\n",
    "df_geo.head()\n",
    "\n",
    "\n",
    "# Reference for World Cities data : https://simplemaps.com/data/world-cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419d58ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-34c9ea845bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mpl_toolkits\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = (14,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72445a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0202b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.4.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (2.4.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (8.3.1)\r\n",
      "Requirement already satisfied: numpy>=1.16 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.21.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2888e6da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '__version__' from 'matplotlib' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e48d66f76db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmpl_toolkits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mpl_toolkits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/mpl_toolkits/basemap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_matplotlib_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0minspect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcleandoc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdedent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '__version__' from 'matplotlib' (unknown location)"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "mpl_toolkits = importlib.import_module('mpl_toolkits')\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82ebb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c834e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fde8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f5fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fcb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1647c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08369b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6c0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ec578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ab871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9317b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b006c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95cf2acf",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ea188",
   "metadata": {},
   "source": [
    "Our models are primarily designed for meteorologists. At the same time, these can be very useful for event organizers in cities across the country – mostly important for outdoor events. The favored model can be also useful for government organizations like military (Navy). The model not only predicts if it’s going to Rain Tomorrow (next day), It also predicts the amount of `Rainfall` for today in amount of rain fell (mm) as well as classifies as `Low`, `Medium` and `High`. Due to our ability to give the data which is easily interpretable for everyone, this is useful for the everyone. It can be integrated by the weather channels and apps as well. \n",
    "\n",
    "\n",
    "The model’s value can be measured in terms of its accuracy; higher the accuracy better the value of the model over existing models in use. Some parties may value our models higher than others depending upon how important the accuracy of prediction of Rainfall for their business or area of operations.\n",
    "\n",
    "\n",
    "Our models can be integrated to the existing feed of weather related data which is useful for our model to predict accurately. This data is easily available from government websites. \n",
    "\n",
    "\n",
    "Data from those sources can be integrated with our model and build an APIs for anyone to consume and monetize by the count of API calls.\n",
    "\n",
    "\n",
    "As we know weather is an ever-changing event and due to climate change, predictive models needs to evolve continuously. The validity of our models need to be tested against the recorded information and improve our models daily with the new data (models can be built within hours).\n",
    "\n",
    "\n",
    "Additional data points may be required like seasons, time of the year, impact of natural events like cyclones/hurricanes, wild fires, El Niño and La Niña effects etc.\n",
    "\n",
    "\n",
    "Overall, our classification models will be more useful since those clearly indicates weather condition for all the interested parties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4eb57",
   "metadata": {},
   "source": [
    "## Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf51f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f5172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a680e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820c927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c634f17b",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70777f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
