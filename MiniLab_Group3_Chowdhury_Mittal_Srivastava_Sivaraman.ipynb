{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1746415f",
   "metadata": {},
   "source": [
    "# Mini Lab : Logistic Regression and Support Vector Machine\n",
    "\n",
    "### Group 3 - Members:\n",
    "\n",
    "_Tai Chowdhury_<br>\n",
    "_Apurv Mittal_<br>\n",
    "_Ravi Sivaraman_<br>\n",
    "_Seemant Srivastava_<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e576f7",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cd903",
   "metadata": {},
   "source": [
    "As discussed in Lab 1, we have acquired the Australian Weather dataset from Kaggle portal. It contains 10 years of weather data collected from many locations across Australia. These are daily weather observations. There are 145,459 observations with 23 attributes in the original dataset. \n",
    "\n",
    "We have chosen RainTomorrow (categorical) and Rainfall (continuous) as predictor variables. RainTomorrow is a categorical attribute which indicates whether it is going to rain tomorrow - yes or no. Rainfall is a continuous attribute that measures amount of rainfall each of the particular locations have received (in mm). Using our models, we will be able to design an algorithm where the bureau can help to predict rainfall for different regions in Australia.\n",
    "\n",
    "In this Lab 2 assignment, we have measured the accuracy and effectiveness of our model for categorical variable RainTomorrow by using 10-fold cross validation against the confusion matrix measurements like: Precision, Recall and Accuracy. We have explored the methods of logistic regression and support vector machine (SVM) models on our dataset. \n",
    "\n",
    "We have used `scikit-learn` packages for our exploration. We ran logistic regression models with all the available solvers in the `scikit-learn` package and compare the effictiveness and accuracy of the model to predict `RainfallTomorrow`. We also measured the duration of model run from each models to compare model performance and efficiency as well.\n",
    " \n",
    "To get started, we will start with loading all the necessary packages for our analysis. We will start our analysis with `df_impute` which is the imputed dataframe from our last explanatory data analysis Lab 1 project. Using this dataframe will ensure data consistency for all the labs going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from shapely.geometry import Point\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8247b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Warnings on final\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229a0c7",
   "metadata": {},
   "source": [
    "## Imputed Data\n",
    "We imputed data in EDA and reusing imputed data from EDA (Lab1) project.\n",
    "Here is the link to the EDA\n",
    "\n",
    "https://nbviewer.jupyter.org/github/ravisiv/AussieWeatherEDA/blob/c0ba412cb75da21eba386ea9ea39f645ad6af1d0/DS7331_Lab1_Group3_Ravi_Taifur_Seemant_Apurv_Submission.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Imputed Australia weather data\n",
    "df_impute = pd.read_csv(\"weatherAUS_imputed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  View the top rows of the data imported\n",
    "df_impute.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fb065",
   "metadata": {},
   "source": [
    "The imputed data doesn't include any null or missing values. Also, we have dropped the columns like: Date of observation and City Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb585c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impute_num = df_impute.columns[df_impute.dtypes == 'float64']\n",
    "df_impute_cat=df_impute.columns[df_impute.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_impute_num)\n",
    "print(\"Categorical Variables:\", df_impute_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6fa55",
   "metadata": {},
   "source": [
    "Before continuing further, we need to check which variables are numeric and which are not. As the models expect numerical variables. We will filter and identify non-numeric variables.\n",
    "\n",
    "`WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` and `RainTomorrow`are not numeric. Here `RainTomorrow` is our response variable. we handle the other variables with hot-one-encoding later in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the original data\n",
    "df_model = df_impute.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a635ed8",
   "metadata": {},
   "source": [
    "Creating a new DataFrame `df_model` for modeling to avoid any changes to the original dataset `df_impute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable to Identify if it RainToday\n",
    "\n",
    "df_model[\"IsRainToday\"] = df_impute['RainToday']\n",
    "\n",
    "# Replacing No with 0 and Yes with 1.\n",
    "\n",
    "df_model['IsRainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0a055",
   "metadata": {},
   "source": [
    "Assigning `0` to No values and `1` to Yes values in `RainToday` (Changed to `IsRainToday`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_impute\", df_impute.shape)\n",
    "print(\"df_model\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the values to check if the data looks good\n",
    "\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a25167",
   "metadata": {},
   "source": [
    "We can observe that the presence of “0” and “1” is almost in the 78:22 ratio. So there is a class imbalance and we have to deal with it. To fight against the class imbalance, we will use here the oversampling of the minority class. Since the size of the dataset is quite small, majority class subsampling wouldn’t make much sense here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f90fc0",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe083dc",
   "metadata": {},
   "source": [
    "Before we create our models, we need to format our attributes. We are converting `RainToday` and `RainTomorrow` into numeric variables to `0` and `1`. We also decided to go ahead with one-hot-encoding `WindGustDir`, `WindDir9am`, and `WindDir3pm` attributes based on the direction of the wind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding using dummies\n",
    "\n",
    "gust_df = pd.get_dummies(df_model.WindGustDir,prefix='GustDir')\n",
    "wind3pm_df = pd.get_dummies(df_model.WindDir3pm,prefix='Wind3pm')\n",
    "wind9am_df = pd.get_dummies(df_model.WindDir9am,prefix='Wind9am')\n",
    "df_model = pd.concat((df_model,gust_df, wind3pm_df, wind9am_df),axis=1) # add back into the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc00c0e",
   "metadata": {},
   "source": [
    "We decided to do one-hot-encoding using dummies function as machine learning algorithms and models requires numerical values for both input and output attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical columns\n",
    "\n",
    "df_model = df_model.drop(['WindDir3pm', 'WindDir9am', 'WindGustDir', 'RainToday'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b973430",
   "metadata": {},
   "source": [
    "After conversions, we are removing these categorical attributes to avoid duplicates as we have those data in numerical format. We are added the newly formatted attributes and rest of the continuous attributes into a new dataframe - df_model. We will use the new dataframe for modeling.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782dd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if Yes is replaced as 1\n",
    "\n",
    "print(\"Are there 1's and 0's in the RainToday column?\", \n",
    "      (df_model['IsRainToday'].sum() > 0) and (df_model['IsRainToday'].sum() < len(df_model['IsRainToday'])))\n",
    "\n",
    "#Non zero output means there is a mixture of 1's and 0's\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634fff9",
   "metadata": {},
   "source": [
    "Checking if the data imputation happened accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_num = df_model.columns[df_model.dtypes == 'float64']\n",
    "df_model_cat=df_model.columns[df_model.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_model_num)\n",
    "print(\"Categorical Variables:\", df_model_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20202992",
   "metadata": {},
   "source": [
    "Check if all the numerical variables are accurately created and if we still have any non-numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f9a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=df_model[df_model_num]\n",
    "y = df_model.RainTomorrow\n",
    "print('features shape:', X.shape) \n",
    "print('target shape:', y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea30c6",
   "metadata": {},
   "source": [
    "Assigning the `RainTomorrow` as our response variable (y) and all other variables include one-hot-encoded values as X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607eb76",
   "metadata": {},
   "source": [
    "### Data Distribution\n",
    "\n",
    "Check if the data distribution is balanced or not for the response variable `RainTomorrow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26445414",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "df_model['RainTomorrow'].value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\n",
    "plt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e7da9",
   "metadata": {},
   "source": [
    "As expected, we see the data for `RainTomorrow` is imbalanced. Majority of the data is for `No` rain vs. `Yes` for `RainTomorrow`.\n",
    "\n",
    "We can observe that the presence of `0` and `1` is almost in the `78:22` ratio. We will be cognizant of the fact that our model may be not very effective if we don't solve for imbalance. We will discuss and adjust for this imbalance in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30623463",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244c582",
   "metadata": {},
   "source": [
    "### Creating Logistic Regression Function\n",
    "\n",
    "The function below creates models with various parameters supported by LogisticRegression. There are various combinations of\n",
    "1. `Model Type`\n",
    "2. `Class Weight`\n",
    "3. `Solver`\n",
    "4. `C`\n",
    "5. `Penalty`\n",
    "6. `Iterations`\n",
    "\n",
    "This function runs for specific Solver, C, Penalty but takes various iterations, and we used 10 iterations.\n",
    "When this function called with Solver, C, Penalty, the function runs for BOTH Class Weight, `balanced` and `None`. The result of both `balanced` and `None` is stored as single row in a dataframe.\n",
    "They are stored in the same row to verify how `balanced` and `None` compares.\n",
    "\n",
    "##### Model Type\n",
    "\n",
    "1. `Shuffle`\n",
    "2. `Stratified`\n",
    "\n",
    "`ShuffleSplit`  — is similar to `Cross Validation` where we can specify the percentage of split for train and test data. However, in regular cross-validation, the data is not split randomly, so, it is good to shuffle the targets before applying the `cross-validation`.\n",
    "\n",
    "\n",
    "`Stratified` — CV technique is very useful with unbalanced dataset. As discussed above our dataset is not balanced and rightly so, we don't expect it to rain and no-rain days to be equal in Australia. The data is expected to be unbalanced and expected to be such in future as well. So using stratified sampling techniques gives us the ability to preserve the proportion of the Rain days vs non-rain days in our dataset. We can be confident that the Train and Test split data is not leaving out important information like entire dataset is of `No` rain days which will give highly inaccurate output eventhough the accuracy might be maintained. \n",
    "\n",
    "In `Stratified` sampling, the data is k-1 split in favor of Train vs Test data.\n",
    "\n",
    "\n",
    "References: \n",
    "https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d\n",
    "https://mclguide.readthedocs.io/en/latest/sklearn/cv.html\n",
    "\n",
    "\n",
    "##### Solver Options\n",
    "Scikit-learn comes with five different solver options. Each solver minimize the cost function. Here are the five options:\n",
    "\n",
    "`newton-cg` — A newton method. Newton methods use an exact Hessian matrix. It's slow for large datasets, because it computes the second derivatives.\n",
    "\n",
    "`lbfgs` — Stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno. It approximates the second derivative matrix updates with gradient evaluations. It stores only the last few updates, so it saves memory. It isn't super fast with large data sets.\n",
    "\n",
    "`liblinear` — Library for Large Linear Classification. Uses a coordinate descent algorithm. Coordinate descent is based on minimizing a multivariate function by solving univariate optimization problems in a loop. In other words, it moves toward the minimum in one direction at a time. It performs pretty well with high dimensionality. It does have a number of drawbacks. It can get stuck, is unable to run in parallel, and can only solve multi-class logistic regression with one-vs.-rest.\n",
    "\n",
    "`sag` — Stochastic Average Gradient descent. A variation of gradient descent and incremental aggregated gradient approaches that uses a random sample of previous gradient values. Fast for big datasets.\n",
    "\n",
    "`saga` — Extension of sag that also allows for L1 regularization. Should generally train faster than sag.\n",
    "\n",
    "Reference for above definitions: https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "\n",
    "##### Penalty\n",
    "\n",
    "Used to specify the norm used in the penalization. The `newton-cg`’, `sag` and `lbfgs` solvers support only `l2` penalties. `elasticnet` is only supported by the `saga` solver. If `none` (not supported by the `liblinear` solver), no regularization is applied.\n",
    "\n",
    "`L1`  or `Lasso` — (`LASSO` is Least Absolute Shrinkage and Selection Operator) uses a penalized least squares approach that squeezes the regression coefficients to 0 when the penalty is large. The algorithm starts with a large penalty and gradually relaxes the penalty to allow for a single variable to be added into the model (the coefficient is no longer `0`). `LASSO` uses `L1` method.\n",
    "\n",
    "`L2` or `Ridge` — adds a penalty equal to the square of the magnitude of coefficients. `L2` will not yield sparse models and all coefficients are shrunk by the same factor (not eliminated). Ridge regression and SVMs use this method.\n",
    "\n",
    "`elasticnet`  — Procedure identical to `LASSO` however the penalty is different. `elasticnet` uses a combination of both the `LASSO` penalty as well as the `RIDGE` regression penalty. It combine both `L1` & `L2` methods.\n",
    "\n",
    "`none` — No penalty\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Reference for Penalty definitions: https://www.statisticshowto.com/regularization/\n",
    "\n",
    "##### C\n",
    "`C` is cost, which we run with `1.0`, `10.0` and `100.0` for each combination of Solver and Penalty.\n",
    "\n",
    "If the solver doesn't support the penalty, those are skipped.\n",
    "\n",
    "\n",
    "The dataframe stores:\n",
    "\n",
    "1. `Model Type`\n",
    "2. `Solver`\n",
    "3. `C`\n",
    "4. `Penalty`\n",
    "5. `Iteration`\n",
    "6. `AccuracyNone`\n",
    "7. `MacroAvgPrecisionNone`\n",
    "7. `WeightedAvgPrecisionNone`\n",
    "8. `DiffMacro/WeightedNone`\n",
    "9. `fprNone`\n",
    "10. `tprNone`,\n",
    "11. `AccuracyBalanced`\n",
    "12. `MacroAvgPrecisionBalanced`\n",
    "13. `WeightedAvgPrecisionBalanced`\n",
    "14. `DiffMacro/WeightedBalanced`\n",
    "15. `fprBalanced`\n",
    "16. `tprBalanced`\n",
    "17. `Classes`\n",
    "\n",
    "`DiffMacro/WeightedBalanced` is the difference between the Presicion of `macro avg` and `Weighted`. Smaller the value, more closer they are.\n",
    "\n",
    "`fprNone` and `tprNone` are stored to run ROC curve.\n",
    "\n",
    "`Classes` is `Yes` and `No` as an array, which are used for labels for plots. They are constant for all rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#model stats dataframe columns for class weight balanced and None\n",
    "model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \n",
    "                         \"MacroAvgPrecisionNone\",\"MacroAvgRecallNone\", \"MacroAvgf1-scoreNone\",\n",
    "                         \"WeightedAvgPrecisionNone\", \"WeightedAvgRecallNone\", \"WeightedAvgf1-scoreNone\",\n",
    "                         \"DiffMacro/WeightedNone\",\"fprNone\", \"tprNone\",\"lr_clfNone\",\n",
    "                         \"AccuracyBalanced\",\n",
    "                         \"MacroAvgPrecisionBalanced\",\"MacroAvgRecallBalanced\", \"MacroAvgf1-scoreBalanced\",\n",
    "                         \"WeightedAvgPrecisionBalanced\", \"WeightedAvgRecallBalanced\", \"WeightedAvgf1-scoreBalanced\",\n",
    "                         \"DiffMacro/WeightedBalanced\",\"fprBalanced\", \"tprBalanced\", \"lr_clfBalanced\",\n",
    "                         \"Classes\", \"Time\"\n",
    "                        ]\n",
    "\n",
    "\n",
    "def create_log_models(model_type,df, iterations,penalty, C, solver):\n",
    "    class_weight = ['balanced', None]\n",
    "    \n",
    "    #Create logreg object for both class weights\n",
    "    lr_clf_balanced = LogisticRegression(penalty=penalty, C=C, class_weight='balanced', solver=solver) \n",
    "    lr_clf_none = LogisticRegression(penalty=penalty, C=C, class_weight=None, solver=solver)\n",
    "    \n",
    "    #Store both objects in a dict for later retrival\n",
    "    lr_clf_dict = { \n",
    "                    \"balanced\": lr_clf_balanced,\n",
    "                    \"None\": lr_clf_none\n",
    "                    }\n",
    "\n",
    "    num_cv_iterations = iterations\n",
    "\n",
    "   \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    num_instances = len(y)\n",
    "    \n",
    "    cv_data = None\n",
    "    if model_type == \"shuffle\":\n",
    "        cv_data = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                             test_size  = 0.2, random_state = 123)\n",
    "\n",
    "    elif model_type == \"stratified\":\n",
    "        cv_data = StratifiedKFold(n_splits=iterations, random_state=123, shuffle=True)\n",
    "        cv_data.get_n_splits(X, y)\n",
    "       \n",
    "    #Initialize variables\n",
    "    iter_num=0\n",
    "    rows = []\n",
    "    stats_dict = {}\n",
    "    target_names = ['No', 'Yes']\n",
    "    classes = None    \n",
    "    scl_obj = StandardScaler()\n",
    "    lr_clf = None\n",
    "    # Run for balanced first with same model and then None with same model\n",
    "    # store the results in same row of dataframe\n",
    "    # This helps to compare none and balanced macro avg\n",
    "   \n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        starttime = timer()\n",
    "        for cw in class_weight:\n",
    "            X_train = X[train_indices]\n",
    "            y_train = y[train_indices]\n",
    "        \n",
    "            scl_obj.fit(X_train)\n",
    "\n",
    "            X_test = X[test_indices]\n",
    "            y_test = y[test_indices]\n",
    "            \n",
    "            #Get the logistic regression object for the current cw class weight\n",
    "            key = None\n",
    "            if cw == None:\n",
    "                key = \"None\"\n",
    "            else:\n",
    "                key = cw\n",
    "                    \n",
    "            lr_clf = lr_clf_dict[key]\n",
    "            \n",
    "            try:\n",
    "                X_train_scaled = scl_obj.transform(X_train) \n",
    "                X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "                lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "                y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "                classes = lr_clf.classes_\n",
    "\n",
    "                acc = mt.accuracy_score(y_test,y_hat)\n",
    "                conf = mt.confusion_matrix(y_test,y_hat)\n",
    "                \n",
    "                class_report = classification_report(y_test, y_hat, target_names, output_dict=True)\n",
    "                \n",
    "                # Macro avg stats\n",
    "                macro_avg_precision = class_report[\"macro avg\"][\"precision\"]\n",
    "                macro_avg_recall = class_report[\"macro avg\"][\"recall\"]\n",
    "                macro_avg_f1_score = class_report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "                #Weighted avg stats\n",
    "                weighted_avg_precision = class_report[\"weighted avg\"][\"precision\"]\n",
    "                weighted_avg_recall = class_report[\"weighted avg\"][\"recall\"]\n",
    "                weighted_avg_f1_score = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "                # Create ROC Curve\n",
    "                y_test_01 = np.where(y_test ==\"Yes\", 1, [0])\n",
    "                y_hat_01 = np.where(y_hat ==\"Yes\", 1, [0])\n",
    "\n",
    "                fpr, tpr, threshold = metrics.roc_curve(y_test_01, y_hat_01)\n",
    "                \n",
    "                #Create a dict of these stats for class weight\n",
    "                #dict will contain stats for balanced on one run, None for the next run\n",
    "                \n",
    "                stats_dict[key] = [acc, \n",
    "                          macro_avg_precision, macro_avg_recall, macro_avg_f1_score,\n",
    "                          weighted_avg_precision,weighted_avg_recall,weighted_avg_f1_score,\n",
    "                          abs(weighted_avg_precision-macro_avg_precision),         \n",
    "                          fpr, tpr, lr_clf ]\n",
    "\n",
    "                print(model_type, solver,cw,C,penalty,iter_num,\"✅\")\n",
    "            except Exception as e:\n",
    "                #print('Error:', str(e))\n",
    "                raise\n",
    "            #end try block  \n",
    "        #end first for loop\n",
    "        #When cursor comes here, model has ran for both None and balanced\n",
    "        #Create a single row of lists combining none and balanced \n",
    "        endtime = timer()\n",
    "        time_taken = endtime - starttime\n",
    "        row = [model_type, solver,C,penalty,iter_num] + stats_dict[\"None\"] + stats_dict['balanced'] + [classes, time_taken ]\n",
    "        rows.append(row)\n",
    "        iter_num+=1\n",
    "    #end next for loop\n",
    "    \n",
    "    #Create a dataframe with the model stats \n",
    "    df_ret = pd.DataFrame(rows, columns = model_stats_columns)\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1acd42",
   "metadata": {},
   "source": [
    "### Running Logistic Regression with various options\n",
    "\n",
    "The code below runs all the combinations of `Penalty`, `Solver`, `C` and shuffle types and stores in dataframe for further analysis later on.\n",
    "\n",
    "If any combinations of `Solver` and `Penalty` isn't supported by Logistic Regression, they are skipped.\n",
    "\n",
    "`iteration` — 10 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eea1d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "penalty=['l1','l2', 'elasticnet', 'none']\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "model_type = ['shuffle', 'stratified']\n",
    "C=[1.0, 10.0, 100.0]\n",
    "iterations = 10\n",
    "\n",
    "#penalty=['l2']\n",
    "#solver = ['liblinear']\n",
    "#model_type = [\"stratified\"]\n",
    "#C=[1.0]\n",
    "#iterations = 2\n",
    "\n",
    "model_perf_df = pd.DataFrame(columns= model_stats_columns)\n",
    "\n",
    "# Run all combinations of penalty, solver, model_type, C\n",
    "# Create a giant dataframe\n",
    "# Each row of dataframe contains stats for each combination of penalty, solver, model_type,C\n",
    "# same row contains the stats for both None and balanced class weight\n",
    "# so we can compare the balanced and None\n",
    "# and pick the model whose diff in precision (for macro avg) is lowest with highest accuracy\n",
    "# This model is the closest to real world\n",
    "\n",
    "for pen in penalty:\n",
    "    for c_index in C:\n",
    "        for solv in solver:\n",
    "            for mdl_type in model_type:\n",
    "                try:\n",
    "\n",
    "                    df_ret = create_log_models(model_type=mdl_type,df=df_model,iterations=iterations, penalty=pen, C=c_index, solver=solv)\n",
    "                    \n",
    "                    model_perf_df = model_perf_df.append(df_ret, ignore_index=True)\n",
    "                    \n",
    "                    #Model deletes RainTomorrow from dataframe\n",
    "                    #put it back from imputed data to run for another set of model\n",
    "                    df_model[\"RainTomorrow\"] = df_impute[\"RainTomorrow\"].values\n",
    "                except Exception as e:\n",
    "                    #print(\"Error in running\", str(e))\n",
    "                    continue\n",
    "                            \n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35335d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_model_copy = model_perf_df.copy()\n",
    "#model_perf_df = perf_model_copy.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1acbda",
   "metadata": {},
   "source": [
    "In these models we used two different methodologies for cross validation namely `Shuffle` and `Stratified`.\n",
    "\n",
    "`ShuffleSplit`  — is similar to `Cross Validation` where we can specify the percentage of split for train and test data. However, in regular cross-validation, the data is not split randomly, so, it is good to shuffle the targets before applying the `cross-validation`.\n",
    "\n",
    "\n",
    "`Stratified` — CV technique is very useful with unbalanced dataset. As discussed above our dataset is not balanced and rightly so, we don't expect it to rain and no-rain days to be equal in Australia. The data is expected to be unbalanced and expected to be such in future as well. So using stratified sampling techniques gives us the ability to preserve the proportion of the Rain days vs non-rain days in our dataset. We can be confident that the Train and Test split data is not leaving out important information like entire dataset is of `No` rain days which will give highly inaccurate output eventhough the accuracy might be maintained. \n",
    "\n",
    "In `Stratified` sampling, the data is k-1 split in favor of Train vs Test data.\n",
    "\n",
    "\n",
    "References: \n",
    "https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d\n",
    "https://mclguide.readthedocs.io/en/latest/sklearn/cv.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0785a1",
   "metadata": {},
   "source": [
    "After running the big `Logistic Regression` model for several combinations. We got output for 660 models.\n",
    "\n",
    "To compare the outputs and make it easier to identify the most appropriate model in terms of accuracy, precision and other factors applicable for Machine Learning models. We decided to put a list of subset of variables including:\n",
    "\n",
    "`Model` — specifies which technique was used for the logistic model. In this case its between ShuffleSplit and Stratified.\n",
    "\n",
    "`AccuracyNone` —  specifies the Accuracy observed by the model where data was not `balanced`.\n",
    "\n",
    "`DiffMacro/WeightedNone` — takes the `Macro Average` and `Weighted Average` of `Precision` from the classification matrix and calculates the difference. The reason to calculate the difference is to check how much variation is in the Precision values based on how the data is split. More details about Precision and Averages is provided below. This variation is calculated on the non `balanced` data.\n",
    "\n",
    "`AccuracyBalanced` — specifies the Accuracy observed by the model where data was `balanced`.\n",
    "\n",
    "`DiffMacro/WeightedBalanced` — same as above this variable calculates the difference between Macro and Weighted Average of Precision of a `balanced` data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d18ba7",
   "metadata": {},
   "source": [
    "### Important Terms\n",
    "\n",
    "`Accuracy` — is a ratio of correctly predicted observation to the total observations. It is a very important aspect to define the success of a model but just the measurement on its own can be deceiving if the observations are not equal for each class. In such cases we might be predicting accurately for one particular class with large observation and may not do very well for other classes.\n",
    "\n",
    "`Precision` — is the ratio of correctly predicted positive observations to the total predicted positive observations. Precision = TP/TP+FP  \n",
    "\n",
    "`Weighted Average` — can be calculated on various output variables of the classification report like Precision, Recall, f1-score. As the name suggests it gives the weighted average of the parameter based on the number of observations or values for each class.\n",
    "\n",
    "`Macro Average` — similar to weighted average, macro average can also be calculated on various output variables of the classification report like Precision, Recall, f1-score. However, the similarity ends here as unlike weighted average, we don't use weights based on the number of observations, rather equal weights are given to each class to calcualte the value. This tells us if the Precision is as good if the dataset was balanced.\n",
    "\n",
    "References:\n",
    "https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "https://datascience.stackexchange.com/questions/65839/macro-average-and-weighted-average-meaning-in-classification-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02993cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "import seaborn\n",
    "\n",
    "def draw_scatter(df, x,y, title=None):\n",
    "\n",
    "    seaborn.set(style='ticks')\n",
    "    cv_type = ['shuffle', 'stratified']\n",
    "\n",
    "    #fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61, ylim=(0,1))\n",
    "    fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)\n",
    "    fg.map(pyplot.scatter, x, y ).add_legend()\n",
    "    #fg.suptitle(title)\n",
    "\n",
    "    for i, ax in enumerate(fg.fig.axes):   \n",
    "         ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n",
    "\n",
    "#Reference: https://stackoverflow.com/questions/14885895/color-by-column-values-in-matplotlib\n",
    "#Ref for Axis rotation: https://stackoverflow.com/questions/26540035/rotate-label-text-in-seaborn-factorplot/43256409#43256409"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9cb22",
   "metadata": {},
   "source": [
    "### Compare Shuffle vs Stratified\n",
    "\n",
    "To compare `Shuffle` and `Stratified`, we plot Shuffle and Stratified with `AccuracyNone` (accuracy for non-balanced data) and `AccuracyBalanced` (accuracy for balanced data).\n",
    "\n",
    "In the plots below, we see the difference between `Shuffle` and `Stratified` in absolute values is very small\n",
    "\n",
    "1. `AccuracyNone` values range from `0.871` to `0.877`, which is `0.006`, is a very small difference\n",
    "2. `AccuraceBalanced` values range from `0.847` to `0.853`, which is `0.006`, is also very small difference\n",
    "\n",
    "Based on the Accuracy alone, we cannot pick any models from one over other. \n",
    "We have to do further analysis of better model, as all models are very close to each other for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a plot by looking shuffle vs stratefied\n",
    "\n",
    "model_perf_df['Models'] = model_perf_df['Model'] + \" / Solver:\" + model_perf_df['Solver'] + \" / Penalty:\" + model_perf_df['Penalty'] + \" / C:\"+model_perf_df['C'].astype(str)\n",
    "\n",
    "model_perf_df = model_perf_df.sort_values(by=['Models'],ascending=False)\n",
    "\n",
    "#Sort by accuracy and take top 5\n",
    "\n",
    "#model_perf_df = model_perf_df.nlargest(5, 'AccuracyNone')\n",
    "\n",
    "draw_scatter(df=model_perf_df, x='Models', y='AccuracyNone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29359d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a plot by looking shuffle vs stratefied\n",
    "\n",
    "model_perf_df['Models'] = model_perf_df['Model'] + \" / Solver:\" + model_perf_df['Solver'] + \" / Penalty:\" + model_perf_df['Penalty'] + \" / C:\"+model_perf_df['C'].astype(str)\n",
    "\n",
    "model_perf_df = model_perf_df.sort_values(by=['Models'],ascending=False)\n",
    "\n",
    "#Sort by accuracy and take top 5\n",
    "\n",
    "#model_perf_df = model_perf_df.nlargest(5, 'AccuracyNone')\n",
    "\n",
    "draw_scatter(df=model_perf_df, x='Models', y='AccuracyBalanced')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69473ccd",
   "metadata": {},
   "source": [
    "### Macro Avg vs Weight Avg Precision comparisons\n",
    "\n",
    "As we couldn't pick the models just from `Accuracy`, we are further our analysis by plotting the difference between the `Macro Avg` precision and `Weighted Avg` precision for all the models. \n",
    "\n",
    "\n",
    "   `DiffMacro/WeightedNone` — takes the `Macro Average` and `Weighted Average` of `Precision` from the classification matrix and calculates the difference. The reason to calculate the difference is to check how much variation is in the `Precision` values based on how the data is split. \n",
    "\n",
    "   `DiffMacro/WeightedBalanced` — same as above this variable calculates the difference between Macro and Weighted Average of Precision of a `balanced` data.\n",
    "   \n",
    "The smaller the difference between `Weight Avg` and `Macro Avg`, the model is closer to real world and also consistent.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d0353",
   "metadata": {},
   "source": [
    "### Weighted Avg vs Macro Avg Precision Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_type = ['shuffle', 'stratified']\n",
    "fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Models', 'DiffMacro/WeightedBalanced').add_legend()\n",
    "#fg.suptitle('Macro Avg - Weighted Avg Precision of Balanced Class Weight')\n",
    "\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n",
    "#Reference: https://stackoverflow.com/questions/14885895/color-by-column-values-in-matplotlib\n",
    "\n",
    "#Ref for Axis rotation: https://stackoverflow.com/questions/26540035/rotate-label-text-in-seaborn-factorplot/43256409#43256409"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac70a7",
   "metadata": {},
   "source": [
    "Now, we plotted the models classified into `Shuffle` and `Stratified` against `DiffMacro/WeightedBalanced` of precision (difference between Macro and Weighted Average of Precision of a balanced data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44246506",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_type = ['shuffle', 'stratified']\n",
    "fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Models', 'DiffMacro/WeightedNone').add_legend()\n",
    "#fg.suptitle('Macro Avg - Weighted Avg Precision of None Class Weight')\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720abe80",
   "metadata": {},
   "source": [
    "Also, plotted the models classified into `Shuffle` and `Stratified` against `DiffMacro/WeightedNone` of precision (difference between Macro and Weighted Average of Precision of non-balanced data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a82ecf",
   "metadata": {},
   "source": [
    "From the above two plots, we notice that the difference between Macro Average of Precision and Weighted Average of Precision varies as below:\n",
    "\n",
    "Balanced Data - `0.091` to `0.097`\n",
    "\n",
    "non-Balanced Data - `0.037` to `0.042`\n",
    "\n",
    "There are two major takeaways from the above plots. \n",
    "\n",
    "1. The `stratifiedkfolds` data sees lower variation compared to `shufflesplit`.\n",
    "\n",
    "2. Its evident that the difference between Macro Average of Precision and Weighted Average of Precision varies a lot more for `Balanced` data compared to `non-balanced` data. \n",
    "\n",
    "Since, lower variation is desired to get more stable and consistent model. We decide to go ahead with `Stratified` and non-balanced data. In our model we have hypertuned `shuffle` as `True` for `stratifiedkfolds` as well. Also, we know `stratifiedkfolds` works well with non-balanced data.\n",
    "\n",
    "With above points in mind, we will continue with `Stratified` and `non-balanced` data models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedabe14",
   "metadata": {},
   "source": [
    "## Model cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f22b20",
   "metadata": {},
   "source": [
    "We have established in the previous sections that `StratifiedKFold` works better for our dataset.\n",
    "\n",
    "The below code removes `ShuffleSplit` model from further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b748013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets focus on Stratified\n",
    "\n",
    "#Remove all shuffle\n",
    "\n",
    "model_perf_df.drop(model_perf_df[model_perf_df.Model == \"shuffle\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293abdd1",
   "metadata": {},
   "source": [
    "From the previous sections we have established, we are removing `balanced` `Class Weight` from our further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Columns of balanced\n",
    "\n",
    "balanced_columns=[\"AccuracyBalanced\",\n",
    "                         \"MacroAvgPrecisionBalanced\",\"MacroAvgRecallBalanced\", \"MacroAvgf1-scoreBalanced\",\n",
    "                         \"WeightedAvgPrecisionBalanced\", \"WeightedAvgRecallBalanced\", \"WeightedAvgf1-scoreBalanced\",\n",
    "                         \"DiffMacro/WeightedBalanced\",\"fprBalanced\", \"tprBalanced\" \n",
    "                        ]\n",
    "\n",
    "model_perf_df = model_perf_df.drop(balanced_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178a8f8",
   "metadata": {},
   "source": [
    "`model_perf_df` has following model data:\n",
    "\n",
    "1. Stratified\n",
    "2. non-balanced Class Weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46430f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98591c",
   "metadata": {},
   "source": [
    "## Pick top model\n",
    "\n",
    "As `Accuracy` and difference between `Weighted Precision` and `Macro Avg Precision` is not significantly varies across the models, we shall sort by `Recall` as for our data, `RainTomorrow` as `Yes` prediction is more valuable. \n",
    "\n",
    "`Recall` is the number of true positives (`TP`) divided by the number of true positives plus the number of false negatives (`FN`). \n",
    "\n",
    "`Recall` = TP/(TP + FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c954948",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df = model_perf_df.sort_values(by=['MacroAvgRecallNone','AccuracyNone', 'MacroAvgf1-scoreNone'], ascending=False)\n",
    "\n",
    "# Pick Top 5\n",
    "\n",
    "model_perf_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = model_perf_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee3da1",
   "metadata": {},
   "source": [
    "Below shows our top logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30618a3d",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b44fc",
   "metadata": {},
   "source": [
    "Now we will plot the ROC curve for our selected top model. \n",
    "\n",
    "`AUC` - `ROC` curve is a performance measurement for the classification problems at various threshold settings. `ROC` is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the `AUC`, the better the model is at predicting 0s as 0s and 1s as 1s. \n",
    "\n",
    "reference: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve of top first model\n",
    "\n",
    "for index, row in top_model.iterrows():\n",
    "    fpr = row[\"fprNone\"]\n",
    "    tpr = row[\"tprNone\"]\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa1969",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110395d",
   "metadata": {},
   "source": [
    "The function below creates a `SVM` model with the given parameters.\n",
    "\n",
    "The function accepts following paramters:\n",
    "\n",
    "1. iterations\n",
    "2. kernel\n",
    "3. degree\n",
    "4. gamma\n",
    "\n",
    "\n",
    "We are using StratifiedKFold because ShuffleSplit (with 80/20 split) is not as good as Stratified for our data. We have explained the rationale for using Stratified in the Logistic Regression section [above](# Look-only-Stratified-models).\n",
    "\n",
    "We are using `Class Weight` as `None`, as `StratifiedKFold` works well for our data; so we decided not to use `Class Weight` as balanced.\n",
    "\n",
    "We are only running the certain combinations we found useful for our data from Logistic Regression, as we are constrianed by time, as running all combinations of parameters for SVM takes a long time to complete.\n",
    "\n",
    "`gamma` — is run only with `auto`, as our data is scaled already. We decided not to use `scale` option.\n",
    "\n",
    "`iteration` — 10 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_svm_model(df, C, iterations):\n",
    "    num_cv_iterations = iterations\n",
    "\n",
    "    model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \n",
    "                         \"MacroAvgPrecisionNone\",\"MacroAvgRecallNone\", \"MacroAvgf1-scoreNone\",\n",
    "                         \"WeightedAvgPrecisionNone\", \"WeightedAvgRecallNone\", \"WeightedAvgf1-scoreNone\",\n",
    "                         \"DiffMacro/WeightedNone\",\"fprNone\", \"tprNone\",\n",
    "                         \"Classes\"\n",
    "                        ]\n",
    "\n",
    "\n",
    "    \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    num_instances = len(y)\n",
    "    \n",
    "    # ShuffleSplit is not a good representation of our data. We already established in the previous\n",
    "    # models that Stratified is better for our model.\n",
    "    # We are choosing to run only Stratified for SVM\n",
    "    \n",
    "    cv_data = StratifiedKFold(n_splits=iterations, random_state=123, shuffle=True)\n",
    "    cv_data.get_n_splits(X, y)\n",
    "\n",
    "    iter_num=0\n",
    "    rows = []\n",
    "    scl_obj = StandardScaler()\n",
    "    model_type = \"svm\"\n",
    "    kernel = \"rbf\"\n",
    "    penalty = None\n",
    "    cw = None\n",
    "    stats_dict = {}\n",
    "    solver = None\n",
    "    classes = None\n",
    "    \n",
    "    \n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        X_train = X[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "        \n",
    "        scl_obj.fit(X_train)\n",
    "\n",
    "        X_test = X[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "        try:\n",
    "            X_train_scaled = scl_obj.transform(X_train) \n",
    "            X_test_scaled = scl_obj.transform(X_test)\n",
    "            y_hat = None\n",
    "          \n",
    "            \n",
    "            svm_clf = SVC(C=C, kernel=kernel, degree=3, gamma='auto') \n",
    "            svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "            y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "            classes = svm_clf.classes_\n",
    "\n",
    "            acc = mt.accuracy_score(y_test,y_hat)\n",
    "            conf = mt.confusion_matrix(y_test,y_hat)\n",
    "            target_names = ['No', 'Yes']\n",
    "            class_report = classification_report(y_test, y_hat, target_names, output_dict=True)\n",
    "            \n",
    "            # Macro avg stats\n",
    "            macro_avg_precision = class_report[\"macro avg\"][\"precision\"]\n",
    "            macro_avg_recall = class_report[\"macro avg\"][\"recall\"]\n",
    "            macro_avg_f1_score = class_report[\"macro avg\"][\"f1-score\"]\n",
    "            \n",
    "            #Weighted avg stats\n",
    "            weighted_avg_precision = class_report[\"weighted avg\"][\"precision\"]\n",
    "            weighted_avg_recall = class_report[\"weighted avg\"][\"recall\"]\n",
    "            weighted_avg_f1_score = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "            \n",
    "            \n",
    "            # Create ROC Curve\n",
    "            y_test_01 = np.where(y_test ==\"Yes\", 1, [0])\n",
    "            y_hat_01 = np.where(y_hat ==\"Yes\", 1, [0])\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_test_01, y_hat_01)\n",
    "            stats_dict[\"None\"] = [acc, \n",
    "                          macro_avg_precision, macro_avg_recall, macro_avg_f1_score,\n",
    "                          weighted_avg_precision,weighted_avg_recall,weighted_avg_f1_score,\n",
    "                          abs(weighted_avg_precision-macro_avg_precision),         \n",
    "                          fpr, tpr ]\n",
    "            \n",
    "            print(model_type, kernel,cw,C,penalty,iter_num,\"✅\")\n",
    "        except Exception as e:\n",
    "            print('Error:', str(e))\n",
    "            raise\n",
    "        \n",
    "        #we are not running balanced class weight, so the values are set to None\n",
    "        row = [model_type, solver,C,penalty,iter_num] + stats_dict[\"None\"]  + [classes]\n",
    "        rows.append(row)\n",
    "        iter_num+=1\n",
    "    #end next for loop\n",
    "    \n",
    "    #Create a dataframe with the model stats \n",
    "    df_ret = pd.DataFrame(rows, columns = model_stats_columns) \n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe50c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "C=[1.0, 10.0, 100.0]\n",
    "\n",
    "model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \n",
    "                         \"MacroAvgPrecisionNone\",\"MacroAvgRecallNone\", \"MacroAvgf1-scoreNone\",\n",
    "                         \"WeightedAvgPrecisionNone\", \"WeightedAvgRecallNone\", \"WeightedAvgf1-scoreNone\",\n",
    "                         \"DiffMacro/WeightedNone\",\"fprNone\", \"tprNone\",\n",
    "                         \"Classes\"\n",
    "                        ]\n",
    "\n",
    "#C=[1.0]\n",
    "iterations = 10\n",
    "\n",
    "model_perf_svm_df = pd.DataFrame(columns= model_stats_columns)\n",
    "\n",
    "for c_i in C:\n",
    "    df_ret_svm = create_svm_model(df=df_model, C=c_i ,iterations=iterations)\n",
    "    model_perf_svm_df = model_perf_svm_df.append(df_ret_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b8cac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_perf_svm_df.sort_values(by=['AccuracyNone'],ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43c674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256916c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "import seaborn\n",
    "fg = seaborn.FacetGrid(data=model_perf_svm_df, hue='Model', height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Model', 'AccuracyNone').add_legend()\n",
    "fg.fig.suptitle('Model By Accuracy')\n",
    "\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "import seaborn\n",
    "fg = seaborn.FacetGrid(data=model_perf_svm_df, hue='Model', height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Model', 'MacroAvgPrecisionNone').add_legend()\n",
    "fg.fig.suptitle('Model By Precision')\n",
    "\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81099e",
   "metadata": {},
   "source": [
    "### Top SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e10173",
   "metadata": {},
   "source": [
    "We are picking the top model by sorting using `Accuracy`, `F1 score`, and `Recall` scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b272547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_svm_df = model_perf_svm_df.sort_values(by=['AccuracyNone', 'MacroAvgf1-scoreNone','MacroAvgRecallNone'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be188928",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_svm_model = model_perf_svm_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24544c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "for index, row in top_svm_model.iterrows():\n",
    "    fpr = row[\"fprNone\"]\n",
    "    tpr = row[\"tprNone\"]\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d78e2",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "\n",
    "lr_clf = top_model[\"lr_clfNone\"]\n",
    "#weights = pd.Series(lr_clf,index=df_model.columns)\n",
    "#weights.plot(kind='bar', figsize=(14,6))\n",
    "lr_clf.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a900e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at feature importance \n",
    "\n",
    "import shap # SHAP for Explaining Models\n",
    "shap.initjs()\n",
    "# Create a tree explainer and understanding the values we have \n",
    "shap_ex = shap.LinearExplainer(lr_clf, X_test)\n",
    "vals = shap_ex.shap_values(X_test)\n",
    "shap.summary_plot(vals, df_model.columns, plot_type=\"bar\")\n",
    "\n",
    "# Reference: https://shap.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effect of all the features through SHAP summary plot:\n",
    "explainer = shap.Explainer(lr_clf, X_train, feature_names=df_model.columns)\n",
    "shap_values = explainer(X_test)\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "# Reference: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install InvalidSchemeCombination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a175f",
   "metadata": {},
   "source": [
    "Explaining the logistic model for Rain Tomorrow prediction:\n",
    "\n",
    "SHAP summary plots give us a birds-eye view of feature importance and what is driving it. The values of the features are their TF-IDF values.\n",
    "\n",
    "The above SHAP summary plot is made of many dots. Each dot has three characteristics:\n",
    "\n",
    "Vertical location shows what feature it is depicting\n",
    "Color shows whether that feature was high or low for that row of the dataset.\n",
    "Horizontal location shows whether the effect of that value caused a higher or lower prediction.\n",
    "For example, the point in the upper left was for 'Sunshine' that caused lower chances of Rain Tomorrow, reducing the prediction by 2.\n",
    "\n",
    "Some things the summary plot is  able to easily pick out:-\n",
    "\n",
    "Remember that higher means more likely to be negative, so in the plots above the “red” features are actually helping raise the chance for Rain Tomorrow, while the negative features are lowering the chance for Rain Tomorrow.\n",
    "The model ignored around 56 features which were of lower importance in predicting the chances of Rain Tomorrow.\n",
    "Usually 'WindGustSpeed' has moderate effert on the prediction, but there are extreme cases of 'WindGustSpeed' where a high value still caused moderate level of prediction.\n",
    "High values of Goal scored caused higher predictions, and low values caused low predictions\n",
    "\n",
    "Reference: https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaining why a sample weather record # 200 is classified as Rain Tomorrow (Yes/No)?\n",
    "ind = 200\n",
    "shap.plots.force(shap_values[ind])\n",
    "# Reference: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap plot provides another global view of the model’s behavior, with a focus on predictions for Rain Tomorrow.\n",
    "#shap.plots.heatmap(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b66aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_by_time = {\n",
    "    '3pm': [f for f in df_impute if '3pm' in f],\n",
    "    '9am': [f for f in df_impute if '9am' in f],\n",
    "    'not_time_based': [f for f in df_impute if '9am' not in f and '3pm' not in f]\n",
    "}\n",
    "\n",
    "groups_by_type = {\n",
    "    'humidity_and_rain': ['Rainfall',\n",
    "                          'Evaporation',\n",
    "                          'Humidity9am',\n",
    "                          'Humidity3pm',\n",
    "                          'RainToday'],\n",
    "    'temperature': ['MinTemp',\n",
    "                    'MaxTemp',\n",
    "                    'Temp9am',\n",
    "                    'Temp3pm'],\n",
    "    'sun_and_clouds': ['Cloud9am',\n",
    "                       'Cloud3pm',\n",
    "                       'Sunshine'],\n",
    "    'wind_and_pressure': ['WindGustDir',\n",
    "                          'WindGustSpeed',\n",
    "                          'WindDir9am',\n",
    "                          'WindDir3pm',\n",
    "                          'WindSpeed9am',\n",
    "                          'WindSpeed3pm',\n",
    "                          'Pressure9am',\n",
    "                          'Pressure3pm'],\n",
    "    'location': ['Location']\n",
    "}\n",
    "\n",
    "shap_time = grouped_shap(shap_vals, df_impute, groups_by_time)\n",
    "shap_type = grouped_shap(shap_vals, df_impute, groups_by_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11923732",
   "metadata": {},
   "source": [
    "## Alternative/Shorter version of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67001765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "\n",
    "accuracies = logistic_regression_short() # this also can help with parallelism\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = mt.ConfusionMatrixDisplay(confusion_matrix=conf[1])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b10b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num,(train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit([train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict([test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Function\n",
    "#Run with various parameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def logistic_regression_short(df, iterations,class_weight, penalty, solver, C, l1_ratio):\n",
    "    lr_clf = LogisticRegression(penalty=penalty, C=C, class_weight=class_weight, solver=solver) \n",
    "\n",
    "  \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    iter_num = 0\n",
    "    \n",
    "    try:\n",
    "        # train the reusable logisitc regression model on the training data\n",
    "        accuracies = cross_val_score(lr_clf, X, y=y, cv=iterations) \n",
    "        for index, acc in enumerate(accuracies):\n",
    "            rows.append([solver,class_weight,C,penalty,index, acc, None])\n",
    "    except Exception as e:\n",
    "        print(\"Error in running\", solver,class_weight,C,penalty,iter_num, str(e))\n",
    "        \n",
    "        \n",
    "    df_ret = pd.DataFrame(rows, columns=[\"Solver\", \"ClassWeight\", \"C\", \"Penalty\",\"Iteration\",\"Accuracy\",\"ConfusionMatrix\"])\n",
    "    return df_ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
