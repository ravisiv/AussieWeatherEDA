{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1746415f",
   "metadata": {},
   "source": [
    "# Lab One : Visualization and Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4b9ff",
   "metadata": {},
   "source": [
    "### Group 3 - Members:\n",
    "\n",
    "_Apurv Mittal_<br>\n",
    "_Seemant Srivastava_<br>\n",
    "_Ravi Sivaraman_<br>\n",
    "_Tai Chowdhury_<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from shapely.geometry import Point\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8247b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Warnings on final\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Australia weather data\n",
    "df = pd.read_csv(\"weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  View the top rows of the data imported\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba99724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Date and Location\n",
    "\n",
    "df = df.drop(['Date', 'Location'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91546666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing records which are blank for Rain today and Rain tomorrow\n",
    "\n",
    "df.dropna(subset = [\"RainToday\"], inplace=True)\n",
    "df.dropna(subset = [\"RainTomorrow\"], inplace=True)\n",
    "\n",
    "\n",
    "# Seperate the data into categorical and numeric\n",
    "\n",
    "df_num = df.columns[df.dtypes == 'float64']\n",
    "df_cat=df.columns[df.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_num)\n",
    "print(\"Categorical Variables:\", df_cat)\n",
    "\n",
    "# REFERENCE: https://www.kite.com/python/answers/how-to-drop-empty-rows-from-a-pandas-dataframe-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529fc00",
   "metadata": {},
   "source": [
    "#### Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac571fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute data (numeric) based on the mean for RainToday and RainTomorrow\n",
    "\n",
    "df_impute = df\n",
    "mat_yesno = df[df_num].groupby([df['RainToday'],df['RainTomorrow']]).mean()\n",
    "RAINTODAY=0\n",
    "RAINTOMORROW=1\n",
    "COUNTER = 0\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for indexattr in mat_yesno.iloc[COUNTER].index:\n",
    "            df_impute.loc[(df_impute[\"RainToday\"] == mat_yesno.iloc[COUNTER].name[RAINTODAY] ) \n",
    "                          & (df_impute[\"RainTomorrow\"] == mat_yesno.iloc[COUNTER].name[RAINTOMORROW]) \n",
    "                          & (df_impute[indexattr].isnull()), indexattr] = mat_yesno.iloc[COUNTER][indexattr]\n",
    "        COUNTER = COUNTER + 1\n",
    "\n",
    "        \n",
    "        \n",
    "# Impute data (categorical) with mode of each variable\n",
    "\n",
    "df_impute['WindDir9am'] = df_impute['WindDir9am'].fillna(df_impute['WindDir9am'].mode()[0])\n",
    "df_impute['WindGustDir'] = df_impute['WindGustDir'].fillna(df_impute['WindGustDir'].mode()[0])\n",
    "df_impute['WindDir3pm'] = df_impute['WindDir3pm'].fillna(df_impute['WindDir3pm'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca19ef",
   "metadata": {},
   "source": [
    "As mentioned above, we imputed data for all numeric variables with the means for the combination of `RainToday` and `RainTomorrow`. We calcualted the value for `RainToday` and `RainTomorrow` both as \"No\" and imputed the data for the missing variables for such combination, similary calculated `RainToday` as \"Yes\" and `RainTomorrow` as \"No\" and imputed the mean value for the variable so and so forth.\n",
    "\n",
    "For categorical variables `WindDir9am`, `WindDir3pm` are covering the direction of the wind at different 9 am and 3 pm respectively, while `WindGustDir`is the direction of the wind gust. All these variables are about the direction and and the largest missing variable is `6.8%` for Wind Direction at 9 am. We decided to impute this data with the Mode for each of the categorical variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb585c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impute_num = df.columns[df.dtypes == 'float64']\n",
    "df_impute_cat=df.columns[df.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_num)\n",
    "print(\"Categorical Variables:\", df_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe083dc",
   "metadata": {},
   "source": [
    "## Creating different dataframes for continous and categorical variables.\n",
    "Assigning the `RainTomorrow` as our response variable (y) and all other continous variable as X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af99221",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance For Rainfall Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the original data\n",
    "df_model = df_impute.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change RainToday from categorical to continuous\n",
    "\n",
    "df_model[\"IsRainToday\"] = df_impute['RainToday']\n",
    "#df_model['IsRainTomorrow'] = df_impute['RainTomorrow']\n",
    "\n",
    "df_model['IsRainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\n",
    "#df_model['IsRainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_impute\", df_impute.shape)\n",
    "print(\"df_model\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26445414",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "df_model['RainTomorrow'].value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\n",
    "plt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d81f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "df_model['RainToday'].value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\n",
    "plt.title('RainToday Indicator No(0) and Yes(1) in the Imbalanced Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a25167",
   "metadata": {},
   "source": [
    "We can observe that the presence of “0” and “1” is almost in the 78:22 ratio. So there is a class imbalance and we have to deal with it. To fight against the class imbalance, we will use here the oversampling of the minority class. Since the size of the dataset is quite small, majority class subsampling wouldn’t make much sense here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f90fc0",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding of the categorical data\n",
    "\n",
    "gust_df = pd.get_dummies(df_model.WindGustDir,prefix='GustDir')\n",
    "wind3pm_df = pd.get_dummies(df_model.WindDir3pm,prefix='Wind3pm')\n",
    "wind9am_df = pd.get_dummies(df_model.WindDir9am,prefix='Wind9am')\n",
    "df_model = pd.concat((df_model,gust_df, wind3pm_df, wind9am_df),axis=1) # add back into the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical columns\n",
    "\n",
    "df_model = df_model.drop(['WindDir3pm', 'WindDir9am', 'WindGustDir', 'RainToday'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782dd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if Yes is replaced as 1\n",
    "\n",
    "print(\"Are there 1's and 0's in the RainToday column?\", \n",
    "      (df_model['IsRainToday'].sum() > 0) and (df_model['IsRainToday'].sum() < len(df_model['IsRainToday'])))\n",
    "\n",
    "#Non zero output means there is a mixture of 1's and 0's\n",
    "#if sum is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f9a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=df_model[df_num]\n",
    "y = df_model.RainTomorrow\n",
    "print('features shape:', X.shape) \n",
    "print('target shape:', y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7086efe",
   "metadata": {},
   "source": [
    "# Commented code\n",
    "model_stats_columns=[\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\",\"ConfusionMatrixNone\", \n",
    "                         \"MacroAvgPrecisionNone\",\"MacroAvgRecallNone\", \"MacroAvgF1ScoreNone\",\n",
    "                         \"WeightedAvgPrecisionNone\",\"WeightedAvgRecallNone\", \"WeightedAvgF1ScoreNone\",\n",
    "                         \"fprNone\", \"tprNone\",\n",
    "                         \"AccuracyBalanced\",\"ConfusionMatrixBalanced\", \n",
    "                         \"MacroAvgPrecisionBalanced\",\"MacroAvgRecallBalanced\", \"MacroAvgF1ScoreBalanced\",\n",
    "                         \"WeightedAvgPrecisionBalanced\",\"WeightedAvgRecallBalanced\", \"WeightedAvgF1ScoreBalanced\",\n",
    "                         \"fprBalanced\", \"tprBalanced\",\n",
    "                         \"Classes\"\n",
    "                        ]\n",
    "                        \n",
    "                        \n",
    " stats_dict[key] = [acc, conf, \n",
    "                  macro_avg_precision, macro_avg_recall,macro_avg_f1_score,\n",
    "                  weighted_avg_precision,weighted_avg_recall,weighted_avg_f1_score,\n",
    "                  fpr, tpr ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30623463",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \"MacroAvgPrecisionNone\",\"fprNone\", \"tprNone\",\n",
    "                         \"AccuracyBalanced\",\"MacroAvgPrecisionBalanced\",\"fprBalanced\", \"tprBalanced\",\n",
    "                         \"DiffMacroVsBalPrecision\",\n",
    "                         \"Classes\"\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#model stats dataframe columns for class weight balanced and None\n",
    "model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \"MacroAvgPrecisionNone\",\"fprNone\", \"tprNone\",\n",
    "                         \"AccuracyBalanced\",\"MacroAvgPrecisionBalanced\",\"fprBalanced\", \"tprBalanced\", \n",
    "                         \"DiffMacroVsBalPrecision\",\n",
    "                         \"Classes\"\n",
    "                        ]\n",
    "\n",
    "\n",
    "def create_log_models(model_type,df, iterations,penalty, C, solver):\n",
    "    class_weight = ['balanced', None]\n",
    "    \n",
    "    #Create logreg object for both class weights\n",
    "    lr_clf_balanced = LogisticRegression(penalty=penalty, C=C, class_weight='balanced', solver=solver) \n",
    "    lr_clf_none = LogisticRegression(penalty=penalty, C=C, class_weight=None, solver=solver)\n",
    "    \n",
    "    #Store both objects in a dict for later retrival\n",
    "    lr_clf_dict = { \n",
    "                    \"balanced\": lr_clf_balanced,\n",
    "                    \"None\": lr_clf_none\n",
    "                    }\n",
    "\n",
    "    num_cv_iterations = iterations\n",
    "\n",
    "   \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    num_instances = len(y)\n",
    "    \n",
    "    cv_data = None\n",
    "    if model_type == \"shuffle\":\n",
    "        cv_data = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                             test_size  = 0.2, random_state = 123)\n",
    "\n",
    "    elif model_type == \"stratified\":\n",
    "        cv_data = StratifiedKFold(n_splits=iterations, random_state=123, shuffle=True)\n",
    "        cv_data.get_n_splits(X, y)\n",
    "       \n",
    "    #Initialize variables\n",
    "    iter_num=0\n",
    "    rows = []\n",
    "    stats_dict = {}\n",
    "    target_names = ['No', 'Yes']\n",
    "    classes = None    \n",
    "    scl_obj = StandardScaler()\n",
    "    \n",
    "    # Run for balanced first with same model and then None with same model\n",
    "    # store the results in same row of dataframe\n",
    "    # This helps to compare none and balanced macro avg\n",
    "   \n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        for cw in class_weight:\n",
    "            X_train = X[train_indices]\n",
    "            y_train = y[train_indices]\n",
    "        \n",
    "            scl_obj.fit(X_train)\n",
    "\n",
    "            X_test = X[test_indices]\n",
    "            y_test = y[test_indices]\n",
    "            \n",
    "            #Get the logistic regression object for the current cw class weight\n",
    "            key = None\n",
    "            if cw == None:\n",
    "                key = \"None\"\n",
    "            else:\n",
    "                key = cw\n",
    "                    \n",
    "            lr_clf = lr_clf_dict[key]\n",
    "            \n",
    "            try:\n",
    "                X_train_scaled = scl_obj.transform(X_train) \n",
    "                X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "                lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "                y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "                classes = lr_clf.classes_\n",
    "\n",
    "                acc = mt.accuracy_score(y_test,y_hat)\n",
    "                conf = mt.confusion_matrix(y_test,y_hat)\n",
    "                \n",
    "                class_report = classification_report(y_test, y_hat, target_names, output_dict=True)\n",
    "                \n",
    "                # Macro avg stats\n",
    "                macro_avg_precision = class_report[\"macro avg\"][\"precision\"]\n",
    "                macro_avg_recall = class_report[\"macro avg\"][\"recall\"]\n",
    "                macro_avg_f1_score = class_report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "                #Weighted avg stats\n",
    "                weighted_avg_precision = class_report[\"weighted avg\"][\"precision\"]\n",
    "                weighted_avg_recall = class_report[\"weighted avg\"][\"recall\"]\n",
    "                weighted_avg_f1_score = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "                # Create ROC Curve\n",
    "                y_test_01 = np.where(y_test ==\"Yes\", 1, [0])\n",
    "                y_hat_01 = np.where(y_hat ==\"Yes\", 1, [0])\n",
    "\n",
    "                fpr, tpr, threshold = metrics.roc_curve(y_test_01, y_hat_01)\n",
    "                #rows.append([model_type,solver,class_weight,C,penalty,iter_num, acc, conf, class_report, fpr,tpr,classes])\n",
    "                \n",
    "                #Create a dict of these stats for class weight\n",
    "                #dict will contain stats for balanced on one run, None for the next run\n",
    "                \n",
    "                stats_dict[key] = [acc, \n",
    "                                  macro_avg_precision,\n",
    "                                  fpr, tpr ]\n",
    "\n",
    "                print(model_type, solver,cw,C,penalty,iter_num,\"✅\")\n",
    "            except Exception as e:\n",
    "                #print('Error:', str(e))\n",
    "                raise\n",
    "            #end try block  \n",
    "        #end first for loop\n",
    "        #When cursor comes here, model has ran for both None and balanced\n",
    "        #Create a single row of lists combining none and balanced \n",
    "        \n",
    "        row = [model_type, solver,C,penalty,iter_num] + stats_dict[\"None\"] + stats_dict['balanced'] + [stats_dict[\"None\"][1] - stats_dict[\"balanced\"][1]] + [classes]\n",
    "        rows.append(row)\n",
    "        iter_num+=1\n",
    "    #end next for loop\n",
    "    \n",
    "    #Create a dataframe with the model stats \n",
    "    df_ret = pd.DataFrame(rows, columns = model_stats_columns)\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eea1d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#penalty=['l1','l2', 'elasticnet', 'none']\n",
    "#solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "#model_type = ['shuffle', 'stratified']\n",
    "#C=[1.0, 10.0, 100.0]\n",
    "\n",
    "\n",
    "#penalty=['l2']\n",
    "#solver = ['liblinear']\n",
    "#model_type = ['shuffle']\n",
    "#C=[1.0]\n",
    "\n",
    "model_perf_df = pd.DataFrame(columns= model_stats_columns)\n",
    "\n",
    "# Run all combinations of penalty, solver, model_type, C\n",
    "# Create a giant dataframe\n",
    "# Each row of dataframe contains stats for each combination of penalty, solver, model_type,C\n",
    "# same row contains the stats for both None and balanced class weight\n",
    "# so we can compare the balanced and None\n",
    "# and pick the model whose diff in precision (for macro avg) is lowest with highest accuracy\n",
    "# This model is the closest to real world\n",
    "\n",
    "for pen in penalty:\n",
    "    for c_index in C:\n",
    "        for solv in solver:\n",
    "            for mdl_type in model_type:\n",
    "                try:\n",
    "                    df_ret = create_log_models(model_type=mdl_type,df=df_model,iterations=3, penalty=pen, C=c_index, solver=solv)\n",
    "                    model_perf_df = model_perf_df.append(df_ret, ignore_index=True)\n",
    "                    \n",
    "                    #Model deletes RainTomorrow from dataframe\n",
    "                    #put it back from imputed data to run for another set of model\n",
    "                    df_model[\"RainTomorrow\"] = df_impute[\"RainTomorrow\"].values\n",
    "                except Exception as e:\n",
    "                    #print(\"Error in running\", str(e))\n",
    "                    continue\n",
    "                            \n",
    "\n",
    "print(model_perf_df)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3191720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total models and each model with balanced and None class weight\n",
    "\n",
    "model_count = len(model_perf_df)\n",
    "\n",
    "print (\"Number of models with balanced and None for each:\", model_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_perf_df.to_csv(\"statdata_log_reg.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "model_perf_df = pd.read_csv(\"statdata_log_reg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e746fff",
   "metadata": {},
   "source": [
    "## Top 5 Models by [BalancedvsNonePrecision] followed by Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15969ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_perf_df.sort_values(by=['DiffMacroVsBalPrecision','AccuracyNone'],ascending=False).head(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "import seaborn\n",
    "\n",
    "\n",
    "model_perf_df['x'] = \"Pen=\" + model_perf_df['Penalty'] + \"/C=\"+model_perf_df['C'].astype(str)\n",
    "\n",
    "seaborn.set(style='ticks')\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "\n",
    "fg = seaborn.FacetGrid(data=model_perf_df, hue='Solver', hue_order=solver, height=10, aspect=1.61)\n",
    "fg.map(pyplot.scatter, 'x', 'AccuracyNone').add_legend()\n",
    "\n",
    "\n",
    "\n",
    "#Reference: https://stackoverflow.com/questions/14885895/color-by-column-values-in-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9820a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set(style='ticks')\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "\n",
    "fg = seaborn.FacetGrid(data=model_perf_df, hue='Solver', hue_order=solver, height=10, aspect=1.61)\n",
    "fg.map(pyplot.scatter, 'x', 'DiffMacroVsBalPrecision').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2945832",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df.plot(kind='scatter',x='x', y='DiffMacroVsBalPrecision', c='DarkBlue' , figsize=(14,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e1882",
   "metadata": {},
   "source": [
    "# Stop here for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_svm_model(df, class_weight, iterations):\n",
    "    num_cv_iterations = iterations\n",
    "  \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    num_instances = len(y)\n",
    "    cv_data = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                             test_size  = 0.2, random_state = 123)\n",
    "\n",
    "     \n",
    "    iter_num=0\n",
    "    rows = []\n",
    "    scl_obj = StandardScaler()\n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        X_train = X[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "        \n",
    "        scl_obj.fit(X_train)\n",
    "\n",
    "        X_test = X[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "        try:\n",
    "            X_train_scaled = scl_obj.transform(X_train) \n",
    "            X_test_scaled = scl_obj.transform(X_test)\n",
    "            y_hat = None\n",
    "          \n",
    "            svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "            svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "            y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "            classes = svm_clf.classes_\n",
    "\n",
    "            acc = mt.accuracy_score(y_test,y_hat)\n",
    "            conf = mt.confusion_matrix(y_test,y_hat)\n",
    "            target_names = ['No', 'Yes']\n",
    "            class_report = classification_report(y_test, y_hat, target_names, output_dict=True)\n",
    "            \n",
    "            # Macro avg stats\n",
    "            macro_avg_precision = class_report[\"macro avg\"][\"precision\"]\n",
    "            macro_avg_recall = class_report[\"macro avg\"][\"recall\"]\n",
    "            macro_avg_f1_score = class_report[\"macro avg\"][\"f1-score\"]\n",
    "            \n",
    "            #Weighted avg stats\n",
    "            weighted_avg_precision = class_report[\"weighted avg\"][\"precision\"]\n",
    "            weighted_avg_recall = class_report[\"weighted avg\"][\"recall\"]\n",
    "            weighted_avg_f1_score = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Create ROC Curve\n",
    "            y_test_01 = np.where(y_test ==\"Yes\", 1, [0])\n",
    "            y_hat_01 = np.where(y_hat ==\"Yes\", 1, [0])\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_test_01, y_hat_01)\n",
    "            rows.append([\"svm\",None,class_weight,0,None,iter_num, acc, conf, class_report, fpr,tpr, classes])\n",
    "\n",
    "            model_stats[cw] = [acc, conf, target_names, ]\n",
    "            \n",
    "            \n",
    "            print(\"Iteration\",iter_num,\"-----> Done\")\n",
    "        except Exception as e:\n",
    "            print('Error:', str(e))\n",
    "            raise\n",
    "        iter_num+=1   \n",
    "   \n",
    "    df_ret = pd.DataFrame(rows, columns=[\"ModelType\",\"Solver\", \"ClassWeight\", \"C\", \"Penalty\",\"Iteration\",\"Accuracy\",\"ConfusionMatrix\", \n",
    "                                         \"ClassificationReport\",\"fpr\", \"tpr\", \"classes\"])\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc6920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b326e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_perf_df.sort_values(by=['Accuracy'],ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7385f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "df_head = model_perf_df.head(5)\n",
    "class_report_none = []\n",
    "class_report_balanced = []\n",
    "for index, row in df_head.iterrows():\n",
    "    \n",
    "    if (row[\"ClassWeight\"] == \"balanced\"):\n",
    "        class_report_balanced.append(row[\"ClassificationReport\"])\n",
    "    else:\n",
    "        class_report_none.append(row[\"ClassificationReport\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d88583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret_svm = create_svm_model(df=df_model, class_weight='balanced',iterations=3)\n",
    "model_perf_df = model_perf_df.append(dt_ret_svm)\n",
    "for index, row in df_ret_svm.iterrows():\n",
    "    fpr = row[\"fpr\"]\n",
    "    tpr = row[\"tpr\"]\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df_model.columns)\n",
    "weights.plot(kind='bar', figsize=(14,6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = mt.ConfusionMatrixDisplay(confusion_matrix=conf[1])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a900e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11923732",
   "metadata": {},
   "source": [
    "## Alternative/Shorter version of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b10b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num,(train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit([train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict([test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Function\n",
    "#Run with various parameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def logistic_regression_short(df, iterations,class_weight, penalty, solver, C, l1_ratio):\n",
    "    lr_clf = LogisticRegression(penalty=penalty, C=C, class_weight=class_weight, solver=solver) \n",
    "\n",
    "  \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    iter_num = 0\n",
    "    \n",
    "    try:\n",
    "        # train the reusable logisitc regression model on the training data\n",
    "        accuracies = cross_val_score(lr_clf, X, y=y, cv=iterations) \n",
    "        for index, acc in enumerate(accuracies):\n",
    "            rows.append([solver,class_weight,C,penalty,index, acc, None])\n",
    "    except Exception as e:\n",
    "        print(\"Error in running\", solver,class_weight,C,penalty,iter_num, str(e))\n",
    "        \n",
    "        \n",
    "    df_ret = pd.DataFrame(rows, columns=[\"Solver\", \"ClassWeight\", \"C\", \"Penalty\",\"Iteration\",\"Accuracy\",\"ConfusionMatrix\"])\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67001765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "\n",
    "accuracies = logistic_regression_short() # this also can help with parallelism\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at feature importance \n",
    "\n",
    "import shap # SHAP for Explaining Models\n",
    "shap.initjs()\n",
    "# Create a tree explainer and understanding the values we have \n",
    "shap_ex = shap.LinearExplainer(lr_clf, X_test)\n",
    "vals = shap_ex.shap_values(X_test)\n",
    "shap.summary_plot(vals, df_model.columns, plot_type=\"bar\")\n",
    "\n",
    "# Reference: https://shap.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effect of all the features through SHAP summary plot:\n",
    "explainer = shap.Explainer(lr_clf, X_train, feature_names=df_model.columns)\n",
    "shap_values = explainer(X_test)\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "# Reference: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a175f",
   "metadata": {},
   "source": [
    "Explaining the logistic model for Rain Tomorrow prediction:\n",
    "\n",
    "SHAP summary plots give us a birds-eye view of feature importance and what is driving it. The values of the features are their TF-IDF values.\n",
    "\n",
    "The above SHAP summary plot is made of many dots. Each dot has three characteristics:\n",
    "\n",
    "Vertical location shows what feature it is depicting\n",
    "Color shows whether that feature was high or low for that row of the dataset.\n",
    "Horizontal location shows whether the effect of that value caused a higher or lower prediction.\n",
    "For example, the point in the upper left was for 'Sunshine' that caused lower chances of Rain Tomorrow, reducing the prediction by 2.\n",
    "\n",
    "Some things the summary plot is  able to easily pick out:-\n",
    "\n",
    "Remember that higher means more likely to be negative, so in the plots above the “red” features are actually helping raise the chance for Rain Tomorrow, while the negative features are lowering the chance for Rain Tomorrow.\n",
    "The model ignored around 56 features which were of lower importance in predicting the chances of Rain Tomorrow.\n",
    "Usually 'WindGustSpeed' has moderate effert on the prediction, but there are extreme cases of 'WindGustSpeed' where a high value still caused moderate level of prediction.\n",
    "High values of Goal scored caused higher predictions, and low values caused low predictions\n",
    "\n",
    "Reference: https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaining why a sample weather record # 200 is classified as Rain Tomorrow (Yes/No)?\n",
    "ind = 200\n",
    "shap.plots.force(shap_values[ind])\n",
    "# Reference: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap plot provides another global view of the model’s behavior, with a focus on predictions for Rain Tomorrow.\n",
    "#shap.plots.heatmap(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b66aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_by_time = {\n",
    "    '3pm': [f for f in df_impute if '3pm' in f],\n",
    "    '9am': [f for f in df_impute if '9am' in f],\n",
    "    'not_time_based': [f for f in df_impute if '9am' not in f and '3pm' not in f]\n",
    "}\n",
    "\n",
    "groups_by_type = {\n",
    "    'humidity_and_rain': ['Rainfall',\n",
    "                          'Evaporation',\n",
    "                          'Humidity9am',\n",
    "                          'Humidity3pm',\n",
    "                          'RainToday'],\n",
    "    'temperature': ['MinTemp',\n",
    "                    'MaxTemp',\n",
    "                    'Temp9am',\n",
    "                    'Temp3pm'],\n",
    "    'sun_and_clouds': ['Cloud9am',\n",
    "                       'Cloud3pm',\n",
    "                       'Sunshine'],\n",
    "    'wind_and_pressure': ['WindGustDir',\n",
    "                          'WindGustSpeed',\n",
    "                          'WindDir9am',\n",
    "                          'WindDir3pm',\n",
    "                          'WindSpeed9am',\n",
    "                          'WindSpeed3pm',\n",
    "                          'Pressure9am',\n",
    "                          'Pressure3pm'],\n",
    "    'location': ['Location']\n",
    "}\n",
    "\n",
    "shap_time = grouped_shap(shap_vals, df_impute, groups_by_time)\n",
    "shap_type = grouped_shap(shap_vals, df_impute, groups_by_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4eff68",
   "metadata": {},
   "source": [
    "### Oversampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "no = df_impute[df_impute.RainTomorrow == 0]\n",
    "yes = df_impute[df_impute.RainTomorrow == 1]\n",
    "yes_oversampled = resample(yes, replace=True, n_samples=len(no), random_state=123)\n",
    "oversampled = pd.concat([no, yes_oversampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_dummies using oversampled dataframe\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"embarked\"\n",
    "gust_df = pd.get_dummies(oversampled.WindGustDir,prefix='GustDir')\n",
    "wind3pm_df = pd.get_dummies(oversampled.WindDir3pm,prefix='Wind3pm')\n",
    "wind9am_df = pd.get_dummies(oversampled.WindDir9am,prefix='Wind9am')\n",
    "oversampled = pd.concat((oversampled,gust_df, wind3pm_df, wind9am_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# replace the current Sex atribute with something slightly more intuitive and readable\n",
    "oversampled['IsRainToday'] = oversampled.RainToday=='Yes' \n",
    "oversampled.IsRainToday = oversampled.IsRainToday.astype(np.int)\n",
    "\n",
    "oversampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "oversampled.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\n",
    "plt.title('RainTomorrow Indicator No(0) and Yes(1) after Oversampling (Balanced Dataset)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6df2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping categorical columns\n",
    "\n",
    "oversampled = oversampled.drop(['WindDir3pm', 'WindDir9am', 'WindGustDir', 'RainToday'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba911d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shufflesplit using oversampled dataframe\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if \"RainTomorrow\" in oversampled:\n",
    "    y = oversampled[\"RainTomorrow\"].values # get the labels we want\n",
    "    del oversampled[\"RainTomorrow\"] # get rid of the class label\n",
    "    X_over = oversampled.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object_over = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2, random_state = 123)\n",
    "                         \n",
    "print(cv_object_over)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
