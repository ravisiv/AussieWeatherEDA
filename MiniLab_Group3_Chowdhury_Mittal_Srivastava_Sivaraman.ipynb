{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1746415f",
   "metadata": {},
   "source": [
    "# Mini Lab : Logistic Regression and Support Vector Machine\n",
    "\n",
    "### Group 3 - Members:\n",
    "\n",
    "_Tai Chowdhury_<br>\n",
    "_Apurv Mittal_<br>\n",
    "_Ravi Sivaraman_<br>\n",
    "_Seemant Srivastava_<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e576f7",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cd903",
   "metadata": {},
   "source": [
    "As discussed in Lab 1, we have acquired the Australian Weather dataset from Kaggle portal. It contains 10 years of weather data collected from many locations across Australia. These are daily weather observations. There are 145,459 observations with 23 attributes in the original dataset. \n",
    "\n",
    "We have chosen RainTomorrow (categorical) and Rainfall (continuous) as predictor variables. RainTomorrow is a categorical attribute which indicates whether it is going to rain tomorrow - yes or no. Rainfall is a continuous attribute that measures amount of rainfall each of the particular locations have received (in mm). Using our models, we will be able to design an algorithm where the bureau can help to predict rainfall for different regions in Australia.\n",
    "\n",
    "In this Lab 2 assignment, we have measured the accuracy and effectiveness of our model for categorical variable RainTomorrow by using 10-fold cross validation against the confusion matrix measurements like: Precision, Recall and Accuracy. We have explored the methods of logistic regression and support vector machine (SVM) models on our dataset. \n",
    "\n",
    "We have used `scikit-learn` packages for our exploration. We ran logistic regression models with all the available solvers in the `scikit-learn` package and compare the effictiveness and accuracy of the model to predict `RainfallTomorrow`. We also measured the duration of model run from each models to compare model performance and efficiency as well.\n",
    " \n",
    "To get started, we will start with loading all the necessary packages for our analysis. We will start our analysis with `df_impute` which is the imputed dataframe from our last explanatory data analysis Lab 1 project. Using this dataframe will ensure data consistency for all the labs going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from shapely.geometry import Point\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8247b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Warnings on final\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229a0c7",
   "metadata": {},
   "source": [
    "## Imputed Data\n",
    "We imputed data in EDA and reusing imputed data from EDA (Lab1) project.\n",
    "Here is the link to the EDA\n",
    "\n",
    "https://nbviewer.jupyter.org/github/ravisiv/AussieWeatherEDA/blob/c0ba412cb75da21eba386ea9ea39f645ad6af1d0/DS7331_Lab1_Group3_Ravi_Taifur_Seemant_Apurv_Submission.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Imputed Australia weather data\n",
    "df_impute = pd.read_csv(\"weatherAUS_imputed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  View the top rows of the data imported\n",
    "df_impute.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fb065",
   "metadata": {},
   "source": [
    "The imputed data doesn't include any null or missing values. Also, we have dropped the columns like: Date of observation and City Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb585c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impute_num = df_impute.columns[df_impute.dtypes == 'float64']\n",
    "df_impute_cat=df_impute.columns[df_impute.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_impute_num)\n",
    "print(\"Categorical Variables:\", df_impute_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6fa55",
   "metadata": {},
   "source": [
    "Before continuing further, we need to check which variables are numeric and which are not. As the models expect numerical variables. We will filter and identify non-numeric variables.\n",
    "\n",
    "`WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` and `RainTomorrow`are not numeric. Here `RainTomorrow` is our response variable. we handle the other variables with hot-one-encoding later in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the original data\n",
    "df_model = df_impute.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a635ed8",
   "metadata": {},
   "source": [
    "Creating a new DataFrame `df_model` for modeling to avoid any changes to the original dataset `df_impute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable to Identify if it RainToday\n",
    "\n",
    "df_model[\"IsRainToday\"] = df_impute['RainToday']\n",
    "\n",
    "# Replacing No with 0 and Yes with 1.\n",
    "\n",
    "df_model['IsRainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0a055",
   "metadata": {},
   "source": [
    "Assigning `0` to No values and `1` to Yes values in `RainToday` (Changed to `IsRainToday`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_impute\", df_impute.shape)\n",
    "print(\"df_model\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the values to check if the data looks good\n",
    "\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a25167",
   "metadata": {},
   "source": [
    "We can observe that the presence of “0” and “1” is almost in the 78:22 ratio. So there is a class imbalance and we have to deal with it. To fight against the class imbalance, we will use here the oversampling of the minority class. Since the size of the dataset is quite small, majority class subsampling wouldn’t make much sense here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f90fc0",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe083dc",
   "metadata": {},
   "source": [
    "Before we create our models, we need to format our attributes. We are converting `RainToday` and `RainTomorrow` into numeric variables to `0` and `1`. We also decided to go ahead with one-hot-encoding `WindGustDir`, `WindDir9am`, and `WindDir3pm` attributes based on the direction of the wind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding using dummies\n",
    "\n",
    "gust_df = pd.get_dummies(df_model.WindGustDir,prefix='GustDir')\n",
    "wind3pm_df = pd.get_dummies(df_model.WindDir3pm,prefix='Wind3pm')\n",
    "wind9am_df = pd.get_dummies(df_model.WindDir9am,prefix='Wind9am')\n",
    "df_model = pd.concat((df_model,gust_df, wind3pm_df, wind9am_df),axis=1) # add back into the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc00c0e",
   "metadata": {},
   "source": [
    "We decided to do one-hot-encoding using dummies function as machine learning algorithms and models requires numerical values for both input and output attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical columns\n",
    "\n",
    "df_model = df_model.drop(['WindDir3pm', 'WindDir9am', 'WindGustDir', 'RainToday'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b973430",
   "metadata": {},
   "source": [
    "After conversions, we are removing these categorical attributes to avoid duplicates as we have those data in numerical format. We are added the newly formatted attributes and rest of the continuous attributes into a new dataframe - df_model. We will use the new dataframe for modeling.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782dd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if Yes is replaced as 1\n",
    "\n",
    "print(\"Are there 1's and 0's in the RainToday column?\", \n",
    "      (df_model['IsRainToday'].sum() > 0) and (df_model['IsRainToday'].sum() < len(df_model['IsRainToday'])))\n",
    "\n",
    "#Non zero output means there is a mixture of 1's and 0's\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634fff9",
   "metadata": {},
   "source": [
    "Checking if the data imputation happened accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_num = df_model.columns[df_model.dtypes == 'float64']\n",
    "df_model_cat=df_model.columns[df_model.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_model_num)\n",
    "print(\"Categorical Variables:\", df_model_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20202992",
   "metadata": {},
   "source": [
    "Check if all the numerical variables are accurately created and if we still have any non-numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f9a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=df_model[df_model_num]\n",
    "y = df_model.RainTomorrow\n",
    "print('features shape:', X.shape) \n",
    "print('target shape:', y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea30c6",
   "metadata": {},
   "source": [
    "Assigning the `RainTomorrow` as our response variable (y) and all other variables include one-hot-encoded values as X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607eb76",
   "metadata": {},
   "source": [
    "### Data Distribution\n",
    "\n",
    "Check if the data distribution is balanced or not for the response variable `RainTomorrow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26445414",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "df_model['RainTomorrow'].value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\n",
    "plt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e7da9",
   "metadata": {},
   "source": [
    "As expected, we see the data for `RainTomorrow` is imbalanced. Majority of the data is for `No` rain vs. `Yes` for `RainTomorrow`.\n",
    "\n",
    "We can observe that the presence of `0` and `1` is almost in the `78:22` ratio. We will be cognizant of the fact that our model may be not very effective if we don't solve for imbalance. We will discuss and adjust for this imbalance in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30623463",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244c582",
   "metadata": {},
   "source": [
    "### Creating Logistic Regression Function\n",
    "\n",
    "The function below creates models with various parameters supported by LogisticRegression. There are various combinations of\n",
    "1. `Model Type`\n",
    "2. `Class Weight`\n",
    "3. `Solver`\n",
    "4. `C`\n",
    "5. `Penalty`\n",
    "6. `Iterations`\n",
    "\n",
    "This function runs for specific Solver, C, Penalty but takes various iterations, and we used 10 iterations.\n",
    "When this function called with Solver, C, Penalty, the function runs for BOTH Class Weight, `balanced` and `None`. The result of both `balanced` and `None` is stored as single row in a dataframe.\n",
    "They are stored in the same row to verify how `balanced` and `None` compares.\n",
    "\n",
    "##### Model Type\n",
    "\n",
    "1. `Shuffle`\n",
    "2. `Stratified`\n",
    "\n",
    "`ShuffleSplit`  — is similar to `Cross Validation` where we can specify the percentage of split for train and test data. However, in regular cross-validation, the data is not split randomly, so, it is good to shuffle the targets before applying the `cross-validation`.\n",
    "\n",
    "\n",
    "`Stratified` — CV technique is very useful with unbalanced dataset. As discussed above our dataset is not balanced and rightly so, we don't expect it to rain and no-rain days to be equal in Australia. The data is expected to be unbalanced and expected to be such in future as well. So using stratified sampling techniques gives us the ability to preserve the proportion of the Rain days vs non-rain days in our dataset. We can be confident that the Train and Test split data is not leaving out important information like entire dataset is of `No` rain days which will give highly inaccurate output eventhough the accuracy might be maintained. \n",
    "\n",
    "In `Stratified` sampling, the data is k-1 split in favor of Train vs Test data.\n",
    "\n",
    "\n",
    "References: \n",
    "https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d\n",
    "https://mclguide.readthedocs.io/en/latest/sklearn/cv.html\n",
    "\n",
    "\n",
    "##### Solver Options\n",
    "Scikit-learn comes with five different solver options. Each solver minimize the cost function. Here are the five options:\n",
    "\n",
    "`newton-cg` — A newton method. Newton methods use an exact Hessian matrix. It's slow for large datasets, because it computes the second derivatives.\n",
    "\n",
    "`lbfgs` — Stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno. It approximates the second derivative matrix updates with gradient evaluations. It stores only the last few updates, so it saves memory. It isn't super fast with large data sets.\n",
    "\n",
    "`liblinear` — Library for Large Linear Classification. Uses a coordinate descent algorithm. Coordinate descent is based on minimizing a multivariate function by solving univariate optimization problems in a loop. In other words, it moves toward the minimum in one direction at a time. It performs pretty well with high dimensionality. It does have a number of drawbacks. It can get stuck, is unable to run in parallel, and can only solve multi-class logistic regression with one-vs.-rest.\n",
    "\n",
    "`sag` — Stochastic Average Gradient descent. A variation of gradient descent and incremental aggregated gradient approaches that uses a random sample of previous gradient values. Fast for big datasets.\n",
    "\n",
    "`saga` — Extension of sag that also allows for L1 regularization. Should generally train faster than sag.\n",
    "\n",
    "Reference for above definitions: https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "\n",
    "##### Penalty\n",
    "\n",
    "Used to specify the norm used in the penalization. The `newton-cg`’, `sag` and `lbfgs` solvers support only `l2` penalties. `elasticnet` is only supported by the `saga` solver. If `none` (not supported by the `liblinear` solver), no regularization is applied.\n",
    "\n",
    "`L1`  or `Lasso` — (`LASSO` is Least Absolute Shrinkage and Selection Operator) uses a penalized least squares approach that squeezes the regression coefficients to 0 when the penalty is large. The algorithm starts with a large penalty and gradually relaxes the penalty to allow for a single variable to be added into the model (the coefficient is no longer `0`). `LASSO` uses `L1` method.\n",
    "\n",
    "`L2` or `Ridge` — adds a penalty equal to the square of the magnitude of coefficients. `L2` will not yield sparse models and all coefficients are shrunk by the same factor (not eliminated). Ridge regression and SVMs use this method.\n",
    "\n",
    "`elasticnet`  — Procedure identical to `LASSO` however the penalty is different. `elasticnet` uses a combination of both the `LASSO` penalty as well as the `RIDGE` regression penalty. It combine both `L1` & `L2` methods.\n",
    "\n",
    "`none` — No penalty\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Reference for Penalty definitions: https://www.statisticshowto.com/regularization/\n",
    "\n",
    "##### C\n",
    "`C` is cost, which we run with `1.0`, `10.0` and `100.0` for each combination of Solver and Penalty.\n",
    "\n",
    "If the solver doesn't support the penalty, those are skipped.\n",
    "\n",
    "\n",
    "The dataframe stores:\n",
    "\n",
    "1. `Model Type`\n",
    "2. `Solver`\n",
    "3. `C`\n",
    "4. `Penalty`\n",
    "5. `Iteration`\n",
    "6. `AccuracyNone`\n",
    "7. `MacroAvgPrecisionNone`\n",
    "7. `WeightedAvgPrecisionNone`\n",
    "8. `DiffMacro/WeightedNone`\n",
    "9. `fprNone`\n",
    "10. `tprNone`,\n",
    "11. `AccuracyBalanced`\n",
    "12. `MacroAvgPrecisionBalanced`\n",
    "13. `WeightedAvgPrecisionBalanced`\n",
    "14. `DiffMacro/WeightedBalanced`\n",
    "15. `fprBalanced`\n",
    "16. `tprBalanced`\n",
    "17. `Classes`\n",
    "\n",
    "`DiffMacro/WeightedBalanced` is the difference between the Presicion of `macro avg` and `Weighted`. Smaller the value, more closer they are.\n",
    "\n",
    "`fprNone` and `tprNone` are stored to run ROC curve.\n",
    "\n",
    "`Classes` is `Yes` and `No` as an array, which are used for labels for plots. They are constant for all rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#model stats dataframe columns for class weight balanced and None\n",
    "model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \n",
    "                         \"MacroAvgPrecisionNone\",\"MacroAvgRecallNone\", \"MacroAvgf1-scoreNone\",\n",
    "                         \"WeightedAvgPrecisionNone\", \"WeightedAvgRecallNone\", \"WeightedAvgf1-scoreNone\",\n",
    "                         \"DiffMacro/WeightedNone\",\"fprNone\", \"tprNone\",\"lr_clfNone\",\n",
    "                         \"AccuracyBalanced\",\n",
    "                         \"MacroAvgPrecisionBalanced\",\"MacroAvgRecallBalanced\", \"MacroAvgf1-scoreBalanced\",\n",
    "                         \"WeightedAvgPrecisionBalanced\", \"WeightedAvgRecallBalanced\", \"WeightedAvgf1-scoreBalanced\",\n",
    "                         \"DiffMacro/WeightedBalanced\",\"fprBalanced\", \"tprBalanced\", \"lr_clfBalanced\",\n",
    "                         \"Classes\", \"Time\"\n",
    "                        ]\n",
    "\n",
    "\n",
    "def create_log_models(model_type,df, iterations,penalty, C, solver):\n",
    "    class_weight = ['balanced', None]\n",
    "    \n",
    "    #Create logreg object for both class weights\n",
    "    lr_clf_balanced = LogisticRegression(penalty=penalty, C=C, class_weight='balanced', solver=solver) \n",
    "    lr_clf_none = LogisticRegression(penalty=penalty, C=C, class_weight=None, solver=solver)\n",
    "    \n",
    "    #Store both objects in a dict for later retrival\n",
    "    lr_clf_dict = { \n",
    "                    \"balanced\": lr_clf_balanced,\n",
    "                    \"None\": lr_clf_none\n",
    "                    }\n",
    "\n",
    "    num_cv_iterations = iterations\n",
    "\n",
    "   \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    num_instances = len(y)\n",
    "    \n",
    "    cv_data = None\n",
    "    if model_type == \"shuffle\":\n",
    "        cv_data = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                             test_size  = 0.2, random_state = 123)\n",
    "\n",
    "    elif model_type == \"stratified\":\n",
    "        cv_data = StratifiedKFold(n_splits=iterations, random_state=123, shuffle=True)\n",
    "        cv_data.get_n_splits(X, y)\n",
    "       \n",
    "    #Initialize variables\n",
    "    iter_num=0\n",
    "    rows = []\n",
    "    stats_dict = {}\n",
    "    target_names = ['No', 'Yes']\n",
    "    classes = None    \n",
    "    scl_obj = StandardScaler()\n",
    "    lr_clf = None\n",
    "    # Run for balanced first with same model and then None with same model\n",
    "    # store the results in same row of dataframe\n",
    "    # This helps to compare none and balanced macro avg\n",
    "   \n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        starttime = timer()\n",
    "        for cw in class_weight:\n",
    "            X_train = X[train_indices]\n",
    "            y_train = y[train_indices]\n",
    "        \n",
    "            scl_obj.fit(X_train)\n",
    "\n",
    "            X_test = X[test_indices]\n",
    "            y_test = y[test_indices]\n",
    "            \n",
    "            #Get the logistic regression object for the current cw class weight\n",
    "            key = None\n",
    "            if cw == None:\n",
    "                key = \"None\"\n",
    "            else:\n",
    "                key = cw\n",
    "                    \n",
    "            lr_clf = lr_clf_dict[key]\n",
    "            \n",
    "            try:\n",
    "                X_train_scaled = scl_obj.transform(X_train) \n",
    "                X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "                lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "                y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "                classes = lr_clf.classes_\n",
    "\n",
    "                acc = mt.accuracy_score(y_test,y_hat)\n",
    "                conf = mt.confusion_matrix(y_test,y_hat)\n",
    "                \n",
    "                class_report = classification_report(y_test, y_hat, target_names, output_dict=True)\n",
    "                \n",
    "                # Macro avg stats\n",
    "                macro_avg_precision = class_report[\"macro avg\"][\"precision\"]\n",
    "                macro_avg_recall = class_report[\"macro avg\"][\"recall\"]\n",
    "                macro_avg_f1_score = class_report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "                #Weighted avg stats\n",
    "                weighted_avg_precision = class_report[\"weighted avg\"][\"precision\"]\n",
    "                weighted_avg_recall = class_report[\"weighted avg\"][\"recall\"]\n",
    "                weighted_avg_f1_score = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "                # Create ROC Curve\n",
    "                y_test_01 = np.where(y_test ==\"Yes\", 1, [0])\n",
    "                y_hat_01 = np.where(y_hat ==\"Yes\", 1, [0])\n",
    "\n",
    "                fpr, tpr, threshold = metrics.roc_curve(y_test_01, y_hat_01)\n",
    "                \n",
    "                #Create a dict of these stats for class weight\n",
    "                #dict will contain stats for balanced on one run, None for the next run\n",
    "                \n",
    "                stats_dict[key] = [acc, \n",
    "                          macro_avg_precision, macro_avg_recall, macro_avg_f1_score,\n",
    "                          weighted_avg_precision,weighted_avg_recall,weighted_avg_f1_score,\n",
    "                          abs(weighted_avg_precision-macro_avg_precision),         \n",
    "                          fpr, tpr, lr_clf ]\n",
    "\n",
    "                print(model_type, solver,cw,C,penalty,iter_num,\"✅\")\n",
    "            except Exception as e:\n",
    "                #print('Error:', str(e))\n",
    "                raise\n",
    "            #end try block  \n",
    "        #end first for loop\n",
    "        #When cursor comes here, model has ran for both None and balanced\n",
    "        #Create a single row of lists combining none and balanced \n",
    "        endtime = timer()\n",
    "        time_taken = endtime - starttime\n",
    "        row = [model_type, solver,C,penalty,iter_num] + stats_dict[\"None\"] + stats_dict['balanced'] + [classes, time_taken ]\n",
    "        rows.append(row)\n",
    "        iter_num+=1\n",
    "    #end next for loop\n",
    "    \n",
    "    #Create a dataframe with the model stats \n",
    "    df_ret = pd.DataFrame(rows, columns = model_stats_columns)\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1acd42",
   "metadata": {},
   "source": [
    "### Running Logistic Regression with various options\n",
    "\n",
    "The code below runs all the combinations of `Penalty`, `Solver`, `C` and shuffle types and stores in dataframe for further analysis later on.\n",
    "\n",
    "If any combinations of `Solver` and `Penalty` isn't supported by Logistic Regression, they are skipped.\n",
    "\n",
    "`iteration` — 10 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eea1d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "penalty=['l1','l2', 'elasticnet', 'none']\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "model_type = ['shuffle', 'stratified']\n",
    "C=[1.0, 10.0, 100.0]\n",
    "iterations = 10\n",
    "\n",
    "#penalty=['l2']\n",
    "#solver = ['liblinear']\n",
    "#model_type = [\"stratified\"]\n",
    "#C=[1.0]\n",
    "#iterations = 2\n",
    "\n",
    "model_perf_df = pd.DataFrame(columns= model_stats_columns)\n",
    "\n",
    "# Run all combinations of penalty, solver, model_type, C\n",
    "# Create a giant dataframe\n",
    "# Each row of dataframe contains stats for each combination of penalty, solver, model_type,C\n",
    "# same row contains the stats for both None and balanced class weight\n",
    "# so we can compare the balanced and None\n",
    "# and pick the model whose diff in precision (for macro avg) is lowest with highest accuracy\n",
    "# This model is the closest to real world\n",
    "\n",
    "for pen in penalty:\n",
    "    for c_index in C:\n",
    "        for solv in solver:\n",
    "            for mdl_type in model_type:\n",
    "                try:\n",
    "\n",
    "                    df_ret = create_log_models(model_type=mdl_type,df=df_model,iterations=iterations, penalty=pen, C=c_index, solver=solv)\n",
    "                    \n",
    "                    model_perf_df = model_perf_df.append(df_ret, ignore_index=True)\n",
    "                    \n",
    "                    #Model deletes RainTomorrow from dataframe\n",
    "                    #put it back from imputed data to run for another set of model\n",
    "                    df_model[\"RainTomorrow\"] = df_impute[\"RainTomorrow\"].values\n",
    "                except Exception as e:\n",
    "                    #print(\"Error in running\", str(e))\n",
    "                    continue\n",
    "                            \n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35335d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_model_copy = model_perf_df.copy()\n",
    "#model_perf_df = perf_model_copy.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1acbda",
   "metadata": {},
   "source": [
    "In these models we used two different methodologies for cross validation namely `Shuffle` and `Stratified`.\n",
    "\n",
    "`ShuffleSplit`  — is similar to `Cross Validation` where we can specify the percentage of split for train and test data. However, in regular cross-validation, the data is not split randomly, so, it is good to shuffle the targets before applying the `cross-validation`.\n",
    "\n",
    "\n",
    "`Stratified` — CV technique is very useful with unbalanced dataset. As discussed above our dataset is not balanced and rightly so, we don't expect it to rain and no-rain days to be equal in Australia. The data is expected to be unbalanced and expected to be such in future as well. So using stratified sampling techniques gives us the ability to preserve the proportion of the Rain days vs non-rain days in our dataset. We can be confident that the Train and Test split data is not leaving out important information like entire dataset is of `No` rain days which will give highly inaccurate output eventhough the accuracy might be maintained. \n",
    "\n",
    "In `Stratified` sampling, the data is k-1 split in favor of Train vs Test data.\n",
    "\n",
    "\n",
    "References: \n",
    "https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d\n",
    "https://mclguide.readthedocs.io/en/latest/sklearn/cv.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0785a1",
   "metadata": {},
   "source": [
    "After running the big `Logistic Regression` model for several combinations. We got output for 660 models.\n",
    "\n",
    "To compare the outputs and make it easier to identify the most appropriate model in terms of accuracy, precision and other factors applicable for Machine Learning models. We decided to put a list of subset of variables including:\n",
    "\n",
    "`Model` — specifies which technique was used for the logistic model. In this case its between ShuffleSplit and Stratified.\n",
    "\n",
    "`AccuracyNone` —  specifies the Accuracy observed by the model where data was not `balanced`.\n",
    "\n",
    "`DiffMacro/WeightedNone` — takes the `Macro Average` and `Weighted Average` of `Precision` from the classification matrix and calculates the difference. The reason to calculate the difference is to check how much variation is in the Precision values based on how the data is split. More details about Precision and Averages is provided below. This variation is calculated on the non `balanced` data.\n",
    "\n",
    "`AccuracyBalanced` — specifies the Accuracy observed by the model where data was `balanced`.\n",
    "\n",
    "`DiffMacro/WeightedBalanced` — same as above this variable calculates the difference between Macro and Weighted Average of Precision of a `balanced` data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d18ba7",
   "metadata": {},
   "source": [
    "### Important Terms\n",
    "\n",
    "`Accuracy` — is a ratio of correctly predicted observation to the total observations. It is a very important aspect to define the success of a model but just the measurement on its own can be deceiving if the observations are not equal for each class. In such cases we might be predicting accurately for one particular class with large observation and may not do very well for other classes.\n",
    "\n",
    "`Precision` — is the ratio of correctly predicted positive observations to the total predicted positive observations. Precision = TP/TP+FP  \n",
    "\n",
    "`Weighted Average` — can be calculated on various output variables of the classification report like Precision, Recall, f1-score. As the name suggests it gives the weighted average of the parameter based on the number of observations or values for each class.\n",
    "\n",
    "`Macro Average` — similar to weighted average, macro average can also be calculated on various output variables of the classification report like Precision, Recall, f1-score. However, the similarity ends here as unlike weighted average, we don't use weights based on the number of observations, rather equal weights are given to each class to calcualte the value. This tells us if the Precision is as good if the dataset was balanced.\n",
    "\n",
    "References:\n",
    "https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "https://datascience.stackexchange.com/questions/65839/macro-average-and-weighted-average-meaning-in-classification-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02993cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "import seaborn\n",
    "\n",
    "def draw_scatter(df, x,y, title=None):\n",
    "\n",
    "    seaborn.set(style='ticks')\n",
    "    cv_type = ['shuffle', 'stratified']\n",
    "\n",
    "    #fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61, ylim=(0,1))\n",
    "    fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)\n",
    "    fg.map(pyplot.scatter, x, y ).add_legend()\n",
    "    #fg.suptitle(title)\n",
    "\n",
    "    for i, ax in enumerate(fg.fig.axes):   \n",
    "         ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n",
    "\n",
    "#Reference: https://stackoverflow.com/questions/14885895/color-by-column-values-in-matplotlib\n",
    "#Ref for Axis rotation: https://stackoverflow.com/questions/26540035/rotate-label-text-in-seaborn-factorplot/43256409#43256409"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9cb22",
   "metadata": {},
   "source": [
    "### Compare Shuffle vs Stratified\n",
    "\n",
    "To compare `Shuffle` and `Stratified`, we plot Shuffle and Stratified with `AccuracyNone` (accuracy for non-balanced data) and `AccuracyBalanced` (accuracy for balanced data).\n",
    "\n",
    "In the plots below, we see the difference between `Shuffle` and `Stratified` in absolute values is very small\n",
    "\n",
    "1. `AccuracyNone` values range from `0.871` to `0.877`, which is `0.006`, is a very small difference\n",
    "2. `AccuraceBalanced` values range from `0.847` to `0.853`, which is `0.006`, is also very small difference\n",
    "\n",
    "Based on the Accuracy alone, we cannot pick any models from one over other. \n",
    "We have to do further analysis of better model, as all models are very close to each other for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a plot by looking shuffle vs stratefied\n",
    "\n",
    "model_perf_df['Models'] = model_perf_df['Model'] + \" / Solver:\" + model_perf_df['Solver'] + \" / Penalty:\" + model_perf_df['Penalty'] + \" / C:\"+model_perf_df['C'].astype(str)\n",
    "\n",
    "model_perf_df = model_perf_df.sort_values(by=['Models'],ascending=False)\n",
    "\n",
    "#Sort by accuracy and take top 5\n",
    "\n",
    "#model_perf_df = model_perf_df.nlargest(5, 'AccuracyNone')\n",
    "\n",
    "draw_scatter(df=model_perf_df, x='Models', y='AccuracyNone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29359d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a plot by looking shuffle vs stratefied\n",
    "\n",
    "model_perf_df['Models'] = model_perf_df['Model'] + \" / Solver:\" + model_perf_df['Solver'] + \" / Penalty:\" + model_perf_df['Penalty'] + \" / C:\"+model_perf_df['C'].astype(str)\n",
    "\n",
    "model_perf_df = model_perf_df.sort_values(by=['Models'],ascending=False)\n",
    "\n",
    "#Sort by accuracy and take top 5\n",
    "\n",
    "#model_perf_df = model_perf_df.nlargest(5, 'AccuracyNone')\n",
    "\n",
    "draw_scatter(df=model_perf_df, x='Models', y='AccuracyBalanced')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69473ccd",
   "metadata": {},
   "source": [
    "### Macro Avg vs Weight Avg Precision comparisons\n",
    "\n",
    "As we couldn't pick the models just from `Accuracy`, we are further our analysis by plotting the difference between the `Macro Avg` precision and `Weighted Avg` precision for all the models. \n",
    "\n",
    "\n",
    "   `DiffMacro/WeightedNone` — takes the `Macro Average` and `Weighted Average` of `Precision` from the classification matrix and calculates the difference. The reason to calculate the difference is to check how much variation is in the `Precision` values based on how the data is split. \n",
    "\n",
    "   `DiffMacro/WeightedBalanced` — same as above this variable calculates the difference between Macro and Weighted Average of Precision of a `balanced` data.\n",
    "   \n",
    "The smaller the difference between `Weight Avg` and `Macro Avg`, the model is closer to real world and also consistent.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d0353",
   "metadata": {},
   "source": [
    "### Weighted Avg vs Macro Avg Precision Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_type = ['shuffle', 'stratified']\n",
    "fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Models', 'DiffMacro/WeightedBalanced').add_legend()\n",
    "#fg.suptitle('Macro Avg - Weighted Avg Precision of Balanced Class Weight')\n",
    "\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n",
    "#Reference: https://stackoverflow.com/questions/14885895/color-by-column-values-in-matplotlib\n",
    "\n",
    "#Ref for Axis rotation: https://stackoverflow.com/questions/26540035/rotate-label-text-in-seaborn-factorplot/43256409#43256409"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac70a7",
   "metadata": {},
   "source": [
    "Now, we plotted the models classified into `Shuffle` and `Stratified` against `DiffMacro/WeightedBalanced` of precision (difference between Macro and Weighted Average of Precision of a balanced data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44246506",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_type = ['shuffle', 'stratified']\n",
    "fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Models', 'DiffMacro/WeightedNone').add_legend()\n",
    "#fg.suptitle('Macro Avg - Weighted Avg Precision of None Class Weight')\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720abe80",
   "metadata": {},
   "source": [
    "Also, plotted the models classified into `Shuffle` and `Stratified` against `DiffMacro/WeightedNone` of precision (difference between Macro and Weighted Average of Precision of non-balanced data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a82ecf",
   "metadata": {},
   "source": [
    "From the above two plots, we notice that the difference between Macro Average of Precision and Weighted Average of Precision varies as below:\n",
    "\n",
    "Balanced Data - `0.091` to `0.097`\n",
    "\n",
    "non-Balanced Data - `0.037` to `0.042`\n",
    "\n",
    "There are two major takeaways from the above plots. \n",
    "\n",
    "1. The `stratifiedkfolds` data sees lower variation compared to `shufflesplit`.\n",
    "\n",
    "2. Its evident that the difference between Macro Average of Precision and Weighted Average of Precision varies a lot more for `Balanced` data compared to `non-balanced` data. \n",
    "\n",
    "Since, lower variation is desired to get more stable and consistent model. We decide to go ahead with `Stratified` and non-balanced data. In our model we have hypertuned `shuffle` as `True` for `stratifiedkfolds` as well. Also, we know `stratifiedkfolds` works well with non-balanced data.\n",
    "\n",
    "With above points in mind, we will continue with `Stratified` and `non-balanced` data models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedabe14",
   "metadata": {},
   "source": [
    "## Model cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f22b20",
   "metadata": {},
   "source": [
    "We have established in the previous sections that `StratifiedKFold` works better for our dataset.\n",
    "\n",
    "The below code removes `ShuffleSplit` model from further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b748013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets focus on Stratified\n",
    "\n",
    "#Remove all shuffle\n",
    "\n",
    "model_perf_df.drop(model_perf_df[model_perf_df.Model == \"shuffle\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293abdd1",
   "metadata": {},
   "source": [
    "From the previous sections we have established, we are removing `balanced` `Class Weight` from our further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Columns of balanced\n",
    "\n",
    "balanced_columns=[\"AccuracyBalanced\",\n",
    "                         \"MacroAvgPrecisionBalanced\",\"MacroAvgRecallBalanced\", \"MacroAvgf1-scoreBalanced\",\n",
    "                         \"WeightedAvgPrecisionBalanced\", \"WeightedAvgRecallBalanced\", \"WeightedAvgf1-scoreBalanced\",\n",
    "                         \"DiffMacro/WeightedBalanced\",\"fprBalanced\", \"tprBalanced\" \n",
    "                        ]\n",
    "\n",
    "model_perf_df = model_perf_df.drop(balanced_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178a8f8",
   "metadata": {},
   "source": [
    "`model_perf_df` has following model data:\n",
    "\n",
    "1. Stratified\n",
    "2. non-balanced Class Weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46430f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98591c",
   "metadata": {},
   "source": [
    "## Pick top logistic regression model\n",
    "\n",
    "As `Accuracy` and difference between `Weighted Precision` and `Macro Avg Precision` is not significantly varies across the models, we shall sort by `Recall` as for our data, `RainTomorrow` as `Yes` prediction is more valuable. \n",
    "\n",
    "`Recall` is the number of true positives (`TP`) divided by the number of true positives plus the number of false negatives (`FN`). \n",
    "\n",
    "`Recall` = TP/(TP + FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c954948",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df = model_perf_df.sort_values(by=['MacroAvgRecallNone','AccuracyNone', 'MacroAvgf1-scoreNone'], ascending=False)\n",
    "\n",
    "# Pick Top 5\n",
    "\n",
    "model_perf_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = model_perf_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee3da1",
   "metadata": {},
   "source": [
    "Below shows our top logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30618a3d",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b44fc",
   "metadata": {},
   "source": [
    "Now we will plot the ROC curve for our selected top model. \n",
    "\n",
    "`AUC` - `ROC` curve is a performance measurement for the classification problems at various threshold settings. `ROC` is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the `AUC`, the better the model is at predicting 0s as 0s and 1s as 1s. \n",
    "\n",
    "reference: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve of top first model\n",
    "\n",
    "for index, row in top_model.iterrows():\n",
    "    fpr = row[\"fprNone\"]\n",
    "    tpr = row[\"tprNone\"]\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b166392",
   "metadata": {},
   "source": [
    "The above plot shows the area under the curve (AUC) for our top model. The score of 0.80 tells us that there is 80% chance that the model will be successful at classifying yes as yes and no as no values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa1969",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110395d",
   "metadata": {},
   "source": [
    "The function below creates a `SVM` model with the given parameters.\n",
    "\n",
    "The function accepts following paramters:\n",
    "\n",
    "1. iterations\n",
    "2. kernel\n",
    "3. degree\n",
    "4. gamma\n",
    "\n",
    "\n",
    "We are using StratifiedKFold because ShuffleSplit (with 80/20 split) is not as good as Stratified for our data. We have explained the rationale for using Stratified in the Logistic Regression section [above](# Look-only-Stratified-models).\n",
    "\n",
    "We are using `Class Weight` as `None`, as `StratifiedKFold` works well for our data; so we decided not to use `Class Weight` as balanced.\n",
    "\n",
    "We are only running the certain combinations we found useful for our data from Logistic Regression, as we are constrianed by time, as running all combinations of parameters for SVM takes a long time to complete.\n",
    "\n",
    "`gamma` — is run only with `auto`, as our data is scaled already. We decided not to use `scale` option.\n",
    "\n",
    "`iteration` — 10 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_svm_model(df, C, iterations):\n",
    "    num_cv_iterations = iterations\n",
    "\n",
    "    model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \n",
    "                         \"MacroAvgPrecisionNone\",\"MacroAvgRecallNone\", \"MacroAvgf1-scoreNone\",\n",
    "                         \"WeightedAvgPrecisionNone\", \"WeightedAvgRecallNone\", \"WeightedAvgf1-scoreNone\",\n",
    "                         \"DiffMacro/WeightedNone\",\"fprNone\", \"tprNone\", \"svm_clf\", \n",
    "                         \"Classes\"\n",
    "                        ]\n",
    "\n",
    "\n",
    "    \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    num_instances = len(y)\n",
    "    \n",
    "    # ShuffleSplit is not a good representation of our data. We already established in the previous\n",
    "    # models that Stratified is better for our model.\n",
    "    # We are choosing to run only Stratified for SVM\n",
    "    \n",
    "    cv_data = StratifiedKFold(n_splits=iterations, random_state=123, shuffle=True)\n",
    "    cv_data.get_n_splits(X, y)\n",
    "\n",
    "    iter_num=0\n",
    "    rows = []\n",
    "    scl_obj = StandardScaler()\n",
    "    model_type = \"svm\"\n",
    "    kernel = \"rbf\"\n",
    "    penalty = None\n",
    "    cw = None\n",
    "    stats_dict = {}\n",
    "    solver = None\n",
    "    classes = None\n",
    "    \n",
    "    \n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        X_train = X[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "        \n",
    "        scl_obj.fit(X_train)\n",
    "\n",
    "        X_test = X[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "        try:\n",
    "            X_train_scaled = scl_obj.transform(X_train) \n",
    "            X_test_scaled = scl_obj.transform(X_test)\n",
    "            y_hat = None\n",
    "          \n",
    "            \n",
    "            svm_clf = SVC(C=C, kernel=kernel, degree=3, gamma='auto') \n",
    "            svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "            y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "            classes = svm_clf.classes_\n",
    "\n",
    "            acc = mt.accuracy_score(y_test,y_hat)\n",
    "            conf = mt.confusion_matrix(y_test,y_hat)\n",
    "            target_names = ['No', 'Yes']\n",
    "            class_report = classification_report(y_test, y_hat, target_names, output_dict=True)\n",
    "            \n",
    "            # Macro avg stats\n",
    "            macro_avg_precision = class_report[\"macro avg\"][\"precision\"]\n",
    "            macro_avg_recall = class_report[\"macro avg\"][\"recall\"]\n",
    "            macro_avg_f1_score = class_report[\"macro avg\"][\"f1-score\"]\n",
    "            \n",
    "            #Weighted avg stats\n",
    "            weighted_avg_precision = class_report[\"weighted avg\"][\"precision\"]\n",
    "            weighted_avg_recall = class_report[\"weighted avg\"][\"recall\"]\n",
    "            weighted_avg_f1_score = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "            \n",
    "            \n",
    "            # Create ROC Curve\n",
    "            y_test_01 = np.where(y_test ==\"Yes\", 1, [0])\n",
    "            y_hat_01 = np.where(y_hat ==\"Yes\", 1, [0])\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_test_01, y_hat_01)\n",
    "            stats_dict[\"None\"] = [acc, \n",
    "                          macro_avg_precision, macro_avg_recall, macro_avg_f1_score,\n",
    "                          weighted_avg_precision,weighted_avg_recall,weighted_avg_f1_score,\n",
    "                          abs(weighted_avg_precision-macro_avg_precision),         \n",
    "                          fpr, tpr, svm_clf ]\n",
    "            \n",
    "            print(model_type, kernel,cw,C,penalty,iter_num,\"✅\")\n",
    "        except Exception as e:\n",
    "            print('Error:', str(e))\n",
    "            raise\n",
    "        \n",
    "        #we are not running balanced class weight, so the values are set to None\n",
    "        row = [model_type, solver,C,penalty,iter_num] + stats_dict[\"None\"]  + [classes]\n",
    "        rows.append(row)\n",
    "        iter_num+=1\n",
    "    #end next for loop\n",
    "    \n",
    "    #Create a dataframe with the model stats \n",
    "    df_ret = pd.DataFrame(rows, columns = model_stats_columns) \n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa5c9f",
   "metadata": {},
   "source": [
    "Here we are running our SVM model using 10 iterations with various C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "C=[1.0, 10.0, 100.0]\n",
    "\n",
    "model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \n",
    "                         \"MacroAvgPrecisionNone\",\"MacroAvgRecallNone\", \"MacroAvgf1-scoreNone\",\n",
    "                         \"WeightedAvgPrecisionNone\", \"WeightedAvgRecallNone\", \"WeightedAvgf1-scoreNone\",\n",
    "                         \"DiffMacro/WeightedNone\",\"fprNone\", \"tprNone\",\"svm_clf\",\n",
    "                         \"Classes\"\n",
    "                        ]\n",
    "\n",
    "#C=[1.0]\n",
    "iterations = 10\n",
    "\n",
    "model_perf_svm_df = pd.DataFrame(columns= model_stats_columns)\n",
    "\n",
    "for c_i in C:\n",
    "    df_ret_svm = create_svm_model(df=df_model, C=c_i ,iterations=iterations)\n",
    "    model_perf_svm_df = model_perf_svm_df.append(df_ret_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b8cac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_perf_svm_df.sort_values(by=['AccuracyNone'],ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a213c77",
   "metadata": {},
   "source": [
    "### SVM Models Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256916c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "import seaborn\n",
    "fg = seaborn.FacetGrid(data=model_perf_svm_df, hue='Model', height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Model', 'AccuracyNone').add_legend()\n",
    "fg.fig.suptitle('Model By Accuracy')\n",
    "\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248ad75",
   "metadata": {},
   "source": [
    "The above plot shows the accuracies for all of SVM models. The scores vary from `0.898` to `0.906` which is only a variation of `0.008`. All our SVM models appear to be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "import seaborn\n",
    "fg = seaborn.FacetGrid(data=model_perf_svm_df, hue='Model', height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Model', 'MacroAvgPrecisionNone').add_legend()\n",
    "fg.fig.suptitle('Model By Precision')\n",
    "\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11483095",
   "metadata": {},
   "source": [
    "The above plot shows macro average precisions for all of SVM models. The scores vary from `0.864` to `0.879` which is only a variation of `0.015`. All our SVM models appear to be very similar again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81099e",
   "metadata": {},
   "source": [
    "### Top SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e10173",
   "metadata": {},
   "source": [
    "We are picking the top model by sorting using `Accuracy`, `F1 score`, and `Recall` scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b272547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_svm_df = model_perf_svm_df.sort_values(by=['AccuracyNone', 'MacroAvgf1-scoreNone','MacroAvgRecallNone'], ascending=False)\n",
    "top_svm_model = model_perf_svm_df.head(1)\n",
    "top_svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609fcba5",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "for index, row in top_svm_model.iterrows():\n",
    "    fpr = row[\"fprNone\"]\n",
    "    tpr = row[\"tprNone\"]\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05e85c",
   "metadata": {},
   "source": [
    "Now we will plot the ROC curve for our selected top model. \n",
    "\n",
    "The above plot shows the area under the curve (AUC) for our top model. The score of 0.84 tells us that there is 84% chance that the model will be successful at classifying yes as yes and no as no values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f9645",
   "metadata": {},
   "source": [
    "### SVM vs Logistic Regression Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ab64b",
   "metadata": {},
   "source": [
    "The SVM algorithm creates a hyperplane or line(decision boundary) which separates data into classes. It finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data.\n",
    "\n",
    "While logistic regression does not create the margins like SVM, instead it can have different decision boundaries with different weights that are near the optimal point.\n",
    "\n",
    "SVM works well with unstructured and semi-structured data like text and images while logistic regression works with already identified independent variables.\n",
    "\n",
    "The risk of overfitting is less in SVM, while Logistic regression is vulnerable to overfitting. Logistic regression and SVM with a linear kernel have similar performance\n",
    "\n",
    "Given the results, our top **SVM model have performed better than the top logistic regression model**. Top SVM model has higher AUC score of `0.84` compared to top logistic regression model's score of `0.80`. The top accuracy score for SVM model is `0.90` which is higher than top accuracy score for logistic regression of `0.87`. Also, the precision score for top SVM model is `0.87` which is higher than precision score for top logistic regression model of `0.83`.\n",
    "\n",
    "In terms of model-run duration, it took around 2.5 hours to run 10 iterations of SVM model with radial kernel (rbg).\n",
    "The duration for logistic regression was around 35 minutes which is significantly faster than SVM models. \n",
    "\n",
    "Logistic regression is very efficient in terms of run-time, due to which we were able to run _over 1200 combination_ of various hypertuning parameters and were able to identify the best combination for our dataset. We used that combination to run for SVM.\n",
    "\n",
    "Though SVM gives a overall better predictability, it may not always be feasible to validate different model combinations with SVM. We recommend to first run Logistic Regression, identify the most suitable variations, and run SVM with those variations.\n",
    "\n",
    "Reference: https://medium.com/axum-labs/logistic-regression-vs-support-vector-machines-svm-c335610a3d16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d78e2",
   "metadata": {},
   "source": [
    "## Feature Importance - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Logistic Regression Model cross validation and performance metrics \n",
    "\n",
    "top_lr_model = LogisticRegression(penalty='none', C=100, class_weight='none', solver='saga')\n",
    "target_names = ['No', 'Yes']\n",
    "del df_model['RainTomorrow']\n",
    "X = df_model.values\n",
    "\n",
    "y=df_impute['RainTomorrow']\n",
    "top_lr_cv_data = StratifiedKFold(n_splits=10, random_state=123, shuffle=True)\n",
    "top_lr_cv_data.get_n_splits(X, y)\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "for train_indices, test_indices in top_lr_cv_data.split(X,y): \n",
    "            \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    scl_obj.fit(X_train)\n",
    "\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    X_train_scaled = scl_obj.transform(X_train) \n",
    "    X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "    top_lr_model.fit(X_train_scaled,y_train)  # train object\n",
    "    y_hat = top_lr_model.predict(X_test_scaled) # get test set precitions\n",
    "    classes = top_lr_model.classes_\n",
    "\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "\n",
    "    class_report = classification_report(y_test, y_hat, target_names)\n",
    "    \n",
    "df_model['RainTomorrow']= df_impute['RainTomorrow'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc77b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix for top LR Model:\\n ', conf)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc832a7",
   "metadata": {},
   "source": [
    "The above classification report provides important metrices for our logistic regression model's performance. \n",
    "\n",
    "The `accuracy` is at 88%.\n",
    "\n",
    "The `precision` and `recall` are higher for predicting `No`-`RainTomorrow`values. The model not as efficient with predicing `Yes`-`RainTomorrow`. \n",
    "\n",
    "The weather prediction is very complex algorithm which scientists have developed over the year and may take lot more attributes into consideration. With the dataset we have, we believe our model prediction is good based on `precision` and `recall` scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "weights = pd.Series(top_lr_model.coef_[0],index=df_model.columns)\n",
    "weights.plot(kind='bar', figsize=(14,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e038b",
   "metadata": {},
   "source": [
    "The above plot depicts the weights of the features from the top logistic regression model.\n",
    "\n",
    "As we can see some features have much higher weight then the others. As per the model following features are of high importance:\n",
    "\n",
    "Positive relation:\n",
    "\n",
    "`Coud3pm` - The clouds at 3pm (afternoon) has high importance in predicting the chances of `RainTomorrow`. This is expected as clouds in the afternoon is expected for the rains the next day.\n",
    "\n",
    "`Pressure9am` - The pressure in the morning is negatively correlated to pressure in the afternoon in terms of predicting the `RainTomorrow`. If the Pressure is high in the morning this increases the chance of Rain the next day as by the afternoon we may observe the change in pressure.\n",
    "\n",
    "`Humidity3pm` - As expected the higher humidity in afternoon has higher predictability for `RainTomorrow`.\n",
    "\n",
    "`WindGustSpeed` - The increase in `WindGustSpeed` has positive impact on the prediction for `RainTomorrow`. Which meets the expectation as the Pressure reduces we may see higher Wind Gusts and may lead to Rains the next day.\n",
    "\n",
    "Negative relation:\n",
    "\n",
    "`Sunshine` - More sunshine leads to lower chances on `RainTomorrow`. This makes sense as this means the cloud cover is less and chances are rain reduces significantly.\n",
    "\n",
    "`Pressure3pm` - Lower pressure in the afternoon leads to higher chances of `RainTomorrow`. This is also expected as the pressure towards the later part of day will have impact on the rain the next day.\n",
    "\n",
    "`MinTemp` - Albeit smaller but the increase in `MinTemp` reduces the chances of `RainTomorrow`. This is in line with the `Sunshine`, as more sunshine will increase the minimum temperature and the chances or Rain will reduce.\n",
    "\n",
    "Surprisingly `RainToday` doesn’t have higher weight and is not among the most important features. One would assume that based on the fact if it rained today the chances of `RainTomorrow` may significantly increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at feature importance \n",
    "\n",
    "import shap # SHAP for Explaining Models\n",
    "shap.initjs()\n",
    "# Create a tree explainer and understanding the values we have \n",
    "shap_ex = shap.LinearExplainer(top_lr_model, X_test)\n",
    "vals = shap_ex.shap_values(X_test)\n",
    "shap.summary_plot(vals, df_model.columns, plot_type=\"bar\")\n",
    "\n",
    "# Reference: https://shap.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b341839",
   "metadata": {},
   "source": [
    "The above plot shows the total impact of each feature on our model. As explained in previous weighted plot, these features have high importance in predicting `RainTomorrow`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effect of all the features through SHAP summary plot:\n",
    "explainer = shap.Explainer(top_lr_model, X_train, feature_names=df_model.columns)\n",
    "shap_values = explainer(X_test)\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "# Reference: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a175f",
   "metadata": {},
   "source": [
    "**Explaining the logistic model for Rain Tomorrow prediction:**\n",
    "\n",
    "SHAP summary plots give us a birds-eye view of feature importance and what is driving it. The above SHAP summary plot is made of many dots. Each dot has three characteristics:\n",
    "\n",
    "The features are listed from high to low rank. The colors indicate how the change in the values of the features impact the output attribute (`RainTomorrow`). For example, on above plot, `Humidity3pm` shows the higher value of it has higher impact on the output. Horizontal location shows whether the effect of that value caused a higher or lower prediction.\n",
    "\n",
    "Some things the summary plot is  able to easily pick out:-\n",
    "\n",
    "Higher means more likely to be positive, so in the plots above the “red” features are actually helping raise the chance for `RainTomorrow`, while the negative features are lowering the chance for `RainTomorrow`.\n",
    "The model ignored around 56 features which were of lower importance in predicting the chances of `RainTomorrow`.\n",
    "\n",
    "Usually `Pressure9am` has moderate effect on the prediction, but there are extreme cases of `WindGustSpeed` where a high value still caused moderate level of prediction.\n",
    "\n",
    "Reference: https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaining why a sample weather record # 200 is classified as Rain Tomorrow (Yes/No)?\n",
    "ind = 200\n",
    "shap.plots.force(shap_values[ind])\n",
    "# Reference: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98153fd",
   "metadata": {},
   "source": [
    "**Interpretation of above prediction:**\n",
    "\n",
    "We predicted -374.07, whereas the base_value is -347. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue. The biggest impact comes from 'Pressure9am' being 1,021. Though the 'Humidity3pm' value has a meaningful effect decreasing the prediction.\n",
    "\n",
    "If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.\n",
    "\n",
    "\n",
    "Reference: https://www.kaggle.com/dansbecker/shap-values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780eeca",
   "metadata": {},
   "source": [
    "## Support Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f3cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "\n",
    "for index, row in top_svm_model.iterrows():\n",
    "    svm_model = row['svm_clf']\n",
    "    print(svm_model.support_vectors_.shape)\n",
    "    print(svm_model.support_.shape)\n",
    "    print(svm_model.n_support_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedb257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019fea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3145e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5710019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f57edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
