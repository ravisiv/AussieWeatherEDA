{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e973b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e87e6a4d",
   "metadata": {},
   "source": [
    "# Lab 2 : Classification\n",
    "\n",
    "### Group 3 - Members:\n",
    "\n",
    "_Tai Chowdhury_<br>\n",
    "_Apurv Mittal_<br>\n",
    "_Ravi Sivaraman_<br>\n",
    "_Seemant Srivastava_<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceff67f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a5315",
   "metadata": {},
   "source": [
    "As discussed in Lab 1, we have acquired the Australian Weather dataset from Kaggle portal. It contains 10 years of weather data collected from many locations across Australia. These are daily weather observations. There are 145,459 observations with 23 attributes in the original dataset. \n",
    "\n",
    "We have chosen RainTomorrow (categorical) and Rainfall (continuous) as predictor variables. RainTomorrow is a categorical attribute which indicates whether it is going to rain tomorrow - yes or no. Rainfall is a continuous attribute that measures amount of rainfall each of the particular locations have received (in mm). Using our models, we will be able to design an algorithm where the bureau can help to predict rainfall for different regions in Australia.\n",
    "\n",
    "In this Lab 2 assignment, we have measured the accuracy and effectiveness of our model for categorical variable RainTomorrow by using 10-fold cross validation against the confusion matrix measurements like: Precision, Recall and Accuracy. We have explored the methods of logistic regression and support vector machine (SVM) models on our dataset. \n",
    "\n",
    "We have used `scikit-learn` packages for our exploration. We ran logistic regression models with all the available solvers in the `scikit-learn` package and compare the effictiveness and accuracy of the model to predict `RainfallTomorrow`. We also measured the duration of model run from each models to compare model performance and efficiency as well.\n",
    " \n",
    "To get started, we will start with loading all the necessary packages for our analysis. We will start our analysis with `df_impute` which is the imputed dataframe from our last explanatory data analysis Lab 1 project. Using this dataframe will ensure data consistency for all the labs going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eae993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from shapely.geometry import Point\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "812bb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Warnings on final\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930ef93",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "We imputed data in EDA and reusing imputed data from EDA (Lab1) project.\n",
    "Here is the link to the EDA\n",
    "\n",
    "https://nbviewer.jupyter.org/github/ravisiv/AussieWeatherEDA/blob/c0ba412cb75da21eba386ea9ea39f645ad6af1d0/DS7331_Lab1_Group3_Ravi_Taifur_Seemant_Apurv_Submission.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183efa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Imputed Australia weather data\n",
    "df_impute = pd.read_csv(\"weatherAUS_imputed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3c6a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>WNW</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.684394</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>WSW</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>3.635105</td>\n",
       "      <td>3.684394</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>WSW</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>3.635105</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>E</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>3.635105</td>\n",
       "      <td>3.684394</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>NW</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine WindGustDir  \\\n",
       "0     13.4     22.9       0.6     6.032209  8.890686           W   \n",
       "1      7.4     25.1       0.0     6.032209  8.890686         WNW   \n",
       "2     12.9     25.7       0.0     6.032209  8.890686         WSW   \n",
       "3      9.2     28.0       0.0     6.032209  8.890686          NE   \n",
       "4     17.5     32.3       1.0     6.032209  8.890686           W   \n",
       "\n",
       "   WindGustSpeed WindDir9am WindDir3pm  WindSpeed9am  ...  Humidity9am  \\\n",
       "0           44.0          W        WNW          20.0  ...         71.0   \n",
       "1           44.0        NNW        WSW           4.0  ...         44.0   \n",
       "2           46.0          W        WSW          19.0  ...         38.0   \n",
       "3           24.0         SE          E          11.0  ...         45.0   \n",
       "4           41.0        ENE         NW           7.0  ...         82.0   \n",
       "\n",
       "   Humidity3pm  Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  \\\n",
       "0         22.0       1007.7       1007.1  8.000000  3.684394     16.9   \n",
       "1         25.0       1010.6       1007.8  3.635105  3.684394     17.2   \n",
       "2         30.0       1007.6       1008.7  3.635105  2.000000     21.0   \n",
       "3         16.0       1017.6       1012.8  3.635105  3.684394     18.1   \n",
       "4         33.0       1010.8       1006.0  7.000000  8.000000     17.8   \n",
       "\n",
       "   Temp3pm  RainToday RainTomorrow  \n",
       "0     21.8         No           No  \n",
       "1     24.3         No           No  \n",
       "2     23.2         No           No  \n",
       "3     26.5         No           No  \n",
       "4     29.7         No           No  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  View the top rows of the data imported\n",
    "df_impute.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa8301",
   "metadata": {},
   "source": [
    "The imputed data doesn't include any null or missing values. Also, we have dropped the columns like: Date of observation and City Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b411ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Variables: Index(['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
      "       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
      "       'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm',\n",
      "       'Temp9am', 'Temp3pm'],\n",
      "      dtype='object')\n",
      "Categorical Variables: Index(['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_impute_num = df_impute.columns[df_impute.dtypes == 'float64']\n",
    "df_impute_cat=df_impute.columns[df_impute.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_impute_num)\n",
    "print(\"Categorical Variables:\", df_impute_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cad24",
   "metadata": {},
   "source": [
    "Before continuing further, we need to check which variables are numeric and which are not. As the models expect numerical variables. We will filter and identify non-numeric variables.\n",
    "\n",
    "`WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday` and `RainTomorrow`are not numeric. Here `RainTomorrow` is our response variable. we handle the other variables with hot-one-encoding later in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bed27653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the original data\n",
    "df_model = df_impute.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7640a88",
   "metadata": {},
   "source": [
    "Creating a new DataFrame `df_model` for modeling to avoid any changes to the original dataset `df_impute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cd3b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable to Identify if it RainToday\n",
    "\n",
    "df_model[\"IsRainToday\"] = df_impute['RainToday']\n",
    "\n",
    "# Replacing No with 0 and Yes with 1.\n",
    "\n",
    "df_model['IsRainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c1422",
   "metadata": {},
   "source": [
    "Assigning `0` to No values and `1` to Yes values in `RainToday` (Changed to `IsRainToday`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7216f8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_impute (140787, 21)\n",
      "df_model (140787, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_impute\", df_impute.shape)\n",
    "print(\"df_model\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af97c6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>IsRainToday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>WNW</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.684394</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>WSW</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>3.635105</td>\n",
       "      <td>3.684394</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>WSW</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>3.635105</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>E</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>3.635105</td>\n",
       "      <td>3.684394</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>NW</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine WindGustDir  \\\n",
       "0     13.4     22.9       0.6     6.032209  8.890686           W   \n",
       "1      7.4     25.1       0.0     6.032209  8.890686         WNW   \n",
       "2     12.9     25.7       0.0     6.032209  8.890686         WSW   \n",
       "3      9.2     28.0       0.0     6.032209  8.890686          NE   \n",
       "4     17.5     32.3       1.0     6.032209  8.890686           W   \n",
       "\n",
       "   WindGustSpeed WindDir9am WindDir3pm  WindSpeed9am  ...  Humidity3pm  \\\n",
       "0           44.0          W        WNW          20.0  ...         22.0   \n",
       "1           44.0        NNW        WSW           4.0  ...         25.0   \n",
       "2           46.0          W        WSW          19.0  ...         30.0   \n",
       "3           24.0         SE          E          11.0  ...         16.0   \n",
       "4           41.0        ENE         NW           7.0  ...         33.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1  8.000000  3.684394     16.9     21.8         No   \n",
       "1       1010.6       1007.8  3.635105  3.684394     17.2     24.3         No   \n",
       "2       1007.6       1008.7  3.635105  2.000000     21.0     23.2         No   \n",
       "3       1017.6       1012.8  3.635105  3.684394     18.1     26.5         No   \n",
       "4       1010.8       1006.0  7.000000  8.000000     17.8     29.7         No   \n",
       "\n",
       "   RainTomorrow IsRainToday  \n",
       "0            No           0  \n",
       "1            No           0  \n",
       "2            No           0  \n",
       "3            No           0  \n",
       "4            No           0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the values to check if the data looks good\n",
    "\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f53f7c",
   "metadata": {},
   "source": [
    "We can observe that the presence of “0” and “1” is almost in the 78:22 ratio. So there is a class imbalance and we have to deal with it. To fight against the class imbalance, we will use here the oversampling of the minority class. Since the size of the dataset is quite small, majority class subsampling wouldn’t make much sense here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbca128",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c7712",
   "metadata": {},
   "source": [
    "Before we create our models, we need to format our attributes. We are converting `RainToday` and `RainTomorrow` into numeric variables to `0` and `1`. We also decided to go ahead with one-hot-encoding `WindGustDir`, `WindDir9am`, and `WindDir3pm` attributes based on the direction of the wind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c1800d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding using dummies\n",
    "\n",
    "gust_df = pd.get_dummies(df_model.WindGustDir,prefix='GustDir', drop_first= True)\n",
    "wind3pm_df = pd.get_dummies(df_model.WindDir3pm,prefix='Wind3pm', drop_first= True)\n",
    "wind9am_df = pd.get_dummies(df_model.WindDir9am,prefix='Wind9am' , drop_first= True)\n",
    "df_model = pd.concat((df_model,gust_df, wind3pm_df, wind9am_df),axis=1) # add back into the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c39f8",
   "metadata": {},
   "source": [
    "We decided to do one-hot-encoding using dummies function as machine learning algorithms and models requires numerical values for both input and output attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1ffb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical columns\n",
    "\n",
    "df_model = df_model.drop(['WindDir3pm', 'WindDir9am', 'WindGustDir', 'RainToday'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b5c09",
   "metadata": {},
   "source": [
    "After conversions, we are removing these categorical attributes to avoid duplicates as we have those data in numerical format. We are added the newly formatted attributes and rest of the continuous attributes into a new dataframe - df_model. We will use the new dataframe for modeling.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0fb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if Yes is replaced as 1\n",
    "\n",
    "print(\"Are there 1's and 0's in the RainToday column?\", \n",
    "      (df_model['IsRainToday'].sum() > 0) and (df_model['IsRainToday'].sum() < len(df_model['IsRainToday'])))\n",
    "\n",
    "#Non zero output means there is a mixture of 1's and 0's\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433c629",
   "metadata": {},
   "source": [
    "Checking if the data imputation happened accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_num = df_model.columns[df_model.dtypes == 'float64']\n",
    "df_model_cat=df_model.columns[df_model.dtypes == 'object']\n",
    "print(\"Numeric Variables:\", df_model_num)\n",
    "print(\"Categorical Variables:\", df_model_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03081b6",
   "metadata": {},
   "source": [
    "Check if all the numerical variables are accurately created and if we still have any non-numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7f8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=df_model[df_model_num]\n",
    "y = df_model.RainTomorrow\n",
    "print('features shape:', X.shape) \n",
    "print('target shape:', y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d694e12",
   "metadata": {},
   "source": [
    "Assigning the `RainTomorrow` as our response variable (y) and all other variables include one-hot-encoded values as X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c1b04",
   "metadata": {},
   "source": [
    "### Data Distribution\n",
    "\n",
    "Check if the data distribution is balanced or not for the response variable `RainTomorrow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "df_model['RainTomorrow'].value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\n",
    "plt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63833e9",
   "metadata": {},
   "source": [
    "As expected, we see the data for `RainTomorrow` is imbalanced. Majority of the data is for `No` rain vs. `Yes` for `RainTomorrow`.\n",
    "\n",
    "We can observe that the presence of `0` and `1` is almost in the `78:22` ratio. We will be cognizant of the fact that our model may be not very effective if we don't solve for imbalance. We will discuss and adjust for this imbalance in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f4251",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936120f",
   "metadata": {},
   "source": [
    "### Explanation of Evaluation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c8273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e59b384",
   "metadata": {},
   "source": [
    "#### Macro Avg vs Weight Avg Precision comparisons\n",
    "\n",
    "As we couldn't pick the models just from Accuracy, we are further our analysis by plotting the difference between the Macro Avg precision and Weighted Avg precision for all the models.\n",
    "\n",
    "`DiffMacro/WeightedNone` — takes the Macro Average and Weighted Average of Precision from the classification matrix and calculates the difference. The reason to calculate the difference is to check how much variation is in the Precision values based on how the data is split.\n",
    "\n",
    "`DiffMacro/WeightedBalanced` — same as above this variable calculates the difference between Macro and Weighted Average of Precision of a balanced data.\n",
    "\n",
    "The smaller the difference between Weight Avg and Macro Avg, the model is closer to real world and also consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fda401",
   "metadata": {},
   "source": [
    "After running the big `Logistic Regression` model for several combinations. We got output for 660 models.\n",
    "\n",
    "To compare the outputs and make it easier to identify the most appropriate model in terms of accuracy, precision and other factors applicable for Machine Learning models. We decided to put a list of subset of variables including:\n",
    "\n",
    "`Model` — specifies which technique was used for the logistic model. In this case its between ShuffleSplit and Stratified.\n",
    "\n",
    "`AccuracyNone` —  specifies the Accuracy observed by the model where data was not `balanced`.\n",
    "\n",
    "`DiffMacro/WeightedNone` — takes the `Macro Average` and `Weighted Average` of `Precision` from the classification matrix and calculates the difference. The reason to calculate the difference is to check how much variation is in the Precision values based on how the data is split. More details about Precision and Averages is provided below. This variation is calculated on the non `balanced` data.\n",
    "\n",
    "`AccuracyBalanced` — specifies the Accuracy observed by the model where data was `balanced`.\n",
    "\n",
    "`DiffMacro/WeightedBalanced` — same as above this variable calculates the difference between Macro and Weighted Average of Precision of a `balanced` data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709afdb",
   "metadata": {},
   "source": [
    "### Important Terms\n",
    "\n",
    "`Accuracy` — is a ratio of correctly predicted observation to the total observations. It is a very important aspect to define the success of a model but just the measurement on its own can be deceiving if the observations are not equal for each class. In such cases we might be predicting accurately for one particular class with large observation and may not do very well for other classes.\n",
    "\n",
    "`Precision` — is the ratio of correctly predicted positive observations to the total predicted positive observations. Precision = TP/TP+FP  \n",
    "\n",
    "`Weighted Average` — can be calculated on various output variables of the classification report like Precision, Recall, f1-score. As the name suggests it gives the weighted average of the parameter based on the number of observations or values for each class.\n",
    "\n",
    "`Macro Average` — similar to weighted average, macro average can also be calculated on various output variables of the classification report like Precision, Recall, f1-score. However, the similarity ends here as unlike weighted average, we don't use weights based on the number of observations, rather equal weights are given to each class to calcualte the value. This tells us if the Precision is as good if the dataset was balanced.\n",
    "\n",
    "References:\n",
    "https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "https://datascience.stackexchange.com/questions/65839/macro-average-and-weighted-average-meaning-in-classification-report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093238d",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232750f3",
   "metadata": {},
   "source": [
    "In these models we used two different methodologies for cross validation namely `Shuffle` and `Stratified`.\n",
    "\n",
    "`ShuffleSplit`  — is similar to `Cross Validation` where we can specify the percentage of split for train and test data. However, in regular cross-validation, the data is not split randomly, so, it is good to shuffle the targets before applying the `cross-validation`.\n",
    "\n",
    "\n",
    "`Stratified` — CV technique is very useful with unbalanced dataset. As discussed above our dataset is not balanced and rightly so, we don't expect it to rain and no-rain days to be equal in Australia. The data is expected to be unbalanced and expected to be such in future as well. So using stratified sampling techniques gives us the ability to preserve the proportion of the Rain days vs non-rain days in our dataset. We can be confident that the Train and Test split data is not leaving out important information like entire dataset is of `No` rain days which will give highly inaccurate output eventhough the accuracy might be maintained. \n",
    "\n",
    "In `Stratified` sampling, the data is k-1 split in favor of Train vs Test data.\n",
    "\n",
    "\n",
    "References: \n",
    "https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d\n",
    "https://mclguide.readthedocs.io/en/latest/sklearn/cv.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82e011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa639453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99ed2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcc5dfea",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535319b",
   "metadata": {},
   "source": [
    "### Creating Logistic Regression Function\n",
    "\n",
    "The function below creates models with various parameters supported by LogisticRegression. There are various combinations of\n",
    "1. `Model Type`\n",
    "2. `Class Weight`\n",
    "3. `Solver`\n",
    "4. `C`\n",
    "5. `Penalty`\n",
    "6. `Iterations`\n",
    "\n",
    "This function runs for specific Solver, C, Penalty but takes various iterations, and we used 10 iterations.\n",
    "When this function called with Solver, C, Penalty, the function runs for BOTH Class Weight, `balanced` and `None`. The result of both `balanced` and `None` is stored as single row in a dataframe.\n",
    "They are stored in the same row to verify how `balanced` and `None` compares.\n",
    "\n",
    "##### Model Type\n",
    "\n",
    "1. `Shuffle`\n",
    "2. `Stratified`\n",
    "\n",
    "`ShuffleSplit`  — is similar to `Cross Validation` where we can specify the percentage of split for train and test data. However, in regular cross-validation, the data is not split randomly, so, it is good to shuffle the targets before applying the `cross-validation`.\n",
    "\n",
    "\n",
    "`Stratified` — CV technique is very useful with unbalanced dataset. As discussed above our dataset is not balanced and rightly so, we don't expect it to rain and no-rain days to be equal in Australia. The data is expected to be unbalanced and expected to be such in future as well. So using stratified sampling techniques gives us the ability to preserve the proportion of the Rain days vs non-rain days in our dataset. We can be confident that the Train and Test split data is not leaving out important information like entire dataset is of `No` rain days which will give highly inaccurate output eventhough the accuracy might be maintained. \n",
    "\n",
    "In `Stratified` sampling, the data is k-1 split in favor of Train vs Test data.\n",
    "\n",
    "\n",
    "References: \n",
    "https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d\n",
    "https://mclguide.readthedocs.io/en/latest/sklearn/cv.html\n",
    "\n",
    "\n",
    "##### Solver Options\n",
    "Scikit-learn comes with five different solver options. Each solver minimize the cost function. Here are the five options:\n",
    "\n",
    "`newton-cg` — A newton method. Newton methods use an exact Hessian matrix. It's slow for large datasets, because it computes the second derivatives.\n",
    "\n",
    "`lbfgs` — Stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno. It approximates the second derivative matrix updates with gradient evaluations. It stores only the last few updates, so it saves memory. It isn't super fast with large data sets.\n",
    "\n",
    "`liblinear` — Library for Large Linear Classification. Uses a coordinate descent algorithm. Coordinate descent is based on minimizing a multivariate function by solving univariate optimization problems in a loop. In other words, it moves toward the minimum in one direction at a time. It performs pretty well with high dimensionality. It does have a number of drawbacks. It can get stuck, is unable to run in parallel, and can only solve multi-class logistic regression with one-vs.-rest.\n",
    "\n",
    "`sag` — Stochastic Average Gradient descent. A variation of gradient descent and incremental aggregated gradient approaches that uses a random sample of previous gradient values. Fast for big datasets.\n",
    "\n",
    "`saga` — Extension of sag that also allows for L1 regularization. Should generally train faster than sag.\n",
    "\n",
    "Reference for above definitions: https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "\n",
    "##### Penalty\n",
    "\n",
    "Used to specify the norm used in the penalization. The `newton-cg`’, `sag` and `lbfgs` solvers support only `l2` penalties. `elasticnet` is only supported by the `saga` solver. If `none` (not supported by the `liblinear` solver), no regularization is applied.\n",
    "\n",
    "`L1`  or `Lasso` — (`LASSO` is Least Absolute Shrinkage and Selection Operator) uses a penalized least squares approach that squeezes the regression coefficients to 0 when the penalty is large. The algorithm starts with a large penalty and gradually relaxes the penalty to allow for a single variable to be added into the model (the coefficient is no longer `0`). `LASSO` uses `L1` method.\n",
    "\n",
    "`L2` or `Ridge` — adds a penalty equal to the square of the magnitude of coefficients. `L2` will not yield sparse models and all coefficients are shrunk by the same factor (not eliminated). Ridge regression and SVMs use this method.\n",
    "\n",
    "`elasticnet`  — Procedure identical to `LASSO` however the penalty is different. `elasticnet` uses a combination of both the `LASSO` penalty as well as the `RIDGE` regression penalty. It combine both `L1` & `L2` methods.\n",
    "\n",
    "`none` — No penalty\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Reference for Penalty definitions: https://www.statisticshowto.com/regularization/\n",
    "\n",
    "##### C\n",
    "`C` is cost, which we run with `1.0`, `10.0` and `100.0` for each combination of Solver and Penalty.\n",
    "\n",
    "If the solver doesn't support the penalty, those are skipped.\n",
    "\n",
    "\n",
    "The dataframe stores:\n",
    "\n",
    "1. `Model Type`\n",
    "2. `Solver`\n",
    "3. `C`\n",
    "4. `Penalty`\n",
    "5. `Iteration`\n",
    "6. `AccuracyNone`\n",
    "7. `MacroAvgPrecisionNone`\n",
    "7. `WeightedAvgPrecisionNone`\n",
    "8. `DiffMacro/WeightedNone`\n",
    "9. `fprNone`\n",
    "10. `tprNone`,\n",
    "11. `AccuracyBalanced`\n",
    "12. `MacroAvgPrecisionBalanced`\n",
    "13. `WeightedAvgPrecisionBalanced`\n",
    "14. `DiffMacro/WeightedBalanced`\n",
    "15. `fprBalanced`\n",
    "16. `tprBalanced`\n",
    "17. `Classes`\n",
    "\n",
    "`DiffMacro/WeightedBalanced` is the difference between the Presicion of `macro avg` and `Weighted`. Smaller the value, more closer they are.\n",
    "\n",
    "`fprNone` and `tprNone` are stored to run ROC curve.\n",
    "\n",
    "`Classes` is `Yes` and `No` as an array, which are used for labels for plots. They are constant for all rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b29947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#model stats dataframe columns for class weight balanced and None\n",
    "model_stats_columns=[\"Model\",\"Solver\", \"C\", \"Penalty\",\"Iteration\",\n",
    "                         \"AccuracyNone\", \n",
    "                         \"MacroAvgPrecisionNone\",\"MacroAvgRecallNone\", \"MacroAvgf1-scoreNone\",\n",
    "                         \"WeightedAvgPrecisionNone\", \"WeightedAvgRecallNone\", \"WeightedAvgf1-scoreNone\",\n",
    "                         \"DiffMacro/WeightedNone\",\"fprNone\", \"tprNone\",\"lr_clfNone\",\n",
    "                         \"AccuracyBalanced\",\n",
    "                         \"MacroAvgPrecisionBalanced\",\"MacroAvgRecallBalanced\", \"MacroAvgf1-scoreBalanced\",\n",
    "                         \"WeightedAvgPrecisionBalanced\", \"WeightedAvgRecallBalanced\", \"WeightedAvgf1-scoreBalanced\",\n",
    "                         \"DiffMacro/WeightedBalanced\",\"fprBalanced\", \"tprBalanced\", \"lr_clfBalanced\",\n",
    "                         \"Classes\", \"Time\"\n",
    "                        ]\n",
    "\n",
    "\n",
    "def create_log_models(model_type,df, iterations,penalty, C, solver):\n",
    "    class_weight = ['balanced', None]\n",
    "    \n",
    "    #Create logreg object for both class weights\n",
    "    lr_clf_balanced = LogisticRegression(penalty=penalty, C=C, class_weight='balanced', solver=solver) \n",
    "    lr_clf_none = LogisticRegression(penalty=penalty, C=C, class_weight=None, solver=solver)\n",
    "    \n",
    "    #Store both objects in a dict for later retrival\n",
    "    lr_clf_dict = { \n",
    "                    \"balanced\": lr_clf_balanced,\n",
    "                    \"None\": lr_clf_none\n",
    "                    }\n",
    "\n",
    "    num_cv_iterations = iterations\n",
    "\n",
    "   \n",
    "    if \"RainTomorrow\" in df:\n",
    "        y = df[\"RainTomorrow\"].values # get the labels we want\n",
    "        del df[\"RainTomorrow\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    num_instances = len(y)\n",
    "    \n",
    "    cv_data = None\n",
    "    if model_type == \"shuffle\":\n",
    "        cv_data = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                             test_size  = 0.2, random_state = 123)\n",
    "\n",
    "    elif model_type == \"stratified\":\n",
    "        cv_data = StratifiedKFold(n_splits=iterations, random_state=123, shuffle=True)\n",
    "        cv_data.get_n_splits(X, y)\n",
    "       \n",
    "    #Initialize variables\n",
    "    iter_num=0\n",
    "    rows = []\n",
    "    stats_dict = {}\n",
    "    target_names = ['No', 'Yes']\n",
    "    classes = None    \n",
    "    scl_obj = StandardScaler()\n",
    "    lr_clf = None\n",
    "    # Run for balanced first with same model and then None with same model\n",
    "    # store the results in same row of dataframe\n",
    "    # This helps to compare none and balanced macro avg\n",
    "   \n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        starttime = timer()\n",
    "        for cw in class_weight:\n",
    "            X_train = X[train_indices]\n",
    "            y_train = y[train_indices]\n",
    "        \n",
    "            scl_obj.fit(X_train)\n",
    "\n",
    "            X_test = X[test_indices]\n",
    "            y_test = y[test_indices]\n",
    "            \n",
    "            #Get the logistic regression object for the current cw class weight\n",
    "            key = None\n",
    "            if cw == None:\n",
    "                key = \"None\"\n",
    "            else:\n",
    "                key = cw\n",
    "                    \n",
    "            lr_clf = lr_clf_dict[key]\n",
    "            \n",
    "            try:\n",
    "                X_train_scaled = scl_obj.transform(X_train) \n",
    "                X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "                lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "                y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "                classes = lr_clf.classes_\n",
    "\n",
    "                acc = mt.accuracy_score(y_test,y_hat)\n",
    "                conf = mt.confusion_matrix(y_test,y_hat)\n",
    "                \n",
    "                class_report = classification_report(y_test, y_hat, target_names, output_dict=True)\n",
    "                \n",
    "                # Macro avg stats\n",
    "                macro_avg_precision = class_report[\"macro avg\"][\"precision\"]\n",
    "                macro_avg_recall = class_report[\"macro avg\"][\"recall\"]\n",
    "                macro_avg_f1_score = class_report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "                #Weighted avg stats\n",
    "                weighted_avg_precision = class_report[\"weighted avg\"][\"precision\"]\n",
    "                weighted_avg_recall = class_report[\"weighted avg\"][\"recall\"]\n",
    "                weighted_avg_f1_score = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "                # Create ROC Curve\n",
    "                y_test_01 = np.where(y_test ==\"Yes\", 1, [0])\n",
    "                y_hat_01 = np.where(y_hat ==\"Yes\", 1, [0])\n",
    "\n",
    "                fpr, tpr, threshold = metrics.roc_curve(y_test_01, y_hat_01)\n",
    "                \n",
    "                #Create a dict of these stats for class weight\n",
    "                #dict will contain stats for balanced on one run, None for the next run\n",
    "                \n",
    "                stats_dict[key] = [acc, \n",
    "                          macro_avg_precision, macro_avg_recall, macro_avg_f1_score,\n",
    "                          weighted_avg_precision,weighted_avg_recall,weighted_avg_f1_score,\n",
    "                          abs(weighted_avg_precision-macro_avg_precision),         \n",
    "                          fpr, tpr, lr_clf ]\n",
    "\n",
    "                print(model_type, solver,cw,C,penalty,iter_num,\"✅\")\n",
    "            except Exception as e:\n",
    "                #print('Error:', str(e))\n",
    "                raise\n",
    "            #end try block  \n",
    "        #end first for loop\n",
    "        #When cursor comes here, model has ran for both None and balanced\n",
    "        #Create a single row of lists combining none and balanced \n",
    "        endtime = timer()\n",
    "        time_taken = endtime - starttime\n",
    "        row = [model_type, solver,C,penalty,iter_num] + stats_dict[\"None\"] + stats_dict['balanced'] + [classes, time_taken ]\n",
    "        rows.append(row)\n",
    "        iter_num+=1\n",
    "    #end next for loop\n",
    "    \n",
    "    #Create a dataframe with the model stats \n",
    "    df_ret = pd.DataFrame(rows, columns = model_stats_columns)\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cada3",
   "metadata": {},
   "source": [
    "### Running Logistic Regression with various options\n",
    "\n",
    "The code below runs all the combinations of `Penalty`, `Solver`, `C` and shuffle types and stores in dataframe for further analysis later on.\n",
    "\n",
    "If any combinations of `Solver` and `Penalty` isn't supported by Logistic Regression, they are skipped.\n",
    "\n",
    "`iteration` — 10 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956a40b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "penalty=['l1','l2', 'elasticnet', 'none']\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "model_type = ['shuffle', 'stratified']\n",
    "C=[1.0, 10.0, 100.0]\n",
    "iterations = 10\n",
    "\n",
    "#penalty=['l2']\n",
    "#solver = ['liblinear']\n",
    "#model_type = [\"stratified\"]\n",
    "#C=[1.0]\n",
    "#iterations = 2\n",
    "\n",
    "model_perf_df = pd.DataFrame(columns= model_stats_columns)\n",
    "\n",
    "# Run all combinations of penalty, solver, model_type, C\n",
    "# Create a giant dataframe\n",
    "# Each row of dataframe contains stats for each combination of penalty, solver, model_type,C\n",
    "# same row contains the stats for both None and balanced class weight\n",
    "# so we can compare the balanced and None\n",
    "# and pick the model whose diff in precision (for macro avg) is lowest with highest accuracy\n",
    "# This model is the closest to real world\n",
    "\n",
    "for pen in penalty:\n",
    "    for c_index in C:\n",
    "        for solv in solver:\n",
    "            for mdl_type in model_type:\n",
    "                try:\n",
    "\n",
    "                    df_ret = create_log_models(model_type=mdl_type,df=df_model,iterations=iterations, penalty=pen, C=c_index, solver=solv)\n",
    "                    \n",
    "                    model_perf_df = model_perf_df.append(df_ret, ignore_index=True)\n",
    "                    \n",
    "                    #Model deletes RainTomorrow from dataframe\n",
    "                    #put it back from imputed data to run for another set of model\n",
    "                    df_model[\"RainTomorrow\"] = df_impute[\"RainTomorrow\"].values\n",
    "                except Exception as e:\n",
    "                    #print(\"Error in running\", str(e))\n",
    "                    continue\n",
    "                            \n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4512a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_model_copy = model_perf_df.copy()\n",
    "#model_perf_df = perf_model_copy.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d35021",
   "metadata": {},
   "source": [
    "In these models we used two different methodologies for cross validation namely `Shuffle` and `Stratified`.\n",
    "\n",
    "`ShuffleSplit`  — is similar to `Cross Validation` where we can specify the percentage of split for train and test data. However, in regular cross-validation, the data is not split randomly, so, it is good to shuffle the targets before applying the `cross-validation`.\n",
    "\n",
    "\n",
    "`Stratified` — CV technique is very useful with unbalanced dataset. As discussed above our dataset is not balanced and rightly so, we don't expect it to rain and no-rain days to be equal in Australia. The data is expected to be unbalanced and expected to be such in future as well. So using stratified sampling techniques gives us the ability to preserve the proportion of the Rain days vs non-rain days in our dataset. We can be confident that the Train and Test split data is not leaving out important information like entire dataset is of `No` rain days which will give highly inaccurate output eventhough the accuracy might be maintained. \n",
    "\n",
    "In `Stratified` sampling, the data is k-1 split in favor of Train vs Test data.\n",
    "\n",
    "\n",
    "References: \n",
    "https://towardsdatascience.com/understanding-8-types-of-cross-validation-80c935a4976d\n",
    "https://mclguide.readthedocs.io/en/latest/sklearn/cv.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfa24e",
   "metadata": {},
   "source": [
    "After running the big `Logistic Regression` model for several combinations. We got output for 660 models.\n",
    "\n",
    "To compare the outputs and make it easier to identify the most appropriate model in terms of accuracy, precision and other factors applicable for Machine Learning models. We decided to put a list of subset of variables including:\n",
    "\n",
    "`Model` — specifies which technique was used for the logistic model. In this case its between ShuffleSplit and Stratified.\n",
    "\n",
    "`AccuracyNone` —  specifies the Accuracy observed by the model where data was not `balanced`.\n",
    "\n",
    "`DiffMacro/WeightedNone` — takes the `Macro Average` and `Weighted Average` of `Precision` from the classification matrix and calculates the difference. The reason to calculate the difference is to check how much variation is in the Precision values based on how the data is split. More details about Precision and Averages is provided below. This variation is calculated on the non `balanced` data.\n",
    "\n",
    "`AccuracyBalanced` — specifies the Accuracy observed by the model where data was `balanced`.\n",
    "\n",
    "`DiffMacro/WeightedBalanced` — same as above this variable calculates the difference between Macro and Weighted Average of Precision of a `balanced` data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8122eb",
   "metadata": {},
   "source": [
    "### Important Terms\n",
    "\n",
    "`Accuracy` — is a ratio of correctly predicted observation to the total observations. It is a very important aspect to define the success of a model but just the measurement on its own can be deceiving if the observations are not equal for each class. In such cases we might be predicting accurately for one particular class with large observation and may not do very well for other classes.\n",
    "\n",
    "`Precision` — is the ratio of correctly predicted positive observations to the total predicted positive observations. Precision = TP/TP+FP  \n",
    "\n",
    "`Weighted Average` — can be calculated on various output variables of the classification report like Precision, Recall, f1-score. As the name suggests it gives the weighted average of the parameter based on the number of observations or values for each class.\n",
    "\n",
    "`Macro Average` — similar to weighted average, macro average can also be calculated on various output variables of the classification report like Precision, Recall, f1-score. However, the similarity ends here as unlike weighted average, we don't use weights based on the number of observations, rather equal weights are given to each class to calcualte the value. This tells us if the Precision is as good if the dataset was balanced.\n",
    "\n",
    "References:\n",
    "https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "https://datascience.stackexchange.com/questions/65839/macro-average-and-weighted-average-meaning-in-classification-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261832e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot\n",
    "import seaborn\n",
    "\n",
    "def draw_scatter(df, x,y, title=None):\n",
    "\n",
    "    seaborn.set(style='ticks')\n",
    "    cv_type = ['shuffle', 'stratified']\n",
    "\n",
    "    #fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61, ylim=(0,1))\n",
    "    fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)\n",
    "    fg.map(pyplot.scatter, x, y ).add_legend()\n",
    "    #fg.suptitle(title)\n",
    "\n",
    "    for i, ax in enumerate(fg.fig.axes):   \n",
    "         ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n",
    "\n",
    "#Reference: https://stackoverflow.com/questions/14885895/color-by-column-values-in-matplotlib\n",
    "#Ref for Axis rotation: https://stackoverflow.com/questions/26540035/rotate-label-text-in-seaborn-factorplot/43256409#43256409"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3857f",
   "metadata": {},
   "source": [
    "### Compare Shuffle vs Stratified\n",
    "\n",
    "To compare `Shuffle` and `Stratified`, we plot Shuffle and Stratified with `AccuracyNone` (accuracy for non-balanced data) and `AccuracyBalanced` (accuracy for balanced data).\n",
    "\n",
    "In the plots below, we see the difference between `Shuffle` and `Stratified` in absolute values is very small\n",
    "\n",
    "1. `AccuracyNone` values range from `0.871` to `0.877`, which is `0.006`, is a very small difference\n",
    "2. `AccuraceBalanced` values range from `0.847` to `0.853`, which is `0.006`, is also very small difference\n",
    "\n",
    "Based on the Accuracy alone, we cannot pick any models from one over other. \n",
    "We have to do further analysis of better model, as all models are very close to each other for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90104146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a plot by looking shuffle vs stratefied\n",
    "\n",
    "model_perf_df['Models'] = model_perf_df['Model'] + \" / Solver:\" + model_perf_df['Solver'] + \" / Penalty:\" + model_perf_df['Penalty'] + \" / C:\"+model_perf_df['C'].astype(str)\n",
    "\n",
    "model_perf_df = model_perf_df.sort_values(by=['Models'],ascending=False)\n",
    "\n",
    "#Sort by accuracy and take top 5\n",
    "\n",
    "#model_perf_df = model_perf_df.nlargest(5, 'AccuracyNone')\n",
    "\n",
    "draw_scatter(df=model_perf_df, x='Models', y='AccuracyNone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac079bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a plot by looking shuffle vs stratefied\n",
    "\n",
    "model_perf_df['Models'] = model_perf_df['Model'] + \" / Solver:\" + model_perf_df['Solver'] + \" / Penalty:\" + model_perf_df['Penalty'] + \" / C:\"+model_perf_df['C'].astype(str)\n",
    "\n",
    "model_perf_df = model_perf_df.sort_values(by=['Models'],ascending=False)\n",
    "\n",
    "#Sort by accuracy and take top 5\n",
    "\n",
    "#model_perf_df = model_perf_df.nlargest(5, 'AccuracyNone')\n",
    "\n",
    "draw_scatter(df=model_perf_df, x='Models', y='AccuracyBalanced')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160229d",
   "metadata": {},
   "source": [
    "### Macro Avg vs Weight Avg Precision comparisons\n",
    "\n",
    "As we couldn't pick the models just from `Accuracy`, we are further our analysis by plotting the difference between the `Macro Avg` precision and `Weighted Avg` precision for all the models. \n",
    "\n",
    "\n",
    "   `DiffMacro/WeightedNone` — takes the `Macro Average` and `Weighted Average` of `Precision` from the classification matrix and calculates the difference. The reason to calculate the difference is to check how much variation is in the `Precision` values based on how the data is split. \n",
    "\n",
    "   `DiffMacro/WeightedBalanced` — same as above this variable calculates the difference between Macro and Weighted Average of Precision of a `balanced` data.\n",
    "   \n",
    "The smaller the difference between `Weight Avg` and `Macro Avg`, the model is closer to real world and also consistent.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6cfac",
   "metadata": {},
   "source": [
    "### Weighted Avg vs Macro Avg Precision Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476eaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_type = ['shuffle', 'stratified']\n",
    "fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Models', 'DiffMacro/WeightedBalanced').add_legend()\n",
    "#fg.suptitle('Macro Avg - Weighted Avg Precision of Balanced Class Weight')\n",
    "\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n",
    "#Reference: https://stackoverflow.com/questions/14885895/color-by-column-values-in-matplotlib\n",
    "\n",
    "#Ref for Axis rotation: https://stackoverflow.com/questions/26540035/rotate-label-text-in-seaborn-factorplot/43256409#43256409"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92750ca7",
   "metadata": {},
   "source": [
    "Now, we plotted the models classified into `Shuffle` and `Stratified` against `DiffMacro/WeightedBalanced` of precision (difference between Macro and Weighted Average of Precision of a balanced data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4859e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_type = ['shuffle', 'stratified']\n",
    "fg = seaborn.FacetGrid(data=model_perf_df, hue='Model', hue_order=cv_type, height=8, aspect=1.61)    \n",
    "fg.map(pyplot.scatter, 'Models', 'DiffMacro/WeightedNone').add_legend()\n",
    "#fg.suptitle('Macro Avg - Weighted Avg Precision of None Class Weight')\n",
    "for i, ax in enumerate(fg.fig.axes):   ## getting all axes of the fig object\n",
    "     ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42963bed",
   "metadata": {},
   "source": [
    "Also, plotted the models classified into `Shuffle` and `Stratified` against `DiffMacro/WeightedNone` of precision (difference between Macro and Weighted Average of Precision of non-balanced data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce52fec",
   "metadata": {},
   "source": [
    "From the above two plots, we notice that the difference between Macro Average of Precision and Weighted Average of Precision varies as below:\n",
    "\n",
    "Balanced Data - `0.091` to `0.097`\n",
    "\n",
    "non-Balanced Data - `0.037` to `0.042`\n",
    "\n",
    "There are two major takeaways from the above plots. \n",
    "\n",
    "1. The `stratifiedkfolds` data sees lower variation compared to `shufflesplit`.\n",
    "\n",
    "2. Its evident that the difference between Macro Average of Precision and Weighted Average of Precision varies a lot more for `Balanced` data compared to `non-balanced` data. \n",
    "\n",
    "Since, lower variation is desired to get more stable and consistent model. We decide to go ahead with `Stratified` and non-balanced data. In our model we have hypertuned `shuffle` as `True` for `stratifiedkfolds` as well. Also, we know `stratifiedkfolds` works well with non-balanced data.\n",
    "\n",
    "With above points in mind, we will continue with `Stratified` and `non-balanced` data models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f035f24",
   "metadata": {},
   "source": [
    "## Model cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ada6a",
   "metadata": {},
   "source": [
    "We have established in the previous sections that `StratifiedKFold` works better for our dataset.\n",
    "\n",
    "The below code removes `ShuffleSplit` model from further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets focus on Stratified\n",
    "\n",
    "#Remove all shuffle\n",
    "\n",
    "model_perf_df.drop(model_perf_df[model_perf_df.Model == \"shuffle\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb4936",
   "metadata": {},
   "source": [
    "From the previous sections we have established, we are removing `balanced` `Class Weight` from our further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Columns of balanced\n",
    "\n",
    "balanced_columns=[\"AccuracyBalanced\",\n",
    "                         \"MacroAvgPrecisionBalanced\",\"MacroAvgRecallBalanced\", \"MacroAvgf1-scoreBalanced\",\n",
    "                         \"WeightedAvgPrecisionBalanced\", \"WeightedAvgRecallBalanced\", \"WeightedAvgf1-scoreBalanced\",\n",
    "                         \"DiffMacro/WeightedBalanced\",\"fprBalanced\", \"tprBalanced\" \n",
    "                        ]\n",
    "\n",
    "model_perf_df = model_perf_df.drop(balanced_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da994618",
   "metadata": {},
   "source": [
    "`model_perf_df` has following model data:\n",
    "\n",
    "1. Stratified\n",
    "2. non-balanced Class Weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fb515",
   "metadata": {},
   "source": [
    "## Pick top logistic regression model\n",
    "\n",
    "As `Accuracy` and difference between `Weighted Precision` and `Macro Avg Precision` is not significantly varies across the models, we shall sort by `Recall` as for our data, `RainTomorrow` as `Yes` prediction is more valuable. \n",
    "\n",
    "`Recall` is the number of true positives (`TP`) divided by the number of true positives plus the number of false negatives (`FN`). \n",
    "\n",
    "`Recall` = TP/(TP + FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c68847",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf_df = model_perf_df.sort_values(by=['MacroAvgRecallNone','AccuracyNone', 'MacroAvgf1-scoreNone'], ascending=False)\n",
    "\n",
    "# Pick Top 5\n",
    "\n",
    "model_perf_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88abdfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = model_perf_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60073b",
   "metadata": {},
   "source": [
    "Below shows our top logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f2aaa",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec6618",
   "metadata": {},
   "source": [
    "Now we will plot the ROC curve for our selected top model. \n",
    "\n",
    "`AUC` - `ROC` curve is a performance measurement for the classification problems at various threshold settings. `ROC` is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the `AUC`, the better the model is at predicting 0s as 0s and 1s as 1s. \n",
    "\n",
    "reference: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85379e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve of top first model\n",
    "\n",
    "for index, row in top_model.iterrows():\n",
    "    fpr = row[\"fprNone\"]\n",
    "    tpr = row[\"tprNone\"]\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0eb39",
   "metadata": {},
   "source": [
    "The above plot shows the area under the curve (AUC) for our top model. The score of 0.80 tells us that there is 80% chance that the model will be successful at classifying yes as yes and no as no values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ac40e",
   "metadata": {},
   "source": [
    "## Important Attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d565cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Logistic Regression Model cross validation and performance metrics \n",
    "\n",
    "top_lr_model = LogisticRegression(penalty='none', C=100, class_weight='none', solver='saga')\n",
    "target_names = ['No', 'Yes']\n",
    "#del df_model['RainTomorrow']\n",
    "X = df_model.values\n",
    "\n",
    "y=df_impute['RainTomorrow']\n",
    "top_lr_cv_data = StratifiedKFold(n_splits=10, random_state=123, shuffle=True)\n",
    "top_lr_cv_data.get_n_splits(X, y)\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "for train_indices, test_indices in top_lr_cv_data.split(X,y): \n",
    "            \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    scl_obj.fit(X_train)\n",
    "\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    X_train_scaled = scl_obj.transform(X_train) \n",
    "    X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "    top_lr_model.fit(X_train_scaled,y_train)  # train object\n",
    "    y_hat = top_lr_model.predict(X_test_scaled) # get test set precitions\n",
    "    classes = top_lr_model.classes_\n",
    "\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "\n",
    "    class_report = classification_report(y_test, y_hat, target_names)\n",
    "    \n",
    "df_model['RainTomorrow']= df_impute['RainTomorrow'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix for top LR Model:\\n ', conf)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c42b32",
   "metadata": {},
   "source": [
    "The above classification report provides important metrices for our logistic regression model's performance. \n",
    "\n",
    "The `accuracy` is at 88%.\n",
    "\n",
    "The `precision` and `recall` are higher for predicting `No`-`RainTomorrow`values. The model not as efficient with predicing `Yes`-`RainTomorrow`. \n",
    "\n",
    "The weather prediction is very complex algorithm which scientists have developed over the year and may take lot more attributes into consideration. With the dataset we have, we believe our model prediction is good based on `precision` and `recall` scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "del df_model['RainTomorrow']\n",
    "weights = pd.Series(top_lr_model.coef_[0],index=df_model.columns)\n",
    "weights.plot(kind='bar', figsize=(14,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553428ba",
   "metadata": {},
   "source": [
    "The above plot depicts the weights of the features from the top logistic regression model.\n",
    "\n",
    "As we can see some features have much higher weight then the others. As per the model following features are of high importance:\n",
    "\n",
    "Positive relation:\n",
    "\n",
    "`Coud3pm` - The clouds at 3pm (afternoon) has high importance in predicting the chances of `RainTomorrow`. This is expected as clouds in the afternoon is expected for the rains the next day.\n",
    "\n",
    "`Pressure9am` - The pressure in the morning is negatively correlated to pressure in the afternoon in terms of predicting the `RainTomorrow`. If the Pressure is high in the morning this increases the chance of Rain the next day as by the afternoon we may observe the change in pressure.\n",
    "\n",
    "`Humidity3pm` - As expected the higher humidity in afternoon has higher predictability for `RainTomorrow`.\n",
    "\n",
    "`WindGustSpeed` - The increase in `WindGustSpeed` has positive impact on the prediction for `RainTomorrow`. Which meets the expectation as the Pressure reduces we may see higher Wind Gusts and may lead to Rains the next day.\n",
    "\n",
    "Negative relation:\n",
    "\n",
    "`Sunshine` - More sunshine leads to lower chances on `RainTomorrow`. This makes sense as this means the cloud cover is less and chances are rain reduces significantly.\n",
    "\n",
    "`Pressure3pm` - Lower pressure in the afternoon leads to higher chances of `RainTomorrow`. This is also expected as the pressure towards the later part of day will have impact on the rain the next day.\n",
    "\n",
    "`MinTemp` - Albeit smaller but the increase in `MinTemp` reduces the chances of `RainTomorrow`. This is in line with the `Sunshine`, as more sunshine will increase the minimum temperature and the chances or Rain will reduce.\n",
    "\n",
    "Surprisingly `RainToday` doesn’t have higher weight and is not among the most important features. One would assume that based on the fact if it rained today the chances of `RainTomorrow` may significantly increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243bbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at feature importance \n",
    "\n",
    "import shap # SHAP for Explaining Models\n",
    "shap.initjs()\n",
    "# Create a tree explainer and understanding the values we have \n",
    "shap_ex = shap.LinearExplainer(top_lr_model, X_test)\n",
    "vals = shap_ex.shap_values(X_test)\n",
    "shap.summary_plot(vals, df_model.columns, plot_type=\"bar\")\n",
    "\n",
    "# Reference: https://shap.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448c702",
   "metadata": {},
   "source": [
    "The above plot shows the total impact of each feature on our model. As explained in previous weighted plot, these features have high importance in predicting `RainTomorrow`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefac438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effect of all the features through SHAP summary plot:\n",
    "explainer = shap.Explainer(top_lr_model, X_train, feature_names=df_model.columns)\n",
    "shap_values = explainer(X_test)\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "# Reference: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf2a0b",
   "metadata": {},
   "source": [
    "**Explaining the logistic model for Rain Tomorrow prediction:**\n",
    "\n",
    "SHAP summary plots give us a birds-eye view of feature importance and what is driving it. The above SHAP summary plot is made of many dots. Each dot has three characteristics:\n",
    "\n",
    "The features are listed from high to low rank. The colors indicate how the change in the values of the features impact the output attribute (`RainTomorrow`). For example, on above plot, `Humidity3pm` shows the higher value of it has higher impact on the output. Horizontal location shows whether the effect of that value caused a higher or lower prediction.\n",
    "\n",
    "Some things the summary plot is  able to easily pick out:-\n",
    "\n",
    "Higher means more likely to be positive, so in the plots above the “red” features are actually helping raise the chance for `RainTomorrow`, while the negative features are lowering the chance for `RainTomorrow`.\n",
    "The model ignored around 56 features which were of lower importance in predicting the chances of `RainTomorrow`.\n",
    "\n",
    "Usually `Pressure9am` has moderate effect on the prediction, but there are extreme cases of `WindGustSpeed` where a high value still caused moderate level of prediction.\n",
    "\n",
    "Reference: https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2907b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaining why a sample weather record # 200 is classified as Rain Tomorrow (Yes/No)?\n",
    "ind = 200\n",
    "shap.plots.force(shap_values[ind])\n",
    "# Reference: https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55aff5b",
   "metadata": {},
   "source": [
    "**Interpretation of above prediction:**\n",
    "\n",
    "We predicted -374.07, whereas the base_value is -347. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue. The biggest impact comes from 'Pressure9am' being 1,021. Though the 'Humidity3pm' value has a meaningful effect decreasing the prediction.\n",
    "\n",
    "If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.\n",
    "\n",
    "\n",
    "Reference: https://www.kaggle.com/dansbecker/shap-values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8a5d4",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a471df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5185c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b900f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4750c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee007c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbf246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1d924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9fbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4dea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee6faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d0f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f8a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2899f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc9250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ad744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077005d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54e7fee0",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83faed00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f75559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b7faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3bc20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795d5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59ad28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0fb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528de1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857775b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ab94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c094a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd662025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b330b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43e630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9da3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf13a23b",
   "metadata": {},
   "source": [
    "### Additional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f3180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ad630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0f150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee5b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069eec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58a547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9663a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca18cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4aa762f",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c9dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bb1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8f27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff11a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e96a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb310dc7",
   "metadata": {},
   "source": [
    "# Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9afde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee45d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53bcbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e288a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8dfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c634f17b",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dcfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20caa688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51027c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5aa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5248552",
   "metadata": {},
   "source": [
    "# Seemant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394a5e0",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bb665",
   "metadata": {},
   "source": [
    "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"RainTomorrow\"] = df_model[\"RainTomorrow\"].map(dict(Yes=1, No=0))\n",
    "y= df_model[\"RainTomorrow\"].values\n",
    "del df_model[\"RainTomorrow\"]\n",
    "X = df_model.values\n",
    "df_model[\"RainTomorrow\"] = df_impute[\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size = 0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgbc = XGBClassifier(objective='binary:logistic')\n",
    "xgbc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4efc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of XGBoost Model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,roc_curve, roc_auc_score\n",
    "y_predxgb = xgbc.predict(X_test)\n",
    "report = classification_report(y_test, y_predxgb)\n",
    "print(report)\n",
    "print(\"Accuracy of the XGBoost Model is:\",accuracy_score(y_test,y_predxgb)*100,\"%\")\n",
    "cm = confusion_matrix(y_test, y_predxgb)\n",
    "sns.heatmap(cm, annot=True,cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix for XGBoost Model\")\n",
    "plt.show()\n",
    "# Reference: https://towardsdatascience.com/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97\n",
    "# Reference: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67379e21",
   "metadata": {},
   "source": [
    "# Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94faafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=300) # increasing number of trees for better accuracy\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe98089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of Random Forest Classifier Model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,roc_curve, roc_auc_score\n",
    "y_pred2 = classifier.predict(X_test)\n",
    "report3 = classification_report(y_test, y_pred2)\n",
    "print(report3)\n",
    "print(\"Accuracy of the Random Forest Model is:\",accuracy_score(y_test,y_pred2)*100,\"%\")\n",
    "cm3 = confusion_matrix(y_test, y_pred2)\n",
    "sns.heatmap(cm3, annot=True)\n",
    "plt.title(\"Confusion Matrix for Random Forest Classifier Model\")\n",
    "plt.show()\n",
    "# Reference: https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "# Reference: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ea7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = classifier.predict_proba(X_test)\n",
    "pos_proba = y_pred_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, pos_proba)\n",
    "plt.plot(fpr, tpr, '*-')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.legend(['Logistic regression', 'Random chance'])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve for Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14386252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7daf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6d19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a67fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773c2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97a0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10a0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010b076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76224c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cfcc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc24ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85928958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d3d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4580f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeefa99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cf843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84080605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f583d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94adbbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9524cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9b471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "124042ec",
   "metadata": {},
   "source": [
    "# Tai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"RainTomorrow\"] = df_model[\"RainTomorrow\"].map(dict(Yes=1, No=0))\n",
    "y= df_model[\"RainTomorrow\"].values\n",
    "del df_model[\"RainTomorrow\"]\n",
    "X = df_model.values\n",
    "yhat = np.zeros(y.shape)\n",
    "df_model[\"RainTomorrow\"] = df_impute[\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "clf = RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1, oob_score=True)\n",
    "for train, test in cv.split(X,y):\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    print ('Random Forest accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(ytrue,yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue,yhat, title=''):\n",
    "    acc_list = per_class_accuracy(ytrue,yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('Class value (one per face)')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "\n",
    "plot_class_acc(y,yhat,title=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of decision trees - there are 150 decision trees\n",
    "\n",
    "clf.estimators_\n",
    "\n",
    "\n",
    "# it helps to see how many 'decision trees' are associated with our random forest model. \n",
    "# From the list, we can visualize some of the 'trees'. We are picking the first index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree diagram for the first index (first tree):\n",
    "\n",
    "from sklearn import tree\n",
    "plt.figure(figsize = (15,10))\n",
    "tree.plot_tree(clf.estimators_[1], filled = True)\n",
    "\n",
    "#Given the diagram below it is hard to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbfcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We cannot produce diagram for all decision trees in one instance. \n",
    "#We can visualize the data portion only for all the decision trees. \n",
    "#We can see how all the decision trees are being classified\n",
    "\n",
    "for i in range(len(clf.estimators_)):\n",
    "    print(tree.export_text(clf.estimators_[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b087c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30c159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89418636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b96778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e01592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e68ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc16cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf5e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ea27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849b413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9f0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66776e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad2597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910fcf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b96c7491",
   "metadata": {},
   "source": [
    "# Ravi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model_stats_columns=[\"Iteration\", \"MSE\", \"RMSE\", \"clf\", \"Time\"]\n",
    "\n",
    "def get_model_stats(model_clf,y_test, y_hat, target_names):\n",
    "   \n",
    "    mse = mean_squared_error(y_test,y_hat)\n",
    "    rmse = mean_squared_error(y_test,y_hat, squared=False)\n",
    "\n",
    "   \n",
    "    # Create ROC Curve\n",
    "    y_test_01 = np.where(y_test ==\"Yes\", 1, [0])\n",
    "    y_hat_01 = np.where(y_hat ==\"Yes\", 1, [0])\n",
    "\n",
    "    #Create a dict of these stats for class weight\n",
    "    #dict will contain stats for balanced on one run, None for the next run\n",
    "\n",
    "    stats_data = [mse, rmse]\n",
    "\n",
    "\n",
    "    stats_row = stats_data \n",
    "\n",
    "    return stats_row\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from timeit import default_timer as timer\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def create_lin_reg_models(df, iterations):\n",
    "    \n",
    "    X = None\n",
    "    y = None\n",
    "    if \"Rainfall\" in df:\n",
    "        y = df[\"Rainfall\"].values # get the labels we want\n",
    "        del df[\"Rainfall\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    print (X)\n",
    "    print (y)\n",
    "    cv_data = ShuffleSplit(n_splits=iterations,\n",
    "                             test_size  = 0.2, random_state = 123)\n",
    "    \n",
    "    \n",
    "    cv_data.get_n_splits(X, y)\n",
    "       \n",
    "    #Initialize variables\n",
    "    iter_num=1\n",
    "    rows = []\n",
    "    target_names = [0, 1]\n",
    "    classes = None    \n",
    "    scl_obj = StandardScaler()\n",
    "    #lr_clf = LinearRegression(n_jobs=4)\n",
    "    #lr_clf = linear_model.Lasso(alpha=0.1)\n",
    "    #estimator = SVR(kernel=\"linear\")\n",
    "    estimator = LinearSVR(random_state=0, tol=1e-5)\n",
    "    lr_clf = RFE(estimator, n_features_to_select=8, step=1)\n",
    "    \n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        starttime = timer()\n",
    "        X_train = X[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "        scl_obj.fit(X_train)\n",
    "\n",
    "        X_test = X[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "            \n",
    "        try:\n",
    "            X_train_scaled = scl_obj.transform(X_train) \n",
    "            X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "            lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "            y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "            print(lr_clf.coef_)            \n",
    "            model_stats = get_model_stats(lr_clf,y_test=y_test,y_hat=y_hat, target_names=target_names )\n",
    "\n",
    "        except Exception as e:\n",
    "            #print('Error:', str(e))\n",
    "            raise\n",
    "        endtime = timer()\n",
    "        time_taken = endtime - starttime\n",
    "        row = [iter_num] + model_stats + [lr_clf, time_taken]\n",
    "        rows.append(row)\n",
    "        iter_num+=1\n",
    "\n",
    "    \n",
    "    #Create a dataframe with the model stats \n",
    "    df_ret = pd.DataFrame(rows, columns = model_stats_columns)\n",
    "    \n",
    "    df_ret.plot.line(x='Iteration', y='RMSE')\n",
    "    return df_ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e796519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def create_random_forest_models(df, iterations):\n",
    "    \n",
    "    X = None\n",
    "    y = None\n",
    "    if \"Rainfall\" in df:\n",
    "        y = df[\"Rainfall\"].values # get the labels we want\n",
    "        del df[\"Rainfall\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    print (X)\n",
    "    print (y)\n",
    "    cv_data = ShuffleSplit(n_splits=iterations,\n",
    "                             test_size  = 0.2, random_state = 123)\n",
    "    \n",
    "    \n",
    "    cv_data.get_n_splits(X, y)\n",
    "       \n",
    "    #Initialize variables\n",
    "    iter_num=1\n",
    "    rows = []\n",
    "    target_names = [0, 1]\n",
    "    scl_obj = StandardScaler()\n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=123)\n",
    "    \n",
    "    \n",
    "    for train_indices, test_indices in cv_data.split(X,y): \n",
    "        starttime = timer()\n",
    "        X_train = X[train_indices]\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "        scl_obj.fit(X_train)\n",
    "        \n",
    "        X_test = X[test_indices]\n",
    "        y_test = y[test_indices]\n",
    "            \n",
    "        try:\n",
    "            X_train_scaled = scl_obj.transform(X_train) \n",
    "            X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "            regr.fit(X_train_scaled,y_train)  # train object\n",
    "            y_hat = regr.predict(X_test_scaled) # get test set precitions\n",
    "            print(y_hat)\n",
    "            \n",
    "            model_stats = get_model_stats(regr,y_test=y_test,y_hat=y_hat, target_names=target_names )\n",
    "\n",
    "        except Exception as e:\n",
    "            #print('Error:', str(e))\n",
    "            raise\n",
    "        endtime = timer()\n",
    "        time_taken = endtime - starttime\n",
    "        row = [iter_num] + model_stats + [regr, time_taken]\n",
    "        rows.append(row)\n",
    "        iter_num+=1\n",
    "\n",
    "\n",
    "    #Create a dataframe with the model stats \n",
    "    df_ret = pd.DataFrame(rows, columns = model_stats_columns)\n",
    "    return df_ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_model = df_model_copy.copy()\n",
    "del df_model['RainTomorrow']\n",
    "del df_model['IsRainToday']\n",
    "del df_model['RainfallAmount']\n",
    "\n",
    "\n",
    "df_ret = create_lin_reg_models(df=df_model,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_model['RainTomorrow']\n",
    "del df_model['IsRainToday']\n",
    "del df_model['RainfallAmount']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6aeb10a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Wind9am_NW</th>\n",
       "      <th>Wind9am_S</th>\n",
       "      <th>Wind9am_SE</th>\n",
       "      <th>Wind9am_SSE</th>\n",
       "      <th>Wind9am_SSW</th>\n",
       "      <th>Wind9am_SW</th>\n",
       "      <th>Wind9am_W</th>\n",
       "      <th>Wind9am_WNW</th>\n",
       "      <th>Wind9am_WSW</th>\n",
       "      <th>RainfallAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>44.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>46.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140782</th>\n",
       "      <td>3.5</td>\n",
       "      <td>21.8</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1024.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140783</th>\n",
       "      <td>2.8</td>\n",
       "      <td>23.4</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1024.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140784</th>\n",
       "      <td>3.6</td>\n",
       "      <td>25.3</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140785</th>\n",
       "      <td>5.4</td>\n",
       "      <td>26.9</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>37.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140786</th>\n",
       "      <td>7.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.032209</td>\n",
       "      <td>8.890686</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1019.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140787 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MinTemp  MaxTemp  Evaporation  Sunshine  WindGustSpeed  WindSpeed9am  \\\n",
       "0          13.4     22.9     6.032209  8.890686           44.0          20.0   \n",
       "1           7.4     25.1     6.032209  8.890686           44.0           4.0   \n",
       "2          12.9     25.7     6.032209  8.890686           46.0          19.0   \n",
       "3           9.2     28.0     6.032209  8.890686           24.0          11.0   \n",
       "4          17.5     32.3     6.032209  8.890686           41.0           7.0   \n",
       "...         ...      ...          ...       ...            ...           ...   \n",
       "140782      3.5     21.8     6.032209  8.890686           31.0          15.0   \n",
       "140783      2.8     23.4     6.032209  8.890686           31.0          13.0   \n",
       "140784      3.6     25.3     6.032209  8.890686           22.0          13.0   \n",
       "140785      5.4     26.9     6.032209  8.890686           37.0           9.0   \n",
       "140786      7.8     27.0     6.032209  8.890686           28.0          13.0   \n",
       "\n",
       "        WindSpeed3pm  Humidity9am  Humidity3pm  Pressure9am  ...  Wind9am_NW  \\\n",
       "0               24.0         71.0         22.0       1007.7  ...           0   \n",
       "1               22.0         44.0         25.0       1010.6  ...           0   \n",
       "2               26.0         38.0         30.0       1007.6  ...           0   \n",
       "3                9.0         45.0         16.0       1017.6  ...           0   \n",
       "4               20.0         82.0         33.0       1010.8  ...           0   \n",
       "...              ...          ...          ...          ...  ...         ...   \n",
       "140782          13.0         59.0         27.0       1024.7  ...           0   \n",
       "140783          11.0         51.0         24.0       1024.6  ...           0   \n",
       "140784           9.0         56.0         21.0       1023.5  ...           0   \n",
       "140785           9.0         53.0         24.0       1021.0  ...           0   \n",
       "140786           7.0         51.0         24.0       1019.4  ...           0   \n",
       "\n",
       "        Wind9am_S  Wind9am_SE  Wind9am_SSE  Wind9am_SSW  Wind9am_SW  \\\n",
       "0               0           0            0            0           0   \n",
       "1               0           0            0            0           0   \n",
       "2               0           0            0            0           0   \n",
       "3               0           1            0            0           0   \n",
       "4               0           0            0            0           0   \n",
       "...           ...         ...          ...          ...         ...   \n",
       "140782          0           0            0            0           0   \n",
       "140783          0           1            0            0           0   \n",
       "140784          0           1            0            0           0   \n",
       "140785          0           1            0            0           0   \n",
       "140786          0           0            1            0           0   \n",
       "\n",
       "        Wind9am_W  Wind9am_WNW  Wind9am_WSW  RainfallAmount  \n",
       "0               1            0            0            None  \n",
       "1               0            0            0            None  \n",
       "2               1            0            0            None  \n",
       "3               0            0            0            None  \n",
       "4               0            0            0            None  \n",
       "...           ...          ...          ...             ...  \n",
       "140782          0            0            0            None  \n",
       "140783          0            0            0            None  \n",
       "140784          0            0            0            None  \n",
       "140785          0            0            0            None  \n",
       "140786          0            0            0            None  \n",
       "\n",
       "[140787 rows x 61 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rain_classifier(row):\n",
    "    if row[\"Rainfall\"] > 30:\n",
    "        return \"High\"\n",
    "    elif row[\"Rainfall\"] > 10 and row[\"Rainfall\"] < 30:\n",
    "        return \"Moderate\"\n",
    "    elif row[\"Rainfall\"] > 1 and row[\"Rainfall\"] < 10:\n",
    "        return \"Low\"\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "df_model[\"RainfallAmount\"] = df_impute.apply(rain_classifier, axis=1)\n",
    "\n",
    "df_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def create_knn_model(df, response):\n",
    "    # Split into training and test set\n",
    "    \n",
    "    y= df[response].values\n",
    "    print(y)\n",
    "    if response in df:\n",
    "        del df[response]\n",
    "    if \"Rainfall\" in df:\n",
    "        del df[\"Rainfall\"]\n",
    "    if \"RainTomorrow\" in df:\n",
    "        del df[\"RainTomorrow\"]\n",
    "#    if \"IsRainToday\" in df:    \n",
    "#        del df[\"IsRainToday\"]\n",
    "    \n",
    "    X = df.values\n",
    "\n",
    "    pca = PCA(n_components = 2)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=123)\n",
    "    \n",
    "    print(y_train)\n",
    "    neighbors = np.arange(1, 9)\n",
    "    train_accuracy = np.empty(len(neighbors))\n",
    "    test_accuracy = np.empty(len(neighbors))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Loop over K values\n",
    "    for i, k in enumerate(neighbors):\n",
    "        #knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn =  DecisionTreeRegressor(random_state=123)\n",
    "        X_train2 = pca.fit_transform(X_train)\n",
    "        X_test2 =  pca.fit_transform(X_test)\n",
    "        knn.fit(X_train2, y_train)\n",
    "\n",
    "        # Compute traning and test data accuracy\n",
    "        train_accuracy[i] = knn.score(X_train2, y_train)\n",
    "        test_accuracy[i] = knn.score(X_test2, y_test)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plot_decision_regions(X_train2, y_train, clf=knn, legend=2)\n",
    "       \n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title('Knn with K='+ str(k))\n",
    "        plt.show()\n",
    "    # Generate plot\n",
    "    plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy for ' + response)\n",
    "    plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy for ' + response)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('n_neighbors')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #find the maximum accuracy and position\n",
    "    best_k = test_accuracy.argmax()+1\n",
    "\n",
    "    #Now use that K to predict\n",
    "\n",
    "    yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "\n",
    "    scl = StandardScaler()\n",
    "    X = scl.fit_transform(X)\n",
    "\n",
    "    # create cross validation iterator\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    # get a handle to the classifier object, which defines the type\n",
    "    clf = KNeighborsClassifier(n_neighbors=best_k)\n",
    "\n",
    "    # now iterate through and get predictions, saved to the correct row in yhat\n",
    "    # NOTE: you can parallelize this using the cross_val_predict method\n",
    "    for train, test in cv.split(X,y):\n",
    "        clf.fit(X[train],y[train])\n",
    "        yhat[test] = clf.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    return total_accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca688cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_copy = df_model.copy()\n",
    "try:\n",
    "    create_knn_model(df=df_model.head(20000),response='RainTomorrow')\n",
    "except Exception as e:\n",
    "    print (e.message)\n",
    "finally:\n",
    "    df_model = df_model_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634346c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df_model[\"RainfallAmount\"].values\n",
    "del df_model[\"RainfallAmount\"]\n",
    "del df_model[\"Rainfall\"]\n",
    "del df_model[\"RainTomorrow\"]\n",
    "del df_model[\"IsRainToday\"]\n",
    "X = df_model.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0450b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"RainTomorrow\"] = df_model[\"RainTomorrow\"].map(dict(Yes=1, No=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba260622",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4075214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cea7ba",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "\n",
    "In Linear Regression Model, the dependent variable (`RainfallAmount`) is dependent of independent variables (the other 16 features). This is an instance of multiple linear regression, which means, the independent variable is dependent of multiple features.\n",
    "\n",
    "Linear Regression works with continous variables and categorical variables doesn't translate well. To make categorical variables, we used One-hot encoding to convert them to numerical vectors. The downside of it, it may introduce multi-collineriaty, but we can minimize by taking only the important features.\n",
    "\n",
    "There are multiple ways of doing Linear Regression, LASSO, Ridge and simple linear regression. We create models of all types and get the best model that has least RMSE values.\n",
    "\n",
    "The function below builds linear regression models:\n",
    "\n",
    "1. Ordinary Least squares\n",
    "\n",
    "LinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "2. LASSO\n",
    "Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
    "\n",
    "The optimization objective for Lasso is:\n",
    "\n",
    "(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
    "\n",
    "Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio=1.0 (no L2 penalty).\n",
    "\n",
    "\n",
    "3. RFE (Recursive Feature Elimination)\n",
    "\n",
    "Feature ranking with recursive feature elimination.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "We create models of these and then store them in a dataframe. We then compare the RMSE of these models, and find which model has lowest RMSE and take as the best model of Regression.\n",
    "\n",
    " \n",
    " #### References for definitions\n",
    " 1. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    " 2. https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    " 3. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96797efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f38b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Trees\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def create_decision_tree_models(df, iterations):\n",
    "    \n",
    "   \n",
    "    if \"Rainfall\" in df:\n",
    "        y = df[\"Rainfall\"].values # get the labels we want\n",
    "        del df[\"Rainfall\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    criterion = [\"mse\", \"friedman_mse\", \"mae\", \"poisson\"]\n",
    "    \n",
    "    for crit in criterion:\n",
    "        regr = DecisionTreeRegressor(criterion=crit, random_state=123)\n",
    "        score = cross_val_score(regr, X, y, cv=iterations, n_jobs=4)\n",
    "        print(crit, score)\n",
    "        #print(regr.tree_)\n",
    "\n",
    "    #Create a dataframe with the model stats \n",
    "#    df_ret = pd.DataFrame(rows, columns = model_stats_columns)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model_copy.copy()\n",
    "if \"RainTomorrow\" in df_model:\n",
    "    del df_model['RainTomorrow']\n",
    "\n",
    "if \"IsRainToday\" in df_model:        \n",
    "    del df_model['IsRainToday']\n",
    "\n",
    "if \"RainfallAmount\" in df_model:   \n",
    "    del df_model['RainfallAmount']\n",
    "    \n",
    "    \n",
    "#df_model = df_model.select_dtypes(exclude=['uint8'])\n",
    "\n",
    "create_models_classification(df=df_model,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654305d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bb855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_copy = df_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfaf18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "954f192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create models\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from timeit import default_timer as timer\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def create_models_continous(model, df, iterations):\n",
    "\n",
    "    X = None\n",
    "    y = None\n",
    "    if \"Rainfall\" in df:\n",
    "        y = df[\"Rainfall\"].values # get the labels we want\n",
    "        del df[\"Rainfall\"] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    random_state = 123\n",
    "    njobs = -1\n",
    "    \n",
    "    #models = [\"LinearRegression\", \"Lasso\", \"LinearSVR\"]\n",
    "    models = [\"LinearRegression\", \"Lasso\"]\n",
    "\n",
    "    for model in models:\n",
    "        clf = None\n",
    "        if model == \"LinearRegression\":\n",
    "            clf = LinearRegression(n_jobs=njobs)\n",
    "        elif model == \"Lasso\":\n",
    "            clf = Lasso(alpha=0.1)\n",
    "        elif model == \"LinearSVR\":    \n",
    "            estimator = LinearSVR(random_state=0, tol=1e-5)\n",
    "            clf = RFE(estimator, n_features_to_select=8, step=1)\n",
    "    \n",
    "        score = cross_val_score(model, X, y, cv=iterations, n_jobs=4, scoring=\"neg_root_mean_squared_error\")\n",
    "        abs_score = [abs(x) for x in score]\n",
    "        print(model, abs_score)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def create_models_classification(model, df, response_feature,iterations):\n",
    "    X = None\n",
    "    y = None\n",
    "    if response_feature in df:\n",
    "        y = df[response_feature].values # get the labels we want\n",
    "        del df[response_feature] # get rid of the class label\n",
    "        X = df.values # use everything else to predict!\n",
    "    else:\n",
    "        print (\"returning\")\n",
    "        return\n",
    "\n",
    "    random_state = 123\n",
    "    njobs = -1\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=iterations,shuffle=True, random_state=random_state)\n",
    "    \n",
    "    #models = [\"LinearRegression\", \"Lasso\", \"LinearSVR\"]\n",
    "\n",
    "    score = cross_val_score(model, X, y, cv=iterations, n_jobs=4, scoring=\"accuracy\")\n",
    "    print(model, score)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fecb64a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming to models\n",
      "GaussianNB() [0.82101001 0.80907735 0.76944385 0.74912991 0.72064777 0.77675971\n",
      " 0.76518219 0.85360136 0.82589857 0.81062651]\n",
      "coming to models\n",
      "MultinomialNB() [nan nan nan nan nan nan nan nan nan nan]\n",
      "coming to models\n",
      "SGDClassifier(alpha=0.001, max_iter=5, n_jobs=-1, random_state=123, tol=None) [0.86703601 0.84395199 0.83876696 0.8557426  0.79771291 0.80687549\n",
      " 0.81475957 0.85729507 0.80473079 0.87427191]\n",
      "coming to models\n",
      "RandomForestClassifier(max_depth=3, n_estimators=200, n_jobs=-1,\n",
      "                       random_state=123) [0.89502095 0.92961148 0.85673698 0.94275162 0.8582996  0.85638185\n",
      " 0.89033312 0.88300895 0.90048302 0.88862054]\n",
      "coming to models\n",
      "LinearSVC(random_state=123) [0.89700973 0.87967895 0.81390724 0.86383976 0.62539953 0.77881952\n",
      " 0.71318986 0.87064924 0.87064924 0.8859213 ]\n",
      "coming to models\n",
      "LogisticRegression(n_jobs=-1, random_state=123) [0.88955181 0.88912565 0.85489026 0.89267704 0.82655018 0.82683429\n",
      " 0.85751829 0.87079131 0.86723966 0.8850689 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#df_model = df_model.select_dtypes(exclude=['uint8'])\n",
    "models = [# KNeighborsClassifier(n_neighbors=8),\n",
    "         GaussianNB(), \n",
    "    MultinomialNB(),\n",
    "    SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3,max_iter=5, \n",
    "                  tol=None, n_jobs=-1, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, \n",
    "                           n_jobs=-1, random_state=123),\n",
    "    LinearSVC(random_state=123),\n",
    "    LogisticRegression(n_jobs=-1, random_state=123)]\n",
    "for model in models:\n",
    "    df_model = df_model_copy.copy()\n",
    "    if \"RainTomorrow\" in df_model:\n",
    "        del df_model['RainTomorrow']\n",
    "\n",
    "    if \"IsRainToday\" in df_model:        \n",
    "        del df_model['IsRainToday']\n",
    "\n",
    "    if \"Rainfall\" in df_model:   \n",
    "        del df_model['Rainfall']\n",
    "    \n",
    "#    create_models_classification(model=model,df=df_model,response_feature=\"RainfallAmount\", iterations=10)\n",
    "    \n",
    "for model in models:\n",
    "    df_model = df_model_copy.copy()\n",
    "    print(\"coming to models\")\n",
    "    if \"RainfallAmount\" in df_model:   \n",
    "        del df_model['RainfallAmount']\n",
    "        \n",
    "    create_models_classification(model=model,df=df_model,response_feature=\"RainTomorrow\", iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "list_of_dir = df_impute.WindGustDir.unique()\n",
    "le.fit(list_of_dir)\n",
    "le.transform(df_impute.WindGustDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be30db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gust_df = le.transform(df_impute.WindGustDir)\n",
    "wind3pm_df = le.transform(df_impute.WindDir3pm)\n",
    "wind9am_df = le.transform(df_impute.WindDir9am)\n",
    "\n",
    "df_model[\"gust\"] =  gust_df.tolist()\n",
    "df_model[\"wind3pm\"] =  wind3pm_df.tolist()\n",
    "df_model[\"wind9am\"] =  wind9am_df.tolist()\n",
    "\n",
    "# Remove original categorical columns\n",
    "df_model = df_model.drop(['WindDir3pm', 'WindDir9am', 'WindGustDir', 'RainToday'], axis = 1)\n",
    "\n",
    "df_model_copy = df_model.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e045d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_copy = df_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46684202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "140782    0\n",
       "140783    0\n",
       "140784    0\n",
       "140785    0\n",
       "140786    0\n",
       "Name: RainfallAmount, Length: 140787, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_model.RainfallAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fb51b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_copy = df_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fdfa8bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         No\n",
       "1         No\n",
       "2         No\n",
       "3         No\n",
       "4         No\n",
       "          ..\n",
       "140782    No\n",
       "140783    No\n",
       "140784    No\n",
       "140785    No\n",
       "140786    No\n",
       "Name: RainTomorrow, Length: 140787, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_copy.RainTomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a4573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adbc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a026739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515201ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439419ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f16ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a4c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa6222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd69c547",
   "metadata": {},
   "source": [
    "# Apurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f71b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"RainTomorrow\"] = df_model[\"RainTomorrow\"].map(dict(Yes=1, No=0))\n",
    "y= df_model[\"RainTomorrow\"].values\n",
    "del df_model[\"RainTomorrow\"]\n",
    "X = df_model.values\n",
    "df_model[\"RainTomorrow\"] = df_impute[\"RainTomorrow\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61efeb",
   "metadata": {},
   "source": [
    "Reference for KNN:\n",
    "    \n",
    "https://www.geeksforgeeks.org/k-nearest-neighbor-algorithm-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc131c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Import necessary modules\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "  \n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size = 0.2, random_state=123)\n",
    "  \n",
    "neighbors = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "  \n",
    "# Loop over K values\n",
    "for i, k in enumerate(neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "      \n",
    "    # Compute traning and test data accuracy\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "  \n",
    "# Generate plot\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "  \n",
    "plt.legend()\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "# create variables we are more familiar with\n",
    "\n",
    "#y = df_model[\"RainTomorrow\"].values # get the labels we want\n",
    "#del df_model[\"RainTomorrow\"] # get rid of the class label\n",
    "#X = df_model.values\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "\n",
    "scl = StandardScaler()\n",
    "X = scl.fit_transform(X)\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "for train, test in cv.split(X,y):\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('KNN accuracy', total_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "output= []\n",
    "for x in yhat:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7378a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7eeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be30330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc05132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7148941d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202da698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2c9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f5a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692567d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a8b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21198f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9b41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03fc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f999ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad550b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
